[
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/index.html",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/intro.html",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/code_guide.html",
    "transcript": "Code Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Handling Secrets Working with git and github Code Guide Contents Handling Secrets Working with git and github Code Guide ¬∂ Handling Secrets ¬∂ üöß Coming soon ‚Ä¶ Working with git and github ¬∂ The recommended way to move code in and out of the hub is via github. This will version control your code via git and serve as one backup location for your research code. The recommended way to authenticate is to use gh-scoped-creds which is preinstalled on the default software images. Please read the in-depth guide by 2i2c for more details. previous Getting Started next Data Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/vm_access.html",
    "transcript": "Getting access to Cloud VMs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs ¬∂ Important This documentation should be considered highly experimental and permissions to follow these instructions are only given to a small subset of testers at the moment. Why? ¬∂ We want to test e.g. containerized workflows that run climate simulations (for online testing of ML parametrizations). For that we need a ‚Äòbare‚Äô VM, and not the JupyterLab interface. Admin considerations ¬∂ To fully separate this testing from the Hub etc we have created a separate project in Gcloud (same billing account and Org). To keep an eye on (e.g. accidental) cost overrun, I added a budget alert (in the linked billing account) that applies to all resources for this new project, and set a 500$ alert with notifications at various thresholds. New users need the following IAM roles (click ‚ÄúGrant Access‚Äù with the email as principals): ‚ÄúCompute Admin‚Äù ‚ÄúService Account User‚Äù How to get access ¬∂ Send your email (private gmail preferred) to Julius Busecke Log into google cloud console and start a small test VM ¬∂ Use the email you sent above to sign into https://console.cloud.google.com Make sure to chose the project ‚ÄúClimSimTesting‚Äù Now navigate to ‚ÄúCompute Engine‚Äù and click on ‚ÄúCreate New Instance‚Äù. Chose a recognizable name (e.g. climsim_<your_name>_test and leave everything else on default for now and click ‚ÄúCreate‚Äù at the bottom. After a short spin up you should see your VM instance with a green checkmark. Try to SSH into it via the cloud console (click on SSH on the far right). Let me know on slack that you were able to create an instance Very Important : Delete your instance after the test. You should do this everytime you are done using the VM to avoid large costs . Click on the triple dots on the right, and chose ‚ÄúDelete‚Äù. previous Bootcamp Guide next Guide for Data and Computation Team Members By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/code_policy.html",
    "transcript": "LEAP-Pangeo Code Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy ¬∂ Enable Science now, but keep evolving. ¬∂ ‚ÄúDon‚Äôt let perfect be the enemy of good‚Äù üöß previous LEAP-Pangeo Implementation Plan next LEAP-Pangeo Data Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/infrastructure_policy.html",
    "transcript": "LEAP-Pangeo Infrastructure Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue LEAP-Pangeo Infrastructure Policy LEAP-Pangeo Infrastructure Policy ¬∂ previous LEAP-Pangeo Data Policy next Infrastructure By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/education.html",
    "transcript": "Education ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP Affiliated Courses Education Contents LEAP Affiliated Courses Education ¬∂ LEAP Affiliated Courses ¬∂ To become a LEAP Affiliated Course you have to fulfill the following requirements: Acknowledge LEAP‚Äôs support on syllabus. Add the course to LEAP‚Äôs education page. If the course will have a github repository, consider developing it within the LEAP GitHub organization . previous Membership next Getting Help By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/how_to_cite.html",
    "transcript": "How to cite LEAP-Pangeo ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo ¬∂ If you use any of the LEAP resources, please follow these guidlines to recognize our work. Add your publication to our LEAP publication tracker ¬∂ Cite LEAP-Pangeo Platform ¬∂ If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). Copy to clipboard Cite Data ¬∂ Please include all datasets used for your work in your citations using the doi of each individual dataset. Don‚Äôt forget to cite your open source packages ¬∂ Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized. previous Getting Help next References By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/references.html",
    "transcript": "References ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue References References ¬∂ AAB+21 Ryan¬†P. Abernathey, Tom Augspurger, Anderson Banihirwe, Charles¬†C. Blackmon-Luca, Timothy¬†J. Crone, Chelle¬†L. Gentemann, Joseph¬†J. Hamman, Naomi Henderson, Chiara Lepore, Theo¬†A. McCaie, Niall¬†H. Robinson, and Richard¬†P. Signell. Cloud-native repositories for big scientific data. Computing in Science Engineering , 23(2):26‚Äì35, 2021. doi:10.1109/MCSE.2021.3059437 . GHA+21 C.¬†L. Gentemann, C.¬†Holdgraf, R.¬†Abernathey, D.¬†Crichton, J.¬†Colliander, E.¬†J. Kearns, Y.¬†Panda, and R.¬†P. Signell. Science storms the cloud. AGU Advances , 2(2):e2020AV000354, 2021. e2020AV000354 2020AV000354. URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020AV000354 , arXiv:https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020AV000354 , doi:https://doi.org/10.1029/2020AV000354 . previous How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/references.md",
    "transcript": "# References ```{bibliography} ```"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/references.html#references",
    "transcript": "References ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue References References ¬∂ AAB+21 Ryan¬†P. Abernathey, Tom Augspurger, Anderson Banihirwe, Charles¬†C. Blackmon-Luca, Timothy¬†J. Crone, Chelle¬†L. Gentemann, Joseph¬†J. Hamman, Naomi Henderson, Chiara Lepore, Theo¬†A. McCaie, Niall¬†H. Robinson, and Richard¬†P. Signell. Cloud-native repositories for big scientific data. Computing in Science Engineering , 23(2):26‚Äì35, 2021. doi:10.1109/MCSE.2021.3059437 . GHA+21 C.¬†L. Gentemann, C.¬†Holdgraf, R.¬†Abernathey, D.¬†Crichton, J.¬†Colliander, E.¬†J. Kearns, Y.¬†Panda, and R.¬†P. Signell. Science storms the cloud. AGU Advances , 2(2):e2020AV000354, 2021. e2020AV000354 2020AV000354. URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020AV000354 , arXiv:https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020AV000354 , doi:https://doi.org/10.1029/2020AV000354 . previous How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/how_to_cite.md",
    "transcript": "# How to cite LEAP-Pangeo If you use any of the LEAP resources, please follow these guidlines to recognize our work. ## Add your publication to our [LEAP publication tracker](https://docs.google.com/spreadsheets/d/1zVfivXK-GKLEma_uc-SAIRs7OP_qfTUmsypAQeVqstI/edit#gid=645657151) ## Cite LEAP-Pangeo Platform If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: ``` We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). ``` ## Cite Data Please include all datasets used for your work in your citations using the doi of each individual dataset. ## Don't forget to cite your open source packages Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/how_to_cite.html#add-your-publication-to-our-leap-publication-tracker",
    "transcript": "How to cite LEAP-Pangeo ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo ¬∂ If you use any of the LEAP resources, please follow these guidlines to recognize our work. Add your publication to our LEAP publication tracker ¬∂ Cite LEAP-Pangeo Platform ¬∂ If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). Copy to clipboard Cite Data ¬∂ Please include all datasets used for your work in your citations using the doi of each individual dataset. Don‚Äôt forget to cite your open source packages ¬∂ Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized. previous Getting Help next References By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/how_to_cite.html#cite-leap-pangeo-platform",
    "transcript": "How to cite LEAP-Pangeo ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo ¬∂ If you use any of the LEAP resources, please follow these guidlines to recognize our work. Add your publication to our LEAP publication tracker ¬∂ Cite LEAP-Pangeo Platform ¬∂ If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). Copy to clipboard Cite Data ¬∂ Please include all datasets used for your work in your citations using the doi of each individual dataset. Don‚Äôt forget to cite your open source packages ¬∂ Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized. previous Getting Help next References By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/how_to_cite.html#cite-data",
    "transcript": "How to cite LEAP-Pangeo ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo ¬∂ If you use any of the LEAP resources, please follow these guidlines to recognize our work. Add your publication to our LEAP publication tracker ¬∂ Cite LEAP-Pangeo Platform ¬∂ If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). Copy to clipboard Cite Data ¬∂ Please include all datasets used for your work in your citations using the doi of each individual dataset. Don‚Äôt forget to cite your open source packages ¬∂ Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized. previous Getting Help next References By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/how_to_cite.html#don-t-forget-to-cite-your-open-source-packages",
    "transcript": "How to cite LEAP-Pangeo ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo ¬∂ If you use any of the LEAP resources, please follow these guidlines to recognize our work. Add your publication to our LEAP publication tracker ¬∂ Cite LEAP-Pangeo Platform ¬∂ If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). Copy to clipboard Cite Data ¬∂ Please include all datasets used for your work in your citations using the doi of each individual dataset. Don‚Äôt forget to cite your open source packages ¬∂ Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized. previous Getting Help next References By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/how_to_cite.html#how-to-cite-leap-pangeo",
    "transcript": "How to cite LEAP-Pangeo ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo ¬∂ If you use any of the LEAP resources, please follow these guidlines to recognize our work. Add your publication to our LEAP publication tracker ¬∂ Cite LEAP-Pangeo Platform ¬∂ If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). Copy to clipboard Cite Data ¬∂ Please include all datasets used for your work in your citations using the doi of each individual dataset. Don‚Äôt forget to cite your open source packages ¬∂ Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized. previous Getting Help next References By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/support.rst",
    "transcript": "How to cite LEAP-Pangeo ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo Contents Add your publication to our LEAP publication tracker Cite LEAP-Pangeo Platform Cite Data Don‚Äôt forget to cite your open source packages How to cite LEAP-Pangeo ¬∂ If you use any of the LEAP resources, please follow these guidlines to recognize our work. Add your publication to our LEAP publication tracker ¬∂ Cite LEAP-Pangeo Platform ¬∂ If you used the JupyterHub platform to perform analysis, please add a statement similar to this to your acknowledgment section of the paper: We acknowledge the computing and storage resources provided by the `NSF Science and Technology Center (STC) Learning the Earth with Artificial intelligence and Physics (LEAP)` (Award # 2019625). Copy to clipboard Cite Data ¬∂ Please include all datasets used for your work in your citations using the doi of each individual dataset. Don‚Äôt forget to cite your open source packages ¬∂ Please take the time to cite all packages used in your work, to ensure that the essential work of open source developers for open science is properly recognized. previous Getting Help next References By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/index.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/intro.md",
    "transcript": "# LEAP Technical Documentation This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. ## Dashboard | Update Status                                                                                                                                  | Contributors                                                                                   | Deployment Status                                                                                                                                                                                                     | | ---------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | [![GitHub last commit](https://img.shields.io/github/last-commit/leap-stc/leap-stc.github.io)](https://github.com/leap-stc/leap-stc.github.io) | ![GitHub contributors](https://img.shields.io/github/contributors/leap-stc/leap-stc.github.io) | [![publish-book](https://github.com/leap-stc/leap-stc.github.io/actions/workflows/publish-book.yaml/badge.svg?style=flat-square)](https://github.com/leap-stc/leap-stc.github.io/actions/workflows/publish-book.yaml) | ## Motivation The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: {cite}`AbernatheyEtAl2021` and {cite}`GentemannEtAl2021`. To summarize these arguments, a shared data and computing platform will: - *Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress.* - *Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives.* This access is provided through our [JupyterHub platform](reference.infrastructure.hub). - *Place actionable data in the hands of LEAP partners to support knowledge transfer.using the [](reference.infrastructure.catalog)*. - *Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research.* ## Contents ```{tableofcontents} ```"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/#dashboard",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/#motivation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/#contents",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/#leap-technical-documentation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/references.html#id2",
    "transcript": "References ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue References References ¬∂ AAB+21 Ryan¬†P. Abernathey, Tom Augspurger, Anderson Banihirwe, Charles¬†C. Blackmon-Luca, Timothy¬†J. Crone, Chelle¬†L. Gentemann, Joseph¬†J. Hamman, Naomi Henderson, Chiara Lepore, Theo¬†A. McCaie, Niall¬†H. Robinson, and Richard¬†P. Signell. Cloud-native repositories for big scientific data. Computing in Science Engineering , 23(2):26‚Äì35, 2021. doi:10.1109/MCSE.2021.3059437 . GHA+21 C.¬†L. Gentemann, C.¬†Holdgraf, R.¬†Abernathey, D.¬†Crichton, J.¬†Colliander, E.¬†J. Kearns, Y.¬†Panda, and R.¬†P. Signell. Science storms the cloud. AGU Advances , 2(2):e2020AV000354, 2021. e2020AV000354 2020AV000354. URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020AV000354 , arXiv:https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020AV000354 , doi:https://doi.org/10.1029/2020AV000354 . previous How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/references.html#id3",
    "transcript": "References ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue References References ¬∂ AAB+21 Ryan¬†P. Abernathey, Tom Augspurger, Anderson Banihirwe, Charles¬†C. Blackmon-Luca, Timothy¬†J. Crone, Chelle¬†L. Gentemann, Joseph¬†J. Hamman, Naomi Henderson, Chiara Lepore, Theo¬†A. McCaie, Niall¬†H. Robinson, and Richard¬†P. Signell. Cloud-native repositories for big scientific data. Computing in Science Engineering , 23(2):26‚Äì35, 2021. doi:10.1109/MCSE.2021.3059437 . GHA+21 C.¬†L. Gentemann, C.¬†Holdgraf, R.¬†Abernathey, D.¬†Crichton, J.¬†Colliander, E.¬†J. Kearns, Y.¬†Panda, and R.¬†P. Signell. Science storms the cloud. AGU Advances , 2(2):e2020AV000354, 2021. e2020AV000354 2020AV000354. URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020AV000354 , arXiv:https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020AV000354 , doi:https://doi.org/10.1029/2020AV000354 . previous How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructure-hub",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/reference/infrastructure.md",
    "transcript": "# Infrastructure (reference.infrastructure.catalog)= ## LEAP-Pangeo Data Catalog |                                      |                                               | | ------------------------------------ | --------------------------------------------- | | **Catalog Address**                  | catalog.leap.columbia.edu                     | | **Management Repo**                  | <https://github.com/leap-stc/data-management> | | **Maintained in collaboration with** | [Carbonplan](https://carbonplan.org)          | For more explanation about the catalog, and its role in the overall vision of LEAP, see [](explanation.architecture). See [](guides.data.ingestion) for details on how to ingest data and link it into the catalog. (reference.infrastructure.hub)= ## LEAP-Pangeo JupyterHub Our team has a cloud-based [JupyterHub](https://jupyter.org/hub). For information who can access the hub with which privileges, please refer to [](reference.membership.tiers). |                       |                                                                                   | | --------------------- | --------------------------------------------------------------------------------- | | **Hub Address**       | <https://leap.2i2c.cloud/>                                                        | | **Hub Location**      | [Google Cloud `us-central1`](https://cloud.google.com/compute/docs/regions-zones) | | **Hub Operator**      | [2i2c](https://2i2c.org/)                                                         | | **Hub Configuration** | <https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap>     | This document goes over the primary technical details of the JupyterHub. - For a quick tutorial on basic usage, please check out our [](tutorial.getting_started) tutorial. - To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the [Architecture](explanation.architecture) page. (reference.infrastructure.hub.server)= ### Server #### Managing Servers You can start and stop your server (and even open multiple named servers) from the `Hub Control Panel`. You can get to the hub control panel by navigating to `https://leap.2i2c.cloud/hub/home` in your browser or navigating to `File > Hub Control Panel` from the JupyterLab Interface. (reference.infrastructure.hub.user_dir)= ### Your User Directory When you open your hub, you can navigate to the \"File Browser\" and see all the files in your User Directory <img width=\"442\" alt=\"image\" src=\"https://github.com/leap-stc/leap-stc.github.io/assets/14314623/3ba6b45a-a077-4824-b0ec-9c111af50c33\"> Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: <img width=\"357\" alt=\"image\" src=\"https://github.com/leap-stc/leap-stc.github.io/assets/14314623/a84c12e2-9f8a-4de1-a3e3-feff1bf59061\"> :::\\{note} As shown in the picture above, every user will see `'/home/jovyan'` as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are *your own files* and they cannot be seen/modified by other users (except admins). ::: The primary purpose of this directory is to store small files, like github repositories and other code. :::\\{warning} To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories `/home/jovyan`. Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. **Users who persistently violate the limit may temporarily get reduced cloud access**. To check how much space you are using in your home directory open a terminal window on the hub and run `du -h --max-depth=1 ~/ | sort -h`. If you want to save larger files for your work use our [](reference.infrastructure.buckets) and consult our [Hub Data Guide](guide.data). See the [FAQs](faq.usr_dir_usage_warning) for guidance on reducing storage. ::: (reference.infrastructure.hub.software_env)= ### The Software Environment The software environment you encounter on the Hub is based upon [docker images](https://www.digitalocean.com/community/tutorials/the-docker-ecosystem-an-introduction-to-common-components) which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between - A list of preselected images - The option of passing a custom docker image via the `\"Other...\"` option. #### Preselected Images LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: - <https://github.com/pangeo-data/pangeo-docker-images/> There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found [here](https://github.com/2i2c-org/infrastructure/blob/master/config/clusters/leap/common.values.yaml). For example, at the time of writing, the version of `pangeo-notebook` is `2022.05.10`. A complete list of all packages installed in this environment is located at: - <https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt> :::\\{attention} We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable `JUPYTER_IMAGE_SPEC`) with your work. You could for example print the following in the first cell of a notebook: ```python import os print(os.environ[\"JUPYTER_IMAGE_SPEC\"]) ``` You can then use that string with the [custom images](reference.infrastructure.hub.image.custom) to reproduce your work with exactly the same environment. ::: (reference.infrastructure.hub.image.custom)= #### Custom Images If you select the `Image > Other...` Option during [server login](hub:server:login) you can paste an arbitrary reference in the form of `docker_registry/organization/image_name:image_version`. As an example we can get the `2023.05.08` version of the pangeo tensorflow notebook by pasting `quay.io/pangeo/ml-notebook:2023.05.08`. If you want to build your own docker image for your project, take a look at [this template](https://github.com/2i2c-org/hub-user-image-template) and the instructions to learn how to use [repo2docker](https://github.com/jupyterhub/repo2docker) to set up CI workflows to automatically build docker images from your repository. (reference.infrastructure.hub.software_env.temp_install)= #### Installing additonal packages You can install additional packages using `pip` and `conda`. However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as [custom images](reference.infrastructure.hub.image.custom). ## Cloud Storage (reference.infrastructrue.osn_pod)= ### m2lines OSN Pod (reference.infrastructrue.osn_pod.organization)= #### OSN Pod Organization The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: - `'leap-pangeo'`: Used for Data Ingestion across the m2lines and LEAP community - Buckets: - `'leap-pangeo-manual'`: **No write access for users** - `'leap-pangeo-pipeline'`: **No write access for users** - `'leap-pangeo-inbox'`: *Write access can be shared with users who want to add data e.g. from an HPC center* - `'m2lines'`: Used for project data and publications from the m2lines project - Buckets: - `'m2lines-pubs'`: **No write access for users** - ... various project buckets - `'leap'`: Used for project data and publications from the LEAP project - Buckets: - `'leap-pubs'`: **No write access for users** - ... various project buckets (reference.infrastructrue.osn_pod.credentials)= #### Credentials :::\\{warning} All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). ::: Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: \"Read-only\" and \"Read-Write\". Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. (reference.infrastructure.buckets)= ## LEAP-Pangeo Cloud Storage Buckets LEAP-Pangeo provides users two cloud buckets to store data. Your [](reference.infrastructure.hub.server) is automatically authenticated to read from any of these buckets but write access might differ (see below). See [](reference.authentication) for details on how to access buckets from 'outside' the JupyterHub. - `gs://leap-scratch/` - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. [More info](https://docs.2i2c.org/user/topics/data/cloud/#scratch-bucket) - `gs://leap-persistent/` - Persistent Storage. Use this bucket for storing results you want to share with other members. - `gs://leap-persistent-ro/` - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use [this](hub.data.upload_hpc) method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. - **Do not put sensitive information (passwords, keys, personal data) into these buckets!** - **When writing to buckets only ever write to your personal folder!** Your personal folder is a combination of the bucketname and your github username (e.g. \\`gs://leap-persistent/funky-user/'). ## Compute üöß (reference.authentication)= ## Access to LEAP-Pangeo resources without the JupyterHub (reference.authentication.temp_token)= ### Temporary Token You can generate a temporary (1 hour) token with read/write access as follows: - Now start up a [LEAP-Pangeo server](https://leap.2i2c.cloud) and open a terminal. Install the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) using mamba ```shell mamba install google-cloud-sdk ``` Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. ```shell gcloud auth print-access-token ``` This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. (reference.authentication.google_groups)= ### Persistent Access via Google Groups We manage access rights through [Google Groups](https://groups.google.com). Please contact the [](support.data_compute_team) to get added to the appropriate group (a gmail address is required for this). ### Service Account If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the [](support.data_compute_team) to discuss options. (reference.arco)= ## Analysis-Ready Cloud-Optimized (ARCO) Data Below you can find some examples of ARCO data formats ### Zarr [zarr](https://zarr.dev) (reference.pangeo-forge)= ## Pangeo-Forge You can find more information about Pangeo-Forge [here](https://pangeo-forge.readthedocs.io/en/latest/)."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#leap-pangeo-data-catalog",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#leap-pangeo-jupyterhub",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#server",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#managing-servers",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#your-user-directory",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#the-software-environment",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#preselected-images",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#custom-images",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#installing-additonal-packages",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#cloud-storage",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#m2lines-osn-pod",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#osn-pod-organization",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#credentials",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#leap-pangeo-cloud-storage-buckets",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#compute",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#access-to-leap-pangeo-resources-without-the-jupyterhub",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#temporary-token",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#persistent-access-via-google-groups",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#service-account",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#analysis-ready-cloud-optimized-arco-data",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#zarr",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#pangeo-forge",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#infrastructure",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#explanation-architecture",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/architecture.md",
    "transcript": "(explanation.architecture)= # LEAP-Pangeo Architecture LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. ## Design Principles In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: - Open source - Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards - Agile development on GitHub - Following industry-standard best practices for continuous deployment, testing, etc. - Resuse of existing technologies and contribution to \"upstream\" open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. ## Design and Architecture ```{figure} https://i.imgur.com/PVhoQUu.png --- name: architecture-diagram --- LEAP-Pangeo high-level architecture diagram ``` There are four primary components to LEAP-Pangeo. (explanation.architecture.data-library)= ### The Data Library The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the [IRI Data Library](https://iridl.ldeo.columbia.edu) mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are - NOAA [OISST](https://www.ncei.noaa.gov/products/optimum-interpolation-sst) sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. - High-resolution climate model simulations from the [NCAR \"EarthWorks\"](https://news.ucar.edu/132760/csu-ncar-develop-high-res-global-model-community-use) project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. - Machine-learning \"challenge datasets,\" published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. - Easily accessible syntheses of climate projections from [CMIP6 data](https://esgf-node.llnl.gov/projects/cmip6/), produced by the LEAP team, for use by industry partners for business strategy and decision making. (explanation.architecture.catalog)= #### Data Catalog A [STAC](https://stacspec.org/) data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: - Searching, crawling, and opening datasets from within notebooks or scripts - \"Crawling\" by search indexes or other machine-to-machine interfaces - A pretty web front-end interface for interactive public browsing The [Radiant Earth MLHub](https://mlhub.earth/) is a great reference for how we imagine the LEAP data catalog will eventually look. ### Data Storage Service The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an [Open Storage Network](https://www.openstoragenetwork.org/) pod which allows data to be accessible from both Google Cloud and NCAR's computing system. #### Pangeo Forge ```{figure} https://raw.githubusercontent.com/pangeo-forge/flow-charts/main/renders/architecture.png --- width: 600px name: pangeo-forge-flow --- Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ``` A central tool for the population and maintenance of the LEAP-Pangeo data catalog is [Pangeo Forge](https://pangeo-forge.readthedocs.io/en/latest/). Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define \"recipes\" that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. ### The Hub ```{figure} https://jupyter.org/assets/homepage/labpreview.webp --- width: 400px name: jupyterlab-preview --- Screenshot from JupyterLab. From <https://jupyter.org/> ``` Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project ```{figure} https://jupyterhub.readthedocs.io/en/stable/_images/jhub-fluxogram.jpeg --- width: 400px name: jupyterhub-architecture --- JupyterHub architecture. From <https://jupyterhub.readthedocs.io/> ``` JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically [every existing programming language](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels). We anticipate that LEAP users will primarily use **Python**, **R**, and **Julia** programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching [R Studio](https://www.rstudio.com/). The Pangeo project already provides [curated Docker images](https://github.com/pangeo-data/pangeo-docker-images) with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in [](reference.membership). ### The Knowledge Graph LEAP \"outputs\" will be of four main types: - **Datasets** (covered above) - **Papers** - traditional scientific publications - **Project Code** - the code behind the papers, used to actually generate the scientific results - **Trained ML Models** - models that can be used directly for inference by others - **Educational Modules** - used for teaching All of these object must be tracked and cataloged in a uniform way. The [](explanation.code_policy) and [](explanation.data-policy) will help set these standards. ```{figure} ../images/LEAP_knowledge_graph.png --- width: 600px name: knowledge-graph --- LEAP Knowledge Graph ``` By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP's impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. ## Related Tools and Platforms It‚Äôs useful to understand the recent history and related efforts in this space. - **[Google Colab](https://research.google.com/colaboratory/faq.html)** is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). - **[Google Earth Engine](https://earthengine.google.org/)** is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. - **[Columbia IRI Data Library](https://iridl.ldeo.columbia.edu/index.html)** is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. - **[Pangeo](http://pangeo.io/)** is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including [Jupyter](https://jupyter.org/), [Xarray](http://xarray.pydata.org/), [Dask](http://dask.pydata.org/), and [Zarr](https://zarr.readthedocs.io/). The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. [Pangeo Cloud](https://pangeo.io/cloud.html) is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. - **[Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/)** is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on [SpatioTemporal Asset Catalog](https://stacspec.org/) - **[Radiant Earth ML Hub](https://www.radiant.earth/mlhub/)** is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. - **[Pangeo Forge](https://pangeo-forge.org/)** is a new initiative, funded by the NSF EarthCube program, to build a platform for \"crowdsourcing\" the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#design-principles",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#design-and-architecture",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#the-data-library",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#data-catalog",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#data-storage-service",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#pangeo-forge",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#the-hub",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#the-knowledge-graph",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#related-tools-and-platforms",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#leap-pangeo-architecture",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#architecture-diagram",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#pangeo-forge-flow",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#jupyterlab-preview",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#jupyterhub-architecture",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#reference-membership",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/reference/membership.md",
    "transcript": "(reference.membership)= # Membership Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. ## Code of Conduct All users of LEAP-Pangeo must abide by the [LEAP Code of Conduct](https://leap.columbia.edu/wp-content/uploads/2023/05/LEAP-Code-of-Conduct_V1_October-2022.pdf). (reference.membership.tiers)= ## Membership Tiers The membership tiers are listed in ascending order of access/privileges. - **PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.)** Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. - **EDUCATION MEMBERSHIP** Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. - **RESEARCH MEMBERSHIP** Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). - **LEAP-FUNDED RESEARCHE MEMBERSHIP**. Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see [the main LEAP website](https://leap.columbia.edu/research-home/leap-pangeo/) ### Administrator and Developer Category The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the [`leap-pangeo-full-access`](https://github.com/orgs/leap-stc/teams/leap-pangeo-full-access) team to have full access to all resources. (reference.membership.team-resources)= ## Github Teams and Resources The access to the JupyterHub is implemented via [Github Teams](https://docs.github.com/en/organizations/organizing-members-into-teams/about-teams) in the [leap-stc](https://github.com/orgs/leap-stc/teams) GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. | Tier                     | Github Team                                                                                   | Resources Available                                                                                                                            | Membership Valid for | | ------------------------ | --------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | -------------------- | | **PUBLIC**               | [leap-pangeo-public-access](https://github.com/orgs/leap-stc/teams/leap-pangeo-public-access) | üöß                                                                                                                                             | üöß                   | | **EDUCATION**            | [leap-pangeo-base-access](https://github.com/orgs/leap-stc/teams/leap-pangeo-base-access)     | Access to [storage](reference.infrastructure.buckets) and computing resource up to 4 cores and 32GB RAM on JupyterHub servers.                 | üöß                   | | **RESEARCH**             | [leap-pangeo-full-access](https://github.com/orgs/leap-stc/teams/leap-pangeo-full-access)     | Access to [storage](reference.infrastructure.buckets) and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. | üöß                   | | **LEAP-FUNDED RESEARCH** | [leap-pangeo-full-access](https://github.com/orgs/leap-stc/teams/leap-pangeo-full-access)     | Access to [storage](reference.infrastructure.buckets) and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. | üöß                   | (users.membership.apply)= ## Applying for membership To become a LEAP member please use the [Application Form](https://forms.gle/RpeaMZh5btTdZtzu8). Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. ## Termination of Access Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. ## Offboarding Process Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the `leap-pangeo-users` group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the `leap-pangeo-users` group. (reference.member_sign_up)= ## Member Sign Up Procedure ![](../images/member_sign_up_schematic.png) All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the [](support.data_compute_team) maintains a set of [Github Actions](https://github.com/leap-stc/member_management/actions/workflows/read_sheet.yaml)(the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate [teams](reference.membership.team-resources). For normal operations users should [apply](users.membership.apply) and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to [](guide.team.admin.member_signup_troubleshooting)."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#code-of-conduct",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#membership-tiers",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#administrator-and-developer-category",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#github-teams-and-resources",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#applying-for-membership",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#termination-of-access",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#offboarding-process",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#member-sign-up-procedure",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#membership",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructure-buckets",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#guides-data-ingestion",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/data_guide.md",
    "transcript": "(guide.data)= # Data Guide Data is fundamental to most people's work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. ## Discovering Dataset You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the [](reference.infrastructure.catalog). ## Working with Data in Cloud Object Storage Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: - [2i2c Docs: Data and Filesystem](https://docs.2i2c.org/user/topics/data/filesystem/) We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several [cloud buckets](reference.infrastructure.buckets), and the following steps illustrate how to work with data in object storage as opposed to a filesystem. ### Tools There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: - [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) (and its submodules [gcsfs](https://gcsfs.readthedocs.io/en/latest/) and [s3fs](https://s3fs.readthedocs.io/en/latest/)) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. - [rclone](https://rclone.org/) which provides a Command Line Interface to many different storage backends. :::\\{admonition} Note on rclone documentation :class: tip, dropdown Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the [docs](https://rclone.org/). If however instructions here are not working for your specific use case, please reach out so we can improve the docs. ::: (data.config-files)= #### Configuration for Authenticated Access Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the [LEAP-Pangeo owned buckets](reference.infrastructure.buckets)) you will need to authenticate with a key/secret pair. :::\\{admonition} Always Handle credentials with care! :class: warning Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See [](guide.code.secrets) for more instructions on how to keep secrets safe. ::: We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s `````{tab-set} ````{tab-item} Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the [aws CLI](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html#cli-configure-files-examples)(installed on the hub by defaule): ```shell aws configure --profile <pick_a_name> ``` Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file `~/.aws/credentials` then contains your key/secret similar to this: ``` [<the_profile_name_you_picked>] aws_access_key_id = *** aws_secret_access_key = *** ``` ```` ````{tab-item} Rclone Rclone has its own [configuration file format](https://rclone.org/docs/#config-config-file) where you can specify the key and secret (and many other settings) in a similar fashion (note the missing `aws_` though!). We recommend setting up the config file (show the default location with `rclone config file`) by hand to look something like this: ``` [<remote_name>] ... # other values access_key_id = XXX secret_access_key = XXX ``` You can have multiple 'remotes' in this file for different cloud buckets. For the [](reference.infrastructrue.osn_pod) use this remote definition: ``` [osn] type = s3 provider = Ceph endpoint = https://nyu1.osn.mghpcc.org access_key_id = XXX secret_access_key = XXX ``` ```` ````` :::\\{warning} Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an [AWS cli profiles](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html#cli-configure-files-format), which can also be used for fsspec. There however seem to be multiple issues ([here](https://forum.rclone.org/t/shared-credentials-file-is-not-recognised/46993)) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or [source profiles?](https://forum.rclone.org/t/s3-profile-failing-when-explicit-s3-endpoint-is-present/36063/4?u=jbusecke), anyways the credentials part of it) work if we define one config file per remote [and use the 'default' profile](https://forum.rclone.org/t/shared-credentials-file-is-not-recognised/46993/2?u=jbusecke)which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. **Please make sure to apply proper caution when [handling secrets](guide.code.secrets) for each config files that stores secrets in plain text!** ::: :::\\{note} You can find more great documentation, specifically on how to use OSN resources, in [this section](https://hytest-org.github.io/hytest/essential_reading/DataSources/Data_S3.html#credentialed-access) of the [HyTEST Docs](https://hytest-org.github.io/hytest/doc/About.html#) ::: (hub.data.setup)= ### (hub.data.list)= ### Inspecting contents of the bucket `````{tab-set} ````{tab-item} Fsspec The initial step in working with fsspec is to create a `filesystem` object which enables the abstraction on top of different object storage system. ```python import fsspec # for Google Storage fs = fsspec.filesystem('gs') # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec.filesystem('s3') # equivalent to s3fs.S3FileSystem() ``` For **authenticated access** you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an [aws profile](data.config-files): ```python fs = fsspec.filesystem( 's3', profile='<the_profile_name_you_picked>',  ## This is the profile name you configured above. client_kwargs={'endpoint_url': 'https://nyu1.osn.mghpcc.org '} # This is the endpoint for the m2lines osn pod ) ``` You can now use the `.ls` method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with ```python fs.ls(\"leap-persistent/funky-user\") # replace with your github username ``` ```` ````{tab-item} Rclone To inspect a bucket you can use clone with the profile ('remote' in rclone terminology) set up [above](data.config-files): ```shell rclone ls <remote_name>:bucket-name/funky-user ``` ```` ````` ### Moving Data `````{tab-set} ````{tab-item} Fsspec üöß ```` ````{tab-item} Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly [authenticated](data.config-files)): ```shell rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory ``` You can also move data between cloud buckets using rclone ```shell rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory\\ <remote_name_b>:<bucket-name>/funky-user/some-directory ``` :::{admonition} Copying single files :class: note To copy single files with rclone use the [copyto command](https://rclone.org/commands/rclone_copyto/) or copy the containing folder and use the `--include` or `--exclude` flags to select the file to copy. ::: :::{note} Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. ::: ```` ````` (hub.data.read_write)= ### Basic writing to and reading from cloud buckets We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like [zarr](https://zarr.dev)(for n-dimensional arrays) and [parquet](https://parquet.apache.org)(for tabular data) (read more [here](https://ieeexplore.ieee.org/document/9354557) why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ```python ds = ... ds_processed = ds.mean(...).resample(...) user_path = \"gs://leap-scratch/funky-user\"  # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed.to_zarr(f\"{user_path}/{store_name}\") ``` This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: ```python import xarray as xr ds = xr.open_dataset( \"gs://leap-scratch/funky-user/processed_store.zarr\", engine=\"zarr\", chunks={} )  # ``` ... and you can give this to any other registered LEAP user and they can load it exactly like you can! :::\\{note} Note that providing the url starting with `gs://...` is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the [](reference.infrastructure.buckets), but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out [](data.config-files) to see an example how to do that. ::: You can also write other files directly to the bucket by using [`fsspec.open`](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.open) similarly to the python builtin [`open`](https://docs.python.org/3/library/functions.html#open) ```python with fsspec.open(\"gs://leap-scratch/funky-user/test.txt\", mode=\"w\") as f: f.write(\"hello world\") ``` Another example of a rountrip save and load with numpy: ```python import numpy as np import fsspec arr = np.array([1, 2, 4]) arr ``` ``` array([1, 2, 4]) ``` ```python with fsspec.open(\"gs://leap-scratch/funky-user/arr_test.npy\", mode=\"wb\") as f: np.save(f, arr) with fsspec.open(\"gs://leap-scratch/jbusecke/arr_test.npy\", mode=\"rb\") as f: arr_reloaded = np.load(f) arr_reloaded ``` ``` array([1, 2, 4]) ``` > Make sure to specify `mode='rb'` or `move='wb'` for binary files. ### Deleting from cloud buckets :::\\{warning} Depending on which cloud bucket you are working, make sure to double check which files you are deleting by [inspecting the contents](hub.data.list) and only working in a subdirectory with your username (e.g. `gs://<leap-bucket>/<your-username>/some/project/structure`. ::: You can remove single files by using a gcsfs/fsspec filessytem as above ```python import gcsfs fs = gcsfs.GCSFileSystem()  # equivalent to fsspec.fs('gs') fs.rm(\"leap-persistent/funky-user/file_to_delete.nc\") ``` If you want to remove zarr stores (which are an 'exploded' data format, and thus represented by a folder structure) you have to recursively delete the store. ```python fs.rm(\"leap-scratch/funky-user/processed_store.zarr\", recursive=True) ``` ## Transfering Data into Cloud Storage We distinguish between two primary *types* of data to upload: \"Original\" and \"Published\" data. - **Published Data** has been published and archived in a publically accessible location (e.g. a data repository like [zenodo](https://zenodo.org) or [figshare](https://figshare.com)). We do not recommend uploading this data to the cloud directly, but instead use [Pangeo Forge](https://pangeo-forge.readthedocs.io/en/latest/) to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. - **Original Data** is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. *Be aware that original data could change rapidly as the data producer is iterating on their code*. We encourage all datasets to be archived and published before using them in scientific publications. (guides.data.ingestion)= ### Ingesting Datasets into Cloud Storage If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into [Analysis-Ready Cloud-Optimized (ARCO)](reference.arco) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see [](reference.infrastructure.buckets) for who can access which resource). Based on the 3 [types of data](explanation.data-policy.types) we host in the [](explanation.architecture.data-library) there are different ways of ingesting data: - Linking an existing (public, egress-free) ARCO dataset to the [](explanation.architecture.catalog) - Ingesting and transforming data into an ARCO copy on [](reference.infrastructure.buckets). - (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work [](explanation.data-policies.access)) We have additional requirements for the data ingestion to make the process sustainable and scalable: - Process needs to be reproducible, e.g. when we want to reingest data to a different storage location - Separation of concerns: The person who knows the dataset (the 'data expert') is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on [Pangeo Forge recipes](https://pangeo-forge.readthedocs.io/en/latest/composition/index.html#recipe-composition). For clearer organization each dataset the recipe should reside in its own repository under the `leap-stc` github organization. Each of these repositories will be called a 'feedstock', which contains additional metadata files (you can read more in the [Pangeo Forge docs](https://pangeo-forge.readthedocs.io/en/latest/deployment/feedstocks.html#from-recipe-to-feedstock)). (guides.data.ingestion_pipeline)= #### How to get new data ingested To start ingesting a dataset follow these steps: 1. Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ['leap-stc/data_management' issue tracker](https://github.com/leap-stc/data-management/issues). You should check existing issues with the tag ['dataset'](https://github.com/leap-stc/data-management/issues?q=is%3Aissue+is%3Aopen+label%3Adataset) to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new [dataset_request](https://github.com/leap-stc/data-management/issues/new?assignees=&labels=dataset&projects=&template=new_dataset.yaml&title=New+Dataset+%5BDataset+Name%5D). Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. 1. Use our [feedstock template](https://github.com/leap-stc/LEAP_template_feedstock) to create a feedstock repostory by following instructions in the README to get you started with either one of the above. 1. If issues arise please reach out to the [](support.data_compute_team) :::\\{note} This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the [](support.data_compute_team) if you have data that you would like to publish. See [](explanation.data-policy.types) for more. ::: (guide.data.upload_manual)= #### How to get new data ingested (if public download is not available) If an option to download the source data is available always try to follow the [pangeo-forge based workflow](guides.data.ingestion_pipeline) first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the 'pull' based paradigm of pangeo-forge will not work. In this case we have an option to 'push' the data to a special \"inbox\" bucket (`'leap-pangeo-inbox'`) on the [](reference.infrastructrue.osn_pod), from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the [template feedstock](https://github.com/leap-stc/LEAP_template_feedstock). **Step by Step instructions** - Reach out to the [](support.data_compute_team). They will contact the OSN pod admin and share bucket credentials for the `'leap-pangeo-inbox'` bucket. - Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate [here](data.config-files). - Upload the data to the 'leap-pangeo-inbox' in **a dedicated folder** (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: - Open a bunch of netcdf files into xarray and use `.to_zarr(...)` to write the data to zarr. - Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! - Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated `'leap-pangeo-manual'` bucket on the OSN pod. They can do this by running [this github action](https://github.com/leap-stc/data-management/blob/main/.github/workflows/transfer.yaml), which requires the subfolder name from above as input. - Once the data is moved, follow the instructions in the [template feedstock](https://github.com/leap-stc/LEAP_template_feedstock) to [\"link an existing dataset\"](https://github.com/leap-stc/LEAP_template_feedstock#linking-existing-arco-datasets) (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the [](support.data_compute_team) if you need support. (guide.data.upload_manual_deprecated)= ### Manually uploading/downloading data to cloud buckets (deprecated) :::\\{warning} This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the [](reference.infrastructure.buckets) but we generally encourage data ingestion to the [](reference.infrastructrue.osn_pod) due to the public access and reduced running cost. See above for instructions. ::: We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see [](explanation.data-policy.reproducibility). We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the [](support.data_compute_team)), you should try the methods below. We will always [prioritize unblocking your work](explanation.code-policy.dont-let-perfect-be-the-enemy-of-good). The below solutions fundamentally rely on the data being 'pushed' to the [](reference.infrastructure.buckets) which usually requires intervention on part of the [](explanation.data-policy.roles.data-expert). This stands in contrast to e.g. data ingestion via [](reference.pangeo-forge) where the [](explanation.data-policy.roles.data-expert) only has to work on the recipe creation and the data is 'pulled' in a reproducible way. For more information see [](explanation.data-policy). Fundamentally the 'pushing' of datasets relies on two components: - Setting up permissions so that you can read/write to the [](reference.infrastructure.buckets) - Several methods to get permissions are described in [](reference.authentication). - Initiating a data transfer from your 'local' machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. #### Upload medium sized original data from your local machine For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. - Set up a new environment on your local machine (e.g. laptop) ```shell mamba env create --name leap_pangeo_transfer python=3.9 google-auth gcsfs jupyterlab xarray zarr dask ``` > Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line - Activate the environment ```shell conda activate leap_pangeo_transfer ``` and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now [generate a temporary token](reference.authentication.temp_token) and copy the resulting token into a plain text file `token.txt` in a convenient location on your **local machine**. - Now start a JupyterLab notebook and paste the following code into a cell: ```python import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open(\"path/to/your/token.txt\") as f: access_token = f.read().strip() # setup a storage client using credentials credentials = Credentials(access_token) fs = gcsfs.GCSFileSystem(token=credentials) ``` > Make sure to replace the `path/to/your/token.txt` with the actual path to your token file. Try to write a small dataset to the cloud: ```python ds = xr.DataArray([1]).to_dataset(name=\"test\") mapper = fs.get_mapper( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" )  # This additional step is necessary to have the correct authentication set ds.to_zarr(mapper) ``` > Replace `<your_username>` with your actual username on the hub. - Make sure that you can read the test dataset from within the hub (go back to [Basic writing to and reading from cloud buckets](hub.data.read_write)). - Now the last step is to paste the code to load your actual dataset into the notebook and use `.to_zarr` to upload it. > Make sure to give the store a meaningful name, and raise an issue in the [data-management repo](https://github.com/leap-stc/data-management/issues) to get the dataset added to the LEAP Data Library. > Make sure to use a different bucket than `leap-scratch`, since that will be deleted every 7 days! For more info refer to the available [storage buckets](reference.infrastructure.buckets). (hub.data.upload_hpc)= #### Uploading large original data from an HPC system (no browser access on the system available) A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. **Conversion Script/Notebook** In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). <!-- TODO: Add an example of why this is bad for performance --> Instead we will load the data into an [`xarray.Dataset`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html) and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with `xarray, gcsfs, zarr` installed (you might need additional dependencies for your particular use case). 1. Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the `Dataset.data` is a [dask array](https://docs.dask.org/en/stable/array.html) 1. Check your dataset: - Check that the metadata is correct. - Check that all the variables/dimensions are in the dataset - Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. <!-- Some more info on chunking? --> 1. Try to write out a subset of the data locally by calling the [`.to_zarr`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.to_zarr.html) method on the dataset. <!-- TODO: Warn not to write out the full dataset --> Once that works we can move on to the authentication. **Upload Prerequisites** Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. - You have to be signed up to LEAP's [Google Groups](reference.authentication.google_groups). - Make sure to install the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) in both your <span style=\"color:#9301B4\">HPC environment</span>, and your local computer that can open a web browser (e.g. your laptop). **Steps** Steps executed on your <span style=\"color:#22B401\">\"local\" computer (e.g. laptop)</span> will be colored in green and steps on your <span style=\"color:#9301B4\">\"remote\" computer (e.g. HPC)</span> in purple. 1. SSH into the HPC 1. <span style=\"color:#9301B4\">Check that you have an internet connection with `ping www.google.com`</span> 1. <span style=\"color:#9301B4\">Request no browser authentication: </span> ``` gcloud auth application-default login --scopes=https://www.googleapis.com/auth/devstorage.read_write,https://www.googleapis.com/auth/iam.test --no-browser ``` > üö® It is very important to include the `--scopes=` argument for security reasons. Do not run this command without it! 1. Follow the onscreen prompt and paste the command into a terminal on your local machine. 1. <span style=\"color:#22B401\">This will open a browser window. Authenticate with the gmail account that was added to the google group. </span> 1. <span style=\"color:#22B401\">Go back to the terminal and follow the onscreen instructions.</span> Copy the text from the command line and <span style=\"color:#9301B4\">paste the command in the open dialog on the remote machine.</span> 1. <span style=\"color:#9301B4\">Make sure to note the path to the auth json!</span> It will be something like `.../.config/gcloud/....json`.</span> Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ ```python import xarray as xr import gcsfs import json with open( \"your_auth_file.json\" ) as f:  # üö® make sure to enter the `.json` file from step 7 token = json.load(f) # test write a small dummy xarray dataset to zarr ds = xr.DataArray([1, 4, 6]).to_dataset(name=\"data\") # Once you have confirmed fs = gcsfs.GCSFileSystem(token=token) mapper = fs.get_mapper( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" )  # üö® enter your leap (github) username here ds.to_zarr(mapper) ``` Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to `.to_zarr` with a [dask progress bar](https://docs.dask.org/en/stable/diagnostics-local.html#progress-bar) ```python from dask.diagnostics import ProgressBar with ProgressBar(): ds.to_zarr(mapper) ``` Once the data has been uploaded, make sure to erase the `.../.config/gcloud/....json` file from step 7, and ask to be removed from the Google Group."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#discovering-dataset",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#working-with-data-in-cloud-object-storage",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#tools",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#configuration-for-authenticated-access",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#hub-data-setup",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#inspecting-contents-of-the-bucket",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#moving-data",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#basic-writing-to-and-reading-from-cloud-buckets",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#deleting-from-cloud-buckets",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#transfering-data-into-cloud-storage",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#ingesting-datasets-into-cloud-storage",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#how-to-get-new-data-ingested",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#how-to-get-new-data-ingested-if-public-download-is-not-available",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#manually-uploading-downloading-data-to-cloud-buckets-deprecated",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#upload-medium-sized-original-data-from-your-local-machine",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#uploading-large-original-data-from-an-hpc-system-no-browser-access-on-the-system-available",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#data-guide",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructure-catalog",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#reference-membership-tiers",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#support-data-compute-team",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#office-hours",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#leap-pangeo-community-hours",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#data-and-computation-team",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#sammy-agrawal-columbia-university",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#raphael-hagen-carbonplan",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#former-team-members",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#julius-busecke-columbia-university",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/support.html#getting-help",
    "transcript": "Getting Help ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .rst .pdf repository open issue Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help Contents Office Hours LEAP Pangeo Community Hours Data and Computation Team Sammy Agrawal (Columbia University) Raphael Hagen (CarbonPlan) Former Team Members Julius Busecke (Columbia University) Getting Help ¬∂ For questions about how to use the Hub, please choose from the following resources Always check the FAQ page first Use the LEAP-Pangeo Discussion Forum to search for previous topics or post new ones Join the LEAP Slack and post an issue in the '#leap-pangeo' channel. Reach out to the Data and Computation Team by mentioning '@data-and-compute' on the LEAP slack channel Tag '@leap-stc/data-and-compute' in a Github issue. Office Hours ¬∂ We also offer in-person and virtual Office Hours on Thursdays for questions about LEAP-Pangeo. You can reserve an appointment here . LEAP Pangeo Community Hours ¬∂ We run a monthly LEAP Pangeo community event, where we‚Äôll work together on everything LEAP Pangeo! This event is open to all, and is currently in-person only (with virtual options in development). Watch out for dates in the regular LEAP emails. If you have a problem with the hub, want to learn how to make your workflow more efficient, or just take an hour to connect with other LEAP folks and code together, bring your laptop and come by! This event is open to all skill levels ‚Äî we aim to make it a fun, collaborative and no-stress event. We‚Äôll have snacks, too! üëÄ Keep an eye out for upcoming dates in the email newsletter üóûÔ∏è Data and Computation Team ¬∂ Sammy Agrawal (Columbia University) ¬∂ Leap Pangeo Intern Raphael Hagen (CarbonPlan) ¬∂ Pangeo-Forge Recipe Development Former Team Members ¬∂ Julius Busecke (Columbia University) ¬∂ Manager, Data and Computation previous Education next How to cite LEAP-Pangeo By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_images/sammy.png",
    "transcript": "sammy.png (1170√ó2532)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_images/raphael.png",
    "transcript": "raphael.png (460√ó460)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_images/julius.jpg",
    "transcript": "julius.jpg (269√ó293)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#reference-membership-team-resources",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#users-membership-apply",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#guide-team-admin-member-signup-troubleshooting",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#tutorial-getting-started",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/tutorials/getting_started.md",
    "transcript": "(tutorial.getting_started)= # Getting Started To get started using the hub, check out this video by [James Munroe](https://github.com/jmunroe) from [2i2c](https://2i2c.org) explaining the architecture. <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/RKXWxtNqWKw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> (hub.access)= ## How can I get access to the Hub Only LEAP members will be able to access the hub. Please [become a member](users.membership.apply) and make sure you accept the [invitation on github](faq.where_is_my_invite) before proceeding ## Hub Usage This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to [edit it yourself](https://github.com/leap-stc/leap-stc.github.io/blob/main/book/leap-pangeo/jupyterhub.md) if you have suggetions for improvement! (hub:server:login)= ### Logging In 1. üëÄ Navigate to <https://leap.2i2c.cloud/> and click the big orange button that says \"Log in to continue\" 1. üîê You will be prompted to authorize a GitHub application. Say \"yes\" to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See [](reference.membership.team-resources) for more information. 1. üì† You will redirect to a screen with the following options. <img width=\"410\" alt=\"image\" src=\"https://github.com/leap-stc/leap-stc.github.io/assets/14314623/088946a1-896f-4ff8-af91-8107c9f14cfd\"> > Note: Depending on your [membership](reference.membership.tiers) you might see additional options (e.g. for GPU machines) You have to make 3 choices here: - The machine type (Choose between \"CPU only\" or \"GPU\" if available) **‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training.** - The software environment (\"Image\"). Find more info in the [Software Environment Section](reference.infrastructure.hub.software_env). - The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM 4. üï• Wait for your server to start up. It can take up to few minutes. #### GPUs Certain members (see [](reference.membership.team-resources)) have access to server instance with GPU. Currently the GPUs are [Nvidia T4](https://www.nvidia.com/en-us/data-center/tesla-t4/) models. To check what GPU is available on your server you can use [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface) in the terminal window. You should get output similar to this: ```shell nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     | |-------------------------------+----------------------+----------------------+ | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | |                               |                      |               MIG M. | |===============================+======================+======================| |   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 | | N/A   41C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default | |                               |                      |                  N/A | +-------------------------------+----------------------+----------------------+ ``` ### Using JupyterLab After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the [user guide](https://jupyterlab.readthedocs.io/en/stable/user/interface.html). ### Shutting Down Your Server Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it's best to shut it down directly. To shut it down, go to <https://leap.2i2c.cloud/hub/home> and click the big red button that says \"Stop My Server\" <img width=\"800\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1197350/167768526-7742a260-d353-4bdb-b9d0-36e9cc17aba1.png\"> You can also navigate to this page from JupyterLab by clicking the `File` menu and going to `Hub Control Panel`."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#how-can-i-get-access-to-the-hub",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#hub-usage",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#logging-in",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#gpus",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#using-jupyterlab",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#shutting-down-your-server",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#getting-started",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#faq-where-is-my-invite",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructure-hub-software-env",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#guide-data",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/code_guide.html#guide-code-secrets",
    "transcript": "Code Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Handling Secrets Working with git and github Code Guide Contents Handling Secrets Working with git and github Code Guide ¬∂ Handling Secrets ¬∂ üöß Coming soon ‚Ä¶ Working with git and github ¬∂ The recommended way to move code in and out of the hub is via github. This will version control your code via git and serve as one backup location for your research code. The recommended way to authenticate is to use gh-scoped-creds which is preinstalled on the default software images. Please read the in-depth guide by 2i2c for more details. previous Getting Started next Data Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/code_guide.md",
    "transcript": "# Code Guide (guide.code.secrets)= ## Handling Secrets üöß Coming soon ... ## Working with git and github The recommended way to move code in and out of the hub is via github. This will version control your code via git and serve as one backup location for your research code. The recommended way to authenticate is to use [gh-scoped-creds](https://docs.2i2c.org/user/topics/data/git/#using-gh-scoped-creds) which is preinstalled on the default software images. Please read the in-depth [guide by 2i2c](https://docs.2i2c.org/user/topics/data/git/#move-code-in-and-out-of-the-hub-with-github) for more details."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/code_guide.html#handling-secrets",
    "transcript": "Code Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Handling Secrets Working with git and github Code Guide Contents Handling Secrets Working with git and github Code Guide ¬∂ Handling Secrets ¬∂ üöß Coming soon ‚Ä¶ Working with git and github ¬∂ The recommended way to move code in and out of the hub is via github. This will version control your code via git and serve as one backup location for your research code. The recommended way to authenticate is to use gh-scoped-creds which is preinstalled on the default software images. Please read the in-depth guide by 2i2c for more details. previous Getting Started next Data Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/code_guide.html#working-with-git-and-github",
    "transcript": "Code Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Handling Secrets Working with git and github Code Guide Contents Handling Secrets Working with git and github Code Guide ¬∂ Handling Secrets ¬∂ üöß Coming soon ‚Ä¶ Working with git and github ¬∂ The recommended way to move code in and out of the hub is via github. This will version control your code via git and serve as one backup location for your research code. The recommended way to authenticate is to use gh-scoped-creds which is preinstalled on the default software images. Please read the in-depth guide by 2i2c for more details. previous Getting Started next Data Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/code_guide.html#code-guide",
    "transcript": "Code Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Handling Secrets Working with git and github Code Guide Contents Handling Secrets Working with git and github Code Guide ¬∂ Handling Secrets ¬∂ üöß Coming soon ‚Ä¶ Working with git and github ¬∂ The recommended way to move code in and out of the hub is via github. This will version control your code via git and serve as one backup location for your research code. The recommended way to authenticate is to use gh-scoped-creds which is preinstalled on the default software images. Please read the in-depth guide by 2i2c for more details. previous Getting Started next Data Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructrue-osn-pod",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#faq-usr-dir-usage-warning",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/faq.md",
    "transcript": "# FAQs (faq.where_is_my_invite)= ## \"Where is my invite?\" Please check your email account (**the one you used to sign up for Github** - this is independent of the email you use for LEAP) for an invite that will look similar to this: <img src=\"../images/email_org_invite.png\" alt=\"LEAPPangeo email invite\" height=\"300\"/> Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the \"Organizations\" tab. <img src=\"../images/gh_org_invite_1.png\" alt=\"LEAPPangeo gh invite\" height=\"300\"/> You can follow that and accept the invitation there aswell. <img src=\"../images/gh_org_invite_2.png\" alt=\"LEAPPangeo gh invite2\" height=\"150\"/> (faq.cannot-log-into-hub)= ## I cannot log into the hub üò± If you are unable to log into the hub, please check the following steps: - [ ] Check if you are member of the [appropriate github teams](reference.membership.tiers). If you **are not** follow these steps: - [ ] Did you [sign up for LEAP membership](users.membership.apply)? This will be done for you if you sign up for an event like the Momentum bootcamp! - [ ] Did you receive a github invite? [Here](faq.where_is_my_invite) is how to check for that. - [ ] Check again if they are part of the [appropriate github teams](reference.membership.tiers). - If these steps do not work, please reach out to the [](support.data_compute_team). If you **are** member of one of the github teams, ask them to try the following steps: - [ ] Refresh the browser cache - [ ] Try a different browser - [ ] Restart the computer - If these steps do not work, please reach out to the [](support.data_compute_team). (faq.usr_dir_usage_warning)= ## I received a warning about space on my User Directory If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about [User Directories](reference.infrastructure.hub.user_dir)). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. **Troubleshooting** - To see which files and directories are taking up the bulk of your storage, run `du -h --max-depth=1 ~/ | sort -h` in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. - Delete cached files, ipython checkpoints, and any other unwanted files. - If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP [cloud bucket](https://leap-stc.github.io/leap-pangeo/jupyterhub.html#leap-pangeo-cloud-storage-buckets) or data catalog. For more information, please consult our [](guide.data) and [](explanation.data-policy). Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the [](support.data_compute_team). ## Dask \"Killed Workers\" The \"Killed Worker\" message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message ### Datasets Chunks too large **Issue** The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. **Solution** You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional [options](https://gateway.dask.org/cluster-options.html) to the dask-gatway cluster upon creation: ```python from dask_gateway import Gateway gateway = Gateway() options = gateway.cluster_options() options.worker_memory = 10  # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway.new_cluster(options) cluster.scale(...) client = cluster.get_client() ``` <!-- TODO: Add example how to change this in HTML repr --> [Example with Solution](https://notebooksharing.space/view/2b6753a5ffe8ddfae1da3b8e2b5507e617de47eb25f758a20c92b62e7e650fd7#displayOptions=)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#where-is-my-invite",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#i-cannot-log-into-the-hub",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#i-received-a-warning-about-space-on-my-user-directory",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#dask-killed-workers",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#datasets-chunks-too-large",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#faqs",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_images/email_org_invite.png",
    "transcript": "email_org_invite.png (1094√ó948)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_images/gh_org_invite_1.png",
    "transcript": "gh_org_invite_1.png (512√ó812)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_images/gh_org_invite_2.png",
    "transcript": "gh_org_invite_2.png (1898√ó440)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructure-hub-user-dir",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/leap-pangeo/jupyterhub.html#leap-pangeo-cloud-storage-buckets",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#explanation-data-policy",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructure-hub-image-custom",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/tutorials/getting_started.html#hub-server-login",
    "transcript": "Getting Started ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started Contents How can I get access to the Hub Hub Usage Logging In GPUs Using JupyterLab Shutting Down Your Server Getting Started ¬∂ To get started using the hub, check out this video by James Munroe from 2i2c explaining the architecture. How can I get access to the Hub ¬∂ Only LEAP members will be able to access the hub. Please become a member and make sure you accept the invitation on github before proceeding Hub Usage ¬∂ This is a rough and ready guide to using the Hub. This documentation will be expanded as we learn and evolve. Feel free to edit it yourself if you have suggetions for improvement! Logging In ¬∂ üëÄ Navigate to https://leap.2i2c.cloud/ and click the big orange button that says ‚ÄúLog in to continue‚Äù üîê You will be prompted to authorize a GitHub application. Say ‚Äúyes‚Äù to everything. Note you must belong to the appropriate GitHub team in order to access the hub. See Github Teams and Resources for more information. üì† You will redirect to a screen with the following options. Note: Depending on your membership you might see additional options (e.g. for GPU machines) You have to make 3 choices here: The machine type (Choose between ‚ÄúCPU only‚Äù or ‚ÄúGPU‚Äù if available) ‚ö†Ô∏èThe GPU images should be used only when needed to accelerate model training. The software environment (‚ÄúImage‚Äù). Find more info in the Software Environment Section . The node share. These are shared resources, and you should try to use the smallest image you need. You can easily start up a new server with a larger share if you find your work to be limited by CPU/RAM üï• Wait for your server to start up. It can take up to few minutes. GPUs ¬∂ Certain members (see Github Teams and Resources ) have access to server instance with GPU. Currently the GPUs are Nvidia T4 models. To check what GPU is available on your server you can use nvidia-smi in the terminal window. You should get output similar to this: nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510 .47.03 Driver Version: 510 .47.03 CUDA Version: 11 .6 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | | =============================== + ====================== + ====================== | | 0 Tesla T4 Off | 00000000 :00:04.0 Off | 0 | | N/A 41C P8 11W / 70W | 0MiB / 15360MiB | 0 % Default | | | | N/A | +-------------------------------+----------------------+----------------------+ Copy to clipboard Using JupyterLab ¬∂ After your server fires up, you will be dropped into a JupyterLab environment. If you are new to JupyterLab, you might want to peruse the user guide . Shutting Down Your Server ¬∂ Your server will shut down automatically after a period of inactivity. However, if you know you are done working, it‚Äôs best to shut it down directly. To shut it down, go to https://leap.2i2c.cloud/hub/home and click the big red button that says ‚ÄúStop My Server‚Äù You can also navigate to this page from JupyterLab by clicking the File menu and going to Hub Control Panel . previous LEAP Technical Documentation next Code Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructure-hub-server",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-authentication",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#hub-data-upload-hpc",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#data-config-files",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#hub-data-list",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-arco",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#explanation-data-policy-types",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#explanation-architecture-data-library",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#explanation-architecture-catalog",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#explanation-data-policies-access",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#guides-data-ingestion-pipeline",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#explanation-data-policy-reproducibility",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/code_policy.html#explanation-code-policy-dont-let-perfect-be-the-enemy-of-good",
    "transcript": "LEAP-Pangeo Code Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy ¬∂ Enable Science now, but keep evolving. ¬∂ ‚ÄúDon‚Äôt let perfect be the enemy of good‚Äù üöß previous LEAP-Pangeo Implementation Plan next LEAP-Pangeo Data Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#explanation-data-policy-roles-data-expert",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-pangeo-forge",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-authentication-temp-token",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/data_guide.html#hub-data-read-write",
    "transcript": "Data Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide Contents Discovering Dataset Working with Data in Cloud Object Storage Tools Configuration for Authenticated Access Inspecting contents of the bucket Moving Data Basic writing to and reading from cloud buckets Deleting from cloud buckets Transfering Data into Cloud Storage Ingesting Datasets into Cloud Storage How to get new data ingested How to get new data ingested (if public download is not available) Manually uploading/downloading data to cloud buckets (deprecated) Upload medium sized original data from your local machine Uploading large original data from an HPC system (no browser access on the system available) Data Guide ¬∂ Data is fundamental to most people‚Äôs work at LEAP. This guide describes best practices how to find, read, write, transfer, ingest, and catalog data. Discovering Dataset ¬∂ You want to have a specific dataset to explore or analyze? There is a good chance that somebody else at LEAP has already worked with the data! So the first thing to look for data should always be a visit to the LEAP-Pangeo Data Catalog . Working with Data in Cloud Object Storage ¬∂ Data and files work differently in the cloud. To help onboard you to this new way of working, we have written a guide to Files and Data in the Cloud: 2i2c Docs: Data and Filesystem We recommend you read this thoroughly, especially the part about Git and GitHub. LEAP provides several cloud buckets , and the following steps illustrate how to work with data in object storage as opposed to a filesystem. Tools ¬∂ There are many tools available to interact with cloud object storage. We currently have basic operations documented for two tools: fsspec (and its submodules gcsfs and s3fs ) which provide filesystem-like access from within a python session. Fsspec is also used by xarray under the hood. rclone which provides a Command Line Interface to many different storage backends. Note on rclone documentation Rclone is a very extensive and powerful tool, but with its many options it can be overwhelming (at least it was for Julius) at the beginning. We will only demonstrate essential options here, for more details see the docs . If however instructions here are not working for your specific use case, please reach out so we can improve the docs. Configuration for Authenticated Access ¬∂ Unless a given cloud bucket allows anonymous access or is preauthenticated within your environment (like it is the case for some of the LEAP-Pangeo owned buckets ) you will need to authenticate with a key/secret pair. Always Handle credentials with care! Always handle secrets with care. Do not store them in plain text that is visible to others (e.g. in a notebook cell that is pushed to a public github repository). See Handling Secrets for more instructions on how to keep secrets safe. We recommend to store your secrets in one of the following configuration files (which will be used in the following example to read and write data): s Fsspec Fsspec supports named [](aws profiles) in a credentials files. You can create one via Generate an aws credential file via the aws CLI (installed on the hub by defaule): aws configure --profile <pick_a_name> Copy to clipboard Pick a sensible name for your profile, particularly if you are working with multiple profiles and buckets. The file ~/.aws/credentials then contains your key/secret similar to this: [ < the_profile_name_you_picked > ] aws_access_key_id = *** aws_secret_access_key = *** Copy to clipboard Rclone Rclone has its own configuration file format where you can specify the key and secret (and many other settings) in a similar fashion (note the missing aws_ though!). We recommend setting up the config file (show the default location with rclone config file ) by hand to look something like this: [ < remote_name > ] ... # other values access_key_id = XXX secret_access_key = XXX Copy to clipboard You can have multiple ‚Äòremotes‚Äô in this file for different cloud buckets. For the m2lines OSN Pod use this remote definition: [ osn ] type = s3 provider = Ceph endpoint = https : // nyu1 . osn . mghpcc . org access_key_id = XXX secret_access_key = XXX Copy to clipboard Warning Ideally we want to store these secrets only in one central location. The natural place for these seems to be in an AWS cli profiles , which can also be used for fsspec. There however seem to be multiple issues ( here ) around this feature in rclone, and so far we have not succeeded in using AWS profiles in rclone. According to those issues we can only make the aws profiles (or source profiles? , anyways the credentials part of it) work if we define one config file per remote and use the ‚Äòdefault‚Äô profile which presumably breaks compatibility with fsspec, and also does not work at all right now. So at the moment we will have to keep the credentials in two separate spots ü§∑‚Äç‚ôÇÔ∏è. Please make sure to apply proper caution when handling secrets for each config files that stores secrets in plain text! Note You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs ¬∂ Inspecting contents of the bucket ¬∂ Fsspec The initial step in working with fsspec is to create a filesystem object which enables the abstraction on top of different object storage system. import fsspec # for Google Storage fs = fsspec . filesystem ( 'gs' ) # equivalent to gcsfs.GCSFileSystem() # for s3 fs = fsspec . filesystem ( 's3' ) # equivalent to s3fs.S3FileSystem() Copy to clipboard For authenticated access you need to pass additional arguments. In this case (for the m2lines OSN pod) we pass a custom endpoint and an aws profile : fs = fsspec . filesystem ( 's3' , profile = '<the_profile_name_you_picked>' , ## This is the profile name you configured above. client_kwargs = { 'endpoint_url' : 'https://nyu1.osn.mghpcc.org ' } # This is the endpoint for the m2lines osn pod ) Copy to clipboard You can now use the .ls method to list contents of a bucket and prefixes. You can e.g. list the contents of your personal folder on the persistent GCS bucket with fs . ls ( \"leap-persistent/funky-user\" ) # replace with your github username Copy to clipboard Rclone To inspect a bucket you can use clone with the profile (‚Äòremote‚Äô in rclone terminology) set up above : rclone ls <remote_name>:bucket-name/funky-user Copy to clipboard Moving Data ¬∂ Fsspec üöß Rclone You can move directories from a local computer to cloud storage with rclone (make sure you are properly authenticated ): rclone copy path/to/local/dir/ <remote_name>:<bucket-name>/funky-user/some-directory Copy to clipboard You can also move data between cloud buckets using rclone rclone copy \\ <remote_name_a>:<bucket-name>/funky-user/some-directory \\ <remote_name_b>:<bucket-name>/funky-user/some-directory Copy to clipboard Copying single files To copy single files with rclone use the copyto command or copy the containing folder and use the --include or --exclude flags to select the file to copy. Note Copying with rclone will stream the data from the source to your computer and back to the target, and thus transfer speed is likely limited by the internet connection of your local machine. Basic writing to and reading from cloud buckets ¬∂ We do not recommend uploading large files (e.g. netcdf) directly to the bucket. Instead we recommend to write data as ARCO (Analysis-Ready Cloud-Optimized) formats like zarr (for n-dimensional arrays) and parquet (for tabular data) (read more here why we recommend ARCO formats). If you work with xarray Datasets switching the storage format is as easy as swapping out a single line when reading/writing data: Xarray provides a method to stream results of a computation to zarr ds = ... ds_processed = ds . mean ( ... ) . resample ( ... ) user_path = \"gs://leap-scratch/funky-user\" # üëÄ make sure to prepend `gs://` to the path or xarray will interpret this as a local path store_name = \"processed_store.zarr\" ds_processed . to_zarr ( f \" { user_path } / { store_name } \" ) Copy to clipboard This will write a zarr store to the scratch bucket. You can read it back into an xarray dataset with this snippet: import xarray as xr ds = xr . open_dataset ( \"gs://leap-scratch/funky-user/processed_store.zarr\" , engine = \"zarr\" , chunks = {} ) # Copy to clipboard ‚Ä¶ and you can give this to any other registered LEAP user and they can load it exactly like you can! Note Note that providing the url starting with gs://... is assumes that you have appropriate credentials set up in your environment to read/write to that bucket. On the hub these are already set up for you to work with the LEAP-Pangeo Cloud Storage Buckets , but if you are trying to interact with non-public buckets you need to authenticate yourself. Check out Configuration for Authenticated Access to see an example how to do that. You can also write other files directly to the bucket by using fsspec.open similarly to the python builtin open with fsspec . open ( \"gs://leap-scratch/funky-user/test.txt\" , mode = \"w\" ) as f : f . write ( \"hello world\" ) Copy to clipboard Another example of a rountrip save and load with numpy: import numpy as np import fsspec arr = np . array ([ 1 , 2 , 4 ]) arr Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard with fsspec . open ( \"gs://leap-scratch/funky-user/arr_test.npy\" , mode = \"wb\" ) as f : np . save ( f , arr ) with fsspec . open ( \"gs://leap-scratch/jbusecke/arr_test.npy\" , mode = \"rb\" ) as f : arr_reloaded = np . load ( f ) arr_reloaded Copy to clipboard array ([ 1 , 2 , 4 ]) Copy to clipboard Make sure to specify mode='rb' or move='wb' for binary files. Deleting from cloud buckets ¬∂ Warning Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. gs://<leap-bucket>/<your-username>/some/project/structure . You can remove single files by using a gcsfs/fsspec filessytem as above import gcsfs fs = gcsfs . GCSFileSystem () # equivalent to fsspec.fs('gs') fs . rm ( \"leap-persistent/funky-user/file_to_delete.nc\" ) Copy to clipboard If you want to remove zarr stores (which are an ‚Äòexploded‚Äô data format, and thus represented by a folder structure) you have to recursively delete the store. fs . rm ( \"leap-scratch/funky-user/processed_store.zarr\" , recursive = True ) Copy to clipboard Transfering Data into Cloud Storage ¬∂ We distinguish between two primary types of data to upload: ‚ÄúOriginal‚Äù and ‚ÄúPublished‚Äù data. Published Data has been published and archived in a publically accessible location (e.g. a data repository like zenodo or figshare ). We do not recommend uploading this data to the cloud directly, but instead use Pangeo Forge to transform and upload it to the cloud. This ensures that the data is stored in an ARCO format and can be easily accessed by other LEAP members. Original Data is any dataset that is produced by researchers at LEAP and has not been published yet. The main use case for this data is to share it with other LEAP members and collaborate on it. For original data we support direct uploaded to the cloud. Be aware that original data could change rapidly as the data producer is iterating on their code . We encourage all datasets to be archived and published before using them in scientific publications. Ingesting Datasets into Cloud Storage ¬∂ If you do not find your dataset in the data catalog we can ingest it. Data ingestion in this context means that we have a programatic way to download and transform data into Analysis-Ready Cloud-Optimized (ARCO) formats in a reproducible way, so that the dataset is available for the LEAP community and beyond (see LEAP-Pangeo Cloud Storage Buckets for who can access which resource). Based on the 3 types of data we host in the The Data Library there are different ways of ingesting data: Linking an existing (public, egress-free) ARCO dataset to the Data Catalog Ingesting and transforming data into an ARCO copy on LEAP-Pangeo Cloud Storage Buckets . (Work in Progress): Creating a virtual zarr store from existing publically hosted legacy format data (e.g. netcdf) The end result should feel indistingushable to the user (i.e. they just copy and paste a snippet and can immediately get to work Data Access ) We have additional requirements for the data ingestion to make the process sustainable and scalable: Process needs to be reproducible, e.g. when we want to reingest data to a different storage location Separation of concerns: The person who knows the dataset (the ‚Äòdata expert‚Äô) is in the unique position to encode their knowledge about the dataset into the recipe, but they should not be concerned with the details of how to execute it and where the data is ultimate stored. This is the responsibility of the Data and Compute team. The way we achieve this is to base our ingestion on Pangeo Forge recipes . For clearer organization each dataset the recipe should reside in its own repository under the leap-stc github organization. Each of these repositories will be called a ‚Äòfeedstock‚Äô, which contains additional metadata files (you can read more in the Pangeo Forge docs ). How to get new data ingested ¬∂ To start ingesting a dataset follow these steps: Let the LEAP community and the Data and Computation Team know about this new dataset. We gather all ingestion requests in our ‚Äòleap-stc/data_management‚Äô issue tracker . You should check existing issues with the tag ‚Äòdataset‚Äô to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request . Making these request in a central location enables others to see which datasets are currently being ingested and what the status is. Use our feedstock template to create a feedstock repostory by following instructions in the README to get you started with either one of the above. If issues arise please reach out to the Data and Computation Team Note This does currently not provide a solution to handle datasets that have been produced by you (as e.g. part of a publication). We are working on formalizing a workflow for this type of data. Please reach out to the Data and Computation Team if you have data that you would like to publish. See Types of Data Used at LEAP for more. How to get new data ingested (if public download is not available) ¬∂ If an option to download the source data is available always try to follow the pangeo-forge based workflow first to maximize reproducibility. But if the data of your choice is located on behind a firewall on an HPC center, the ‚Äòpull‚Äô based paradigm of pangeo-forge will not work. In this case we have an option to ‚Äòpush‚Äô the data to a special ‚Äúinbox‚Äù bucket ( 'leap-pangeo-inbox' ) on the m2lines OSN Pod , from there an admin can move the data to another dedicated bucket and the data can be added to the catalog using the template feedstock . Step by Step instructions Reach out to the Data and Computation Team . They will contact the OSN pod admin and share bucket credentials for the 'leap-pangeo-inbox' bucket. Authenticate to that bucket from a compute location that has access to your desired data and the internet. You can find instructions on how to authenticate here . Upload the data to the ‚Äòleap-pangeo-inbox‚Äô in a dedicated folder (note the exact name of that folder, it is important for the later steps). How you exactly achieve the upload will depend on your preference. Some common options include: Open a bunch of netcdf files into xarray and use .to_zarr(...) to write the data to zarr. Use fsspec or rclone to move an existing zarr store to the target bucket Either way the uploaded folder should contain one or more zarr stores! Once you have confirmed that all data is uploaded, ask an admin to move this data to the dedicated 'leap-pangeo-manual' bucket on the OSN pod. They can do this by running this github action , which requires the subfolder name from above as input. Once the data is moved, follow the instructions in the template feedstock to ‚Äúlink an existing dataset‚Äù (The actual ingestion, i.e. conversion to zarr has been done manually in this case). Reach out to the Data and Computation Team if you need support. Manually uploading/downloading data to cloud buckets (deprecated) ¬∂ Warning This section of the docs is just retained for completeness. There might be special situations where it is beneficial/necessary to upload data to the LEAP-Pangeo Cloud Storage Buckets but we generally encourage data ingestion to the m2lines OSN Pod due to the public access and reduced running cost. See above for instructions. We discourage manually moving datasets to our cloud storage as much as possible since it is hard to reproduce these datasets at a future point (if e.g. the dataset maintainer has moved on to a different position) (see Reproducibility . We encourage you to try out the methods above, but if these should not work for some reason (and you were not able to find a solution with the Data and Computation Team ), you should try the methods below. We will always prioritize unblocking your work . The below solutions fundamentally rely on the data being ‚Äòpushed‚Äô to the LEAP-Pangeo Cloud Storage Buckets which usually requires intervention on part of the Data Expert . This stands in contrast to e.g. data ingestion via Pangeo-Forge where the Data Expert only has to work on the recipe creation and the data is ‚Äòpulled‚Äô in a reproducible way. For more information see LEAP-Pangeo Data Policy . Fundamentally the ‚Äòpushing‚Äô of datasets relies on two components: Setting up permissions so that you can read/write to the LEAP-Pangeo Cloud Storage Buckets - Several methods to get permissions are described in Access to LEAP-Pangeo resources without the JupyterHub . Initiating a data transfer from your ‚Äòlocal‚Äô machine (e.g. your laptop, a server, or HPC Cluster). You can check on some methods below. Upload medium sized original data from your local machine ¬∂ For medium sized datasets, that can be uploaded within an hour, you can use a temporary access token generated on the JupyterHub to upload data to the cloud. Set up a new environment on your local machine (e.g. laptop) mamba env create --name leap_pangeo_transfer python = 3 .9 google-auth gcsfs jupyterlab xarray zarr dask Copy to clipboard Add any other dependencies (e.g. netcdf4) that you need to read your data at the end of the line Activate the environment conda activate leap_pangeo_transfer Copy to clipboard and set up a jupyter notbook (or a pure python script) that loads your data in as few xarray datasets as possible. For instance, if you have one dataset that consists of many files split in time, you should set your notebook up to read all the files using xarray into a single dataset, and then try to write out a small part of the dataset to a zarr store. Now generate a temporary token and copy the resulting token into a plain text file token.txt in a convenient location on your local machine . Now start a JupyterLab notebook and paste the following code into a cell: import gcsfs import xarray as xr from google.cloud import storage from google.oauth2.credentials import Credentials # import an access token # - option 1: read an access token from a file with open ( \"path/to/your/token.txt\" ) as f : access_token = f . read () . strip () # setup a storage client using credentials credentials = Credentials ( access_token ) fs = gcsfs . GCSFileSystem ( token = credentials ) Copy to clipboard Make sure to replace the path/to/your/token.txt with the actual path to your token file. Try to write a small dataset to the cloud: ds = xr . DataArray ([ 1 ]) . to_dataset ( name = \"test\" ) mapper = fs . get_mapper ( \"gs://leap-scratch/<your_username>/test_offsite_upload.zarr\" ) # This additional step is necessary to have the correct authentication set ds . to_zarr ( mapper ) Copy to clipboard Replace <your_username> with your actual username on the hub. Make sure that you can read the test dataset from within the hub (go back to Basic writing to and reading from cloud buckets ). Now the last step is to paste the code to load your actual dataset into the notebook and use .to_zarr to upload it. Make sure to give the store a meaningful name, and raise an issue in the data-management repo to get the dataset added to the LEAP Data Library. Make sure to use a different bucket than leap-scratch , since that will be deleted every 7 days! For more info refer to the available storage buckets . Uploading large original data from an HPC system (no browser access on the system available) ¬∂ A commong scenario is the following: A researcher/student has run a simulation on a High Performance Computer (HPC) at their institution, but now wants to collaboratively work on the analysis or train a machine learning model with this data. For this they need to upload it to the cloud storage. The following steps will guide you through the steps needed to authenticate and upload data to the cloud, but might have to be slightly modified depending on the actual setup of the users HPC. Conversion Script/Notebook In most cases you do not just want to upload the data in its current form (e.g. many netcdf files). Instead we will load the data into an xarray.Dataset and then write that Dataset object directly to a zarr store in the cloud. For this you need a python environment with xarray, gcsfs, zarr installed (you might need additional dependencies for your particular use case). Spend some time to set up a python script/jupyter notebook on the HPC system that opens your files and combines them in to one or more xarray.Datasets (combine as many files as sensible into a single dataset). Make sure that your data is lazily loaded and the Dataset.data is a dask array Check your dataset: Check that the metadata is correct. Check that all the variables/dimensions are in the dataset Check the dask chunksize. A general rule is to aim for around 100MB size, but the size and structure of chunking that is optimal depends heavily on the later use case. Try to write out a subset of the data locally by calling the .to_zarr method on the dataset. Once that works we can move on to the authentication. Upload Prerequisites Before we are able to set up authentication we need to make sure our HPC and local computer (required) are set up correctly. You have to be signed up to LEAP‚Äôs Google Groups . Make sure to install the Google Cloud SDK in both your HPC environment , and your local computer that can open a web browser (e.g. your laptop). Steps Steps executed on your ‚Äùlocal‚Äù computer (e.g. laptop) will be colored in green and steps on your ‚Äùremote‚Äù computer (e.g. HPC) in purple. SSH into the HPC Check that you have an internet connection with ping www.google.com Request no browser authentication: gcloud auth application - default login -- scopes = https : // www . googleapis . com / auth / devstorage . read_write , https : // www . googleapis . com / auth / iam . test -- no - browser Copy to clipboard üö® It is very important to include the --scopes= argument for security reasons. Do not run this command without it! Follow the onscreen prompt and paste the command into a terminal on your local machine. This will open a browser window. Authenticate with the gmail account that was added to the google group. Go back to the terminal and follow the onscreen instructions. Copy the text from the command line and paste the command in the open dialog on the remote machine. Make sure to note the path to the auth json! It will be something like .../.config/gcloud/....json . Now you are have everything you need to authenticate. Lets verify that you can write a small dummy dataset to the cloud. In your notebook/script run the following (make sure to replace the filename and your username as instructed). Your dataset should now be available for all LEAP members üéâüöÄ import xarray as xr import gcsfs import json with open ( \"your_auth_file.json\" ) as f : # üö® make sure to enter the `.json` file from step 7 token = json . load ( f ) # test write a small dummy xarray dataset to zarr ds = xr . DataArray ([ 1 , 4 , 6 ]) . to_dataset ( name = \"data\" ) # Once you have confirmed fs = gcsfs . GCSFileSystem ( token = token ) mapper = fs . get_mapper ( \"gs://leap-persistent/<username>/testing/demo_write_from_remote.zarr\" ) # üö® enter your leap (github) username here ds . to_zarr ( mapper ) Copy to clipboard Now you can repeat the same steps but replace your dataset with the full dataset from above and leave your python code running until the upload has finished. Depending on the internet connection speed and the size of the full dataset, this can take a while. If you want to see a progress bar, you can wrap the call to .to_zarr with a dask progress bar from dask.diagnostics import ProgressBar with ProgressBar (): ds . to_zarr ( mapper ) Copy to clipboard Once the data has been uploaded, make sure to erase the .../.config/gcloud/....json file from step 7, and ask to be removed from the Google Group. previous Code Guide next Compute Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-authentication-google-groups",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/code_policy.html#explanation-code-policy",
    "transcript": "LEAP-Pangeo Code Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy ¬∂ Enable Science now, but keep evolving. ¬∂ ‚ÄúDon‚Äôt let perfect be the enemy of good‚Äù üöß previous LEAP-Pangeo Implementation Plan next LEAP-Pangeo Data Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/code_policy.md",
    "transcript": "(explanation.code_policy)= # LEAP-Pangeo Code Policy (explanation.code-policy.dont-let-perfect-be-the-enemy-of-good)= ## Enable Science now, but keep evolving. \"Don't let perfect be the enemy of good\" üöß"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/code_policy.html#enable-science-now-but-keep-evolving",
    "transcript": "LEAP-Pangeo Code Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy ¬∂ Enable Science now, but keep evolving. ¬∂ ‚ÄúDon‚Äôt let perfect be the enemy of good‚Äù üöß previous LEAP-Pangeo Implementation Plan next LEAP-Pangeo Data Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/code_policy.html#leap-pangeo-code-policy",
    "transcript": "LEAP-Pangeo Code Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy Contents Enable Science now, but keep evolving. LEAP-Pangeo Code Policy ¬∂ Enable Science now, but keep evolving. ¬∂ ‚ÄúDon‚Äôt let perfect be the enemy of good‚Äù üöß previous LEAP-Pangeo Implementation Plan next LEAP-Pangeo Data Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_images/LEAP_knowledge_graph.png",
    "transcript": "LEAP_knowledge_graph.png (1018√ó814)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/architecture.html#knowledge-graph",
    "transcript": "LEAP-Pangeo Architecture ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture Contents Design Principles Design and Architecture The Data Library Data Catalog Data Storage Service Pangeo Forge The Hub The Knowledge Graph Related Tools and Platforms LEAP-Pangeo Architecture ¬∂ LEAP-Pangeo is a cloud-based data and computing platform that will be used to support research, education, and knowledge transfer within the LEAP program. Design Principles ¬∂ In the proposal, we committed to building this in a way that enables the tools and infrastructure to be reused and remixed. So The challenge for LEAP Pangeo is to deploy an ‚Äúenterprise quality‚Äù platform built entirely out of open-source tools, and to make this platform as reusable and useful for the broader climate science community as possible. We committed to following the following design principles: Open source Modular system: built out of smaller, standalone pieces which interoperate through clearly documented interfaces / standards Agile development on GitHub Following industry-standard best practices for continuous deployment, testing, etc. Resuse of existing technologies and contribution to ‚Äúupstream‚Äù open source projects on which LEAP-Pangeo depends (rather than development of new stuff just for the sake of it). This is a key part of our sustainability plan. Design and Architecture ¬∂ Fig. 1 LEAP-Pangeo high-level architecture diagram ¬∂ There are four primary components to LEAP-Pangeo. The Data Library ¬∂ The data library will provide analysis-ready, cloud-optimized data for all aspects of LEAP. The data library is directly inspired by the IRI Data Library mentioned above; however, LEAP-Pangeo data will be hosted in the cloud, for maximum impact, accessibility, and interoperability. The contents of the data library will evolve dynamically based on the needs of the project. Examples of data that may become part of the library are NOAA OISST sea-surface temperature data, used in workshops and classes to illustrate the fundamentals of geospatial data science. High-resolution climate model simulations from the NCAR ‚ÄúEarthWorks‚Äù project, used by LEAP researchers to develop machine-learning parameterizations of climate processes like cloud and ocean eddies. Machine-learning ‚Äúchallenge datasets,‚Äù published by the LEAP Team and accessible to the world, to help broading participation by ML researchers into climate science. Easily accessible syntheses of climate projections from CMIP6 data , produced by the LEAP team, for use by industry partners for business strategy and decision making. Data Catalog ¬∂ A STAC data catalog be used to enumerate all LEAP-Pangeo datasets and provide this information to the public. The catalog will store all relevant metadata about LEAP datasets following established metadata standards (e.g. CF Conventions). It will also provide direct links to raw data in cloud object storage. The catalog will facilitate several different modes of access: Searching, crawling, and opening datasets from within notebooks or scripts ‚ÄúCrawling‚Äù by search indexes or other machine-to-machine interfaces A pretty web front-end interface for interactive public browsing The Radiant Earth MLHub is a great reference for how we imagine the LEAP data catalog will eventually look. Data Storage Service ¬∂ The underlying technology for the LEAP Data catalog will be cloud object storage (e.g. Amazon S3), which enables high throughput concurrent access to many simultaneous users over the public internet. Cloud Object Storage is the most performant, cost-effective, and simple way to serve such large volumes of data. Initially, the LEAP data will be stored in Google Cloud Storage, in the same cloud region as the JupyterHub. Going forward, we will work with NCAR to obtain an Open Storage Network pod which allows data to be accessible from both Google Cloud and NCAR‚Äôs computing system. Pangeo Forge ¬∂ Fig. 2 Pangeo Forge high-level workflow. Diagram from https://github.com/pangeo-forge/flow-charts ¬∂ A central tool for the population and maintenance of the LEAP-Pangeo data catalog is Pangeo Forge . Pangeo Forge is an open source tool for data Extraction, Transformation, and Loading (ETL). The goal of Pangeo Forge is to make it easy to extract data from traditional data repositories and deposit in cloud object storage in analysis-ready, cloud-optimized (ARCO) format. Pangeo Forge works by allowing domain scientists to define ‚Äúrecipes‚Äù that describe data transformation pipelines. These recipes are stored in GitHub repositories. Continuous integration monitors GitHub and automatically executes the data pipelines when needed. The use of distributed, cloud-based processing allows very large volumes of data to be processed quickly. Pangeo Forge is a new project, funded by the NSF EarthCube program. LEAP-Pangeo will provide a high-impact use case for Pangeo Forge, and Pangeo Forge will empower and enhance LEAP research. This synergistic relationship with be mutually beneficial to two NSF-sponsored projects. Using Pangeo Forge effectively will require LEAP scientists and data engineers to engage with the open-source development process around Pangeo Forge and related technologies. The Hub ¬∂ Fig. 3 Screenshot from JupyterLab. From https://jupyter.org/ ¬∂ Jupyter Notebook / Lab has emerged as the standard tool for doing interactive data science. Jupyter supports combining rich text, code, and generated outputs (e.g. figures) into a single document, creating a way to communicate and share complete data-science research project Fig. 4 JupyterHub architecture. From https://jupyterhub.readthedocs.io/ ¬∂ JupyterHub is a multi-user Jupyter Notebook / Lab environment that runs on a server. JupyterHub provides a gateway to highly customized software environments backed by dedicated computing with specified resources (CPU, RAM, GPU, etc.) Running in the cloud, JupyterHub can scale up to accommodate any number of simultaneous users with no degradation in performance. JupyterHub environments can support basically every existing programming language . We anticipate that LEAP users will primarily use Python , R , and Julia programming languages. In addition to Jupyter Notebook / Lab, JupyterHub also supports launching R Studio . The Pangeo project already provides curated Docker images with full-featured Python software environments for environmental data science. These environments will be the starting point for LEAP environments. They may be augmented as LEAP evolves with more specific software as needed by research projects. Use management and access control for the Hub are described in Membership . The Knowledge Graph ¬∂ LEAP ‚Äúoutputs‚Äù will be of four main types: Datasets (covered above) Papers - traditional scientific publications Project Code - the code behind the papers, used to actually generate the scientific results Trained ML Models - models that can be used directly for inference by others Educational Modules - used for teaching All of these object must be tracked and cataloged in a uniform way. The LEAP-Pangeo Code Policy and LEAP-Pangeo Data Policy will help set these standards. Fig. 5 LEAP Knowledge Graph ¬∂ By tracking the linked relationships between datasets, papers, code, models, and educational , we will generate a ‚Äúknowledge graph‚Äù. This graph will reveal the dynamic, evolving state of the outputs of LEAP research and the relationships between different elements of the project. By also tracking participations (i.e. humans), we will build a novel and inspiring track record of LEAP‚Äôs impacts through the project lifetime. This is the most open-ended aspect of our infrastructure. Organizing and displaying this information effectively is a challenging problem in information architecture and systems design. Related Tools and Platforms ¬∂ It‚Äôs useful to understand the recent history and related efforts in this space. Google Colab is a free notebook-in-the-cloud service run by Google. It is built around the open source Jupyter project, but with advanced notebook sharing capabilities (like Google Docs). Google Earth Engine is a reference point for all cloud geospatial analytics platforms. It‚Äôs actually a standalone application that is separate from Google Cloud, the single instance of a highly customized, black box (i.e. not open source) application that enables parallel computing on distributed data. It‚Äôs very good at what it was designed for (analyzing satellite images), but isn‚Äôt easily adapted to other applications, such as machine learning. Columbia IRI Data Library is a powerful and freely accessible online data repository and analysis tool that allows a user to view, analyze, and download hundreds of terabytes of climate-related data through a standard web browser. Due to its somewhat outdated architecture, IRI data library cannot easily be updated or adapted to new projects. Pangeo is an open science community oriented around open-source python tools for big-data geoscience. It is a loose ecosystem of interoperable python packages including Jupyter , Xarray , Dask , and Zarr . The Pangeo tools have been deployed in nearly all commercial clouds (AWS, GCP, Azure) as well as HPC environments. Pangeo Cloud is a publicly accessible data-proximate computing environment based on Pangeo tools. Pangeo is used heavily within NCAR. Microsoft Planetary Computer is a collection of datasets and computational tools hosted by Microsoft in the Azure cloud. It combines Pangeo-style computing environments with a data library based on SpatioTemporal Asset Catalog Radiant Earth ML Hub is a cloud-based open library dedicated to Earth observation training data for use with machine learning algorithms. It focuses mostly on data access and curation. Data are cataloged using STAC. Pangeo Forge is a new initiative, funded by the NSF EarthCube program, to build a platform for ‚Äúcrowdsourcing‚Äù the production of analysis-ready, cloud-optimized data. Once operational, Pangeo Forge will be a useful tool for many different projects which need data in the cloud. Of these different tools, we opt to build on Pangeo because of its open-source, grassroots foundations in the climate data science community, strong uptake within NCAR, and track-record of support from NSF. previous FAQs next LEAP-Pangeo Implementation Plan By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/intro.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/tutorials/getting_started.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/code_guide.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/data_guide.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/compute_guide.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/education_guide.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/bootcamp_guide.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/vm_access.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/team_guide.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/faq.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/architecture.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/implementation.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/code_policy.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/data_policy.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/infrastructure_policy.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/reference/infrastructure.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/reference/membership.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/reference/education.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/support.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/references.html",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/_sources/how_to_cite.md",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/support.rst#add-your-publication-to-our-leap-publication-tracker",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/support.rst#cite-leap-pangeo-platform",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/support.rst#cite-data",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/support.rst#don-t-forget-to-cite-your-open-source-packages",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/support.rst#how-to-cite-leap-pangeo",
    "transcript": "Page not found ¬∑ GitHub Pages 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like http://example.com/ ) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages . GitHub Status ‚Äî @githubstatus"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/reference/education.md",
    "transcript": "# Education (reference.education.leap_affiliated_course)= ## LEAP Affiliated Courses To become a LEAP Affiliated Course you have to fulfill the following requirements: - Acknowledge LEAP's support on syllabus. - Add the course to LEAP's education page. - If the course will have a github repository, consider developing it within the [LEAP GitHub organization](https://github.com/leap-stc)."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/education.html#leap-affiliated-courses",
    "transcript": "Education ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP Affiliated Courses Education Contents LEAP Affiliated Courses Education ¬∂ LEAP Affiliated Courses ¬∂ To become a LEAP Affiliated Course you have to fulfill the following requirements: Acknowledge LEAP‚Äôs support on syllabus. Add the course to LEAP‚Äôs education page. If the course will have a github repository, consider developing it within the LEAP GitHub organization . previous Membership next Getting Help By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/education.html#education",
    "transcript": "Education ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP Affiliated Courses Education Contents LEAP Affiliated Courses Education ¬∂ LEAP Affiliated Courses ¬∂ To become a LEAP Affiliated Course you have to fulfill the following requirements: Acknowledge LEAP‚Äôs support on syllabus. Add the course to LEAP‚Äôs education page. If the course will have a github repository, consider developing it within the LEAP GitHub organization . previous Membership next Getting Help By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/infrastructure_policy.md",
    "transcript": "# LEAP-Pangeo Infrastructure Policy"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/infrastructure_policy.html#leap-pangeo-infrastructure-policy",
    "transcript": "LEAP-Pangeo Infrastructure Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue LEAP-Pangeo Infrastructure Policy LEAP-Pangeo Infrastructure Policy ¬∂ previous LEAP-Pangeo Data Policy next Infrastructure By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/data_policy.md",
    "transcript": "--- abbreviations: ARCO: Analysis-Ready Cloud-Optimized --- (explanation.data-policy)= # LEAP-Pangeo Data Policy (explanation.data-policies.access)= ## Data Access üöß (explanation.data-policy.reproducibility)= ## Reproducibility üöß (explanation.data-policy.types)= ## Types of Data Used at LEAP Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in {abbr}`ARCO` formats in the cloud. :::\\{admonition} LEAP produced :class: dropdown Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also [](guides.data.ingestion)). ::: :::\\{admonition} LEAP ingested :class: dropdown Data that is already publically available but has been ingested into cloud storage in {abbr}`ARCO` formats. The actual data/metadata has not been modified from the original. ::: :::\\{admonition} LEAP curated :class: dropdown Data that is already available in {abbr}`ARCO` formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. ::: ## Roles Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): (explanation.data-policy.roles.data-expert)= ### Data Expert üöß"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#data-access",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#reproducibility",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#types-of-data-used-at-leap",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#roles",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#data-expert",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/data_policy.html#leap-pangeo-data-policy",
    "transcript": "LEAP-Pangeo Data Policy ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy Contents Data Access Reproducibility Types of Data Used at LEAP Roles Data Expert LEAP-Pangeo Data Policy ¬∂ Data Access ¬∂ üöß Reproducibility ¬∂ üöß Types of Data Used at LEAP ¬∂ Within the LEAP project we distinguish between several different types of data mostly based on whether the data was used or produced at LEAP and if the data is already accessible in ARCO formats in the cloud. LEAP produced Data that has been created or modified by LEAP researchers. We currently do not provide any way of ensuring that data is archived, and users should never rely on LEAP-Pangeo resources as the only replicate of valuable data (see also Ingesting Datasets into Cloud Storage ). LEAP ingested Data that is already publically available but has been ingested into cloud storage in ARCO formats. The actual data/metadata has not been modified from the original. LEAP curated Data that is already available in ARCO formats in publically accessible object storage. Adding this data to the LEAP-Pangeo Catalog enables us to visualize it with the Data Viewer, and collect all datasets of importance in one single location, but none of the data itself is modified. Roles ¬∂ Many different people at LEAP interact with data in various ways. Here is a list of typical roles (some people have multiple roles): Data Expert ¬∂ üöß previous LEAP-Pangeo Code Policy next LEAP-Pangeo Infrastructure Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/explanation/implementation.md",
    "transcript": "# LEAP-Pangeo Implementation Plan The different elements of the project can be implemented in parallel and gradually connected together. ## Roles :::\\{admonition} Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. ::: ### Data Engineering The {doc}`architecture`, particularly the Data Library, will require expertise in modern data engineering, including the following areas: - Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr - Geospatial data catalogs and APIs - Cloud object storage - Cloud automation and data pipelines - Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) - GitHub workflows - Continuous integration and agile development - Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: - [Development Seed](https://developmentseed.org/) - [Element 84](https://www.element84.com/) - [Azavea](https://www.azavea.com/) - [Radiant Earth](https://www.radiant.earth/) ### DevOps for Cloud Hub Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: - Strong experience with Docker and containerization of workflows - Deploying cloud-native applications (particularly [JupyterHub](https://zero-to-jupyterhub.readthedocs.io/en/latest/)) using Kubernetes and Helm - Continuous deployment using GitHub workflows - Monitoring and optimizing cloud costs in multi-user JupyterHub environments - Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. - Continuous integration and agile development - Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: - [2i2c](https://2i2c.org/) - [Quansight](https://www.quansight.com/) ### Full-Stack Web Development Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as \"full-stack web development\". - Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) - Development and deployment of REST API endpoints for backend services - Consumption of data from third-party APIs (e.g. GitHub API) - Familiarity with Jupyter Notebook format - Continuous integration and agile development - Track record of contribution to multi-stakeholder open-source software #### Education and Training :::\\{admonition} Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? ::: Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. ### Contractors vs. Employees |                 | Pros                                                                                                                                          | Cons                                                                             | | --------------- | --------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | | **Employees**   | Longer-term commitment to project. Better integration with on-campus activities.                                                              | Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. | | **Contractors** | Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don't have to deal with hiring. Acccess to top technical talent. | Potentially less integrated into project.                                        | ## Timeline What follows is a possible timeline for implementation. ### Fall 2021 #### Activities - üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. - üìç Provide basic end-user documentation for using the Hub (comparable to [Pangeo Cloud docs](https://pangeo.io/cloud.html)). #### Milestones - ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. ### Spring 2021 #### Activities - üìç Conduct data survey to assess data needs of research, education, and outreach activities. - üìç Data engineers work with researchers on data ingestion. - üìç Refine Hub environment based on initial feedback. #### Milestones - ‚úÖ LEAP researchers ingest first datasets into cloud data library. - ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. ### Summer 2021 #### Activities - üìç Launch initial data catalog - üìç Begin training program #### Milestones - ‚úÖ Perform first LEAP-Pangeo training for participations - ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. - ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#roles",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#data-engineering",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#devops-for-cloud-hub",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#full-stack-web-development",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#education-and-training",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#contractors-vs-employees",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#timeline",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#fall-2021",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#activities",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#milestones",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#spring-2021",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#id1",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#id2",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#summer-2021",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#id3",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#id4",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/explanation/implementation.html#leap-pangeo-implementation-plan",
    "transcript": "LEAP-Pangeo Implementation Plan ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan Contents Roles Data Engineering DevOps for Cloud Hub Full-Stack Web Development Education and Training Contractors vs. Employees Timeline Fall 2021 Activities Milestones Spring 2021 Activities Milestones Summer 2021 Activities Milestones LEAP-Pangeo Implementation Plan ¬∂ The different elements of the project can be implemented in parallel and gradually connected together. Roles ¬∂ Decision Needed An open question for LEAP-Pangeo is whether to develop and maintain our infrastructure via subcontracts or via Columbia employees whom we hire. Below the roles are enumerated in a generic way according to the needed expertise. Data Engineering ¬∂ The LEAP-Pangeo Architecture , particularly the Data Library, will require expertise in modern data engineering, including the following areas: Geospatial data formats and metadata standards, including modern cloud-optimized formats such as Parquet and Zarr Geospatial data catalogs and APIs Cloud object storage Cloud automation and data pipelines Distributed computing frameworks for data science (e.g. Dask, Prefect, Apache Beam) GitHub workflows Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: Development Seed Element 84 Azavea Radiant Earth DevOps for Cloud Hub ¬∂ Developing and operating the LEAP-Pangeo JupyterHub will require the following expertise: Strong experience with Docker and containerization of workflows Deploying cloud-native applications (particularly JupyterHub ) using Kubernetes and Helm Continuous deployment using GitHub workflows Monitoring and optimizing cloud costs in multi-user JupyterHub environments Building machine-learning environments for Python and R users with tools such as Conda, Conda Forge, and repo2docker. Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Possible contractors who fit this role: 2i2c Quansight Full-Stack Web Development ¬∂ Developing the LEAP Knowledge Graph, including the library of papers, open-source code and machine-learning models, will require a mix of skills commonly referred to as ‚Äúfull-stack web development‚Äù. Front-end web development using HTML, CSS, and modern Javascript frameworks (e.g. React, Vue, etc.) Development and deployment of REST API endpoints for backend services Consumption of data from third-party APIs (e.g. GitHub API) Familiarity with Jupyter Notebook format Continuous integration and agile development Track record of contribution to multi-stakeholder open-source software Education and Training ¬∂ Decision Needed What is the scope of LEAP-Pangeo training? How much should we expect trainees to learn? What is the intersection with other educational activities, including for-credit courses? Training participants in using LEAP-Pangeo will require expertise in research computing pedagogy and state-of-the-art knowledge of best practices in scientific computing, machine learning, and cloud computing. Contractors vs. Employees ¬∂ Pros Cons Employees Longer-term commitment to project. Better integration with on-campus activities. Slow hiring. Recruiting challenges. Uncertainty they can deliver needed results. Contractors Can spin up rapidly. Proven track records. Connection to broader ecosystems. Don‚Äôt have to deal with hiring. Acccess to top technical talent. Potentially less integrated into project. Timeline ¬∂ What follows is a possible timeline for implementation. Fall 2021 ¬∂ Activities ¬∂ üìç Deploy generic Pangeo JupyterHub on Google Cloud using supported credits. üìç Provide basic end-user documentation for using the Hub (comparable to Pangeo Cloud docs ). Milestones ¬∂ ‚úÖ LEAP members log into the hub and run their first code against existing cloud data. Spring 2021 ¬∂ Activities ¬∂ üìç Conduct data survey to assess data needs of research, education, and outreach activities. üìç Data engineers work with researchers on data ingestion. üìç Refine Hub environment based on initial feedback. Milestones ¬∂ ‚úÖ LEAP researchers ingest first datasets into cloud data library. ‚úÖ LEAP seminar uses LEAP-Pangeo data science environments for teaching. Summer 2021 ¬∂ Activities ¬∂ üìç Launch initial data catalog üìç Begin training program Milestones ¬∂ ‚úÖ Perform first LEAP-Pangeo training for participations ‚úÖ LEAP REU interns successfully use LEAP-Pangeo for projects. ‚úÖ First LEAP publications are added to the knowledge graph, along with supporting data and code previous LEAP-Pangeo Architecture next LEAP-Pangeo Code Policy By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/team_guide.md",
    "transcript": "# Guide for Data and Computation Team Members This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. ## Onboarding ### Checklist for new members - [ ] Ask to be added to the [Data and Computation Github team](https://github.com/orgs/leap-stc/teams/data-team/members) - [ ] Ask to be added to the `@data-and-compute` Slack user group - [ ] Subscribe to [](onboarding.slack) - [ ] Consider enabling notifications for [](onboarding.github) - [ ] Make a PR to the `_config.yaml` file [here](https://github.com/leap-stc/leap-stc.github.io/blob/fd69890ffc2f1871968e39b1c460370a0b3f98b3/book/_config.yml#L40-L51) in a PR. to add a picture and your personal data to the webpage. - [ ] Get access to the [Grafana Dashboard](https://grafana.leap.2i2c.cloud) - [ ] Request access to a service account to monitor Google Dataflow and Storage from the [Google Cloud Console](https://console.cloud.google.com/welcome?project=leap-pangeo) by raising an issue [here](https://github.com/leap-stc/data-and-compute-team/issues) - Instructions for admin: - Go to the Google Cloud Console > IAM > Grant Access - Add the following permissions: - `Dataflow Admin` - `Storage Admin` - `Logs Viewer` - `Monitoring Viewer` - `Logs Viewer` - `Compute Viewer` (onboarding.slack)= ### Relevant Slack channels - [`data-and-computation-team`](https://leap-nsf-stc.slack.com/archives/C065KPT1S4Q): Private channel for internal discussions, please contact the Manager for Data and Computing to be added. - [`leap-pangeo`](https://leap-nsf-stc.slack.com/archives/C02KB0DDB6E): Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. (onboarding.github)= ### Relevant github repos - [`leap-stc.github.io`](https://github.com/leap-stc/leap-stc.github.io): The source for the [LEAP-Pangeo technical documentation](https://leap-stc.github.io/intro.html). This also contains [LEAP-Pangeo Discussion Forum](https://github.com/leap-stc/leap-stc.github.io/discussions) - [`data-management`](https://github.com/leap-stc/data-management): Contains all the [pangeo-forge](https://pangeo-forge.readthedocs.io/en/latest/) based code/automation to ingest new datasets. Point users to the [issue tracker](https://github.com/leap-stc/data-management/issues/new/choose) to request new datasets. ## Regular Maintenance This section documents common tasks performed as part of the Data and Computation Team's duties. ### Monitor User Directory Useage The [user directories](reference.infrastructure.hub.user_dir) of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that \\<50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: - How much space is left on the [entire shared volume](https://grafana.leap.2i2c.cloud/d/hub-dashboard/jupyterhub-dashboard?orgId=1&viewPanel=23)? If the free space falls *below 20%* we need to discuss further actions. - [Which users are taking up more than the allowed 50GB](https://grafana.leap.2i2c.cloud/d/bd232539-52d0-4435-8a62-fe637dc822be/home-directory-usage-dashboard?orgId=1). For users who take up more than 50GB the suggested action is to reach out via slack or email. ### Regularly Updating the Software Environment We aim to provide users with [up-to-date default software environments](reference.infrastructure.hub.software_env). This currently requires to change the [pangeo-docker-images](https://github.com/pangeo-data/pangeo-docker-images) tag manually. - Make sure you are subscribed to release notifications on [pangeo-docker-images](https://github.com/pangeo-data/pangeo-docker-images) to recieve Github notification about new releases - To bump the version, submit a PR to [this config file](https://github.com/2i2c-org/infrastructure/blob/master/config/clusters/leap/common.values.yaml) in the [2i2c infrastructure repo](https://github.com/2i2c-org/infrastructure). In that PR you need to change the image tag for *all image choices*, see an example [here](https://github.com/2i2c-org/infrastructure/pull/3688). - To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the `generate_emails_token(github_action_mode=False)` function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap.pangeo@gmail.com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. ## Offboarding members - [] Delete personal `dct-team-<first_name>` service account in IAM (needs admin priviliges). ## Admin Tasks This part of the guide is reserved for team members with admin access to the `'leap-stc'` github organization! (guide.team.admin.renew_member_token)= ### Renewing Personal fine grained access token for LEAP member management In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via [this gh action](https://github.com/leap-stc/member_management/blob/main/.github/workflows/read_sheet.yaml)) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the `\"ORG_TOKEN\"` secret. The person creating the token will usually be the Manager for Data and Computation. :::\\{note} Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a 'dummy' user? ::: #### Steps - Make sure you have access to set secrets on the private [member_management repo](https://github.com/leap-stc/member_management) - Go to the personal account \"Settings>Developer Settings\" Tab. From there naviate to \"Personal Access Token>Fine-Grained tokens\" - If present click on \"LEAP member management token\", othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. - Generate or regenerate the token - The required permissions are \"Read and Write access to members\" and \"Read Access to actions and metadata\" - Set the expiration to a full year (the current limit set on the org level) - Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) - Go to the [member_management repo](https://github.com/leap-stc/member_management) and navigate to \"Settings > Secrets and Variables > Actions\" and open the \"ORG_TOKEN\" to edit - Paste the above token from the clipboard and save. - Run the [Member Add Action](https://github.com/leap-stc/member_management/actions/workflows/read_sheet.yaml) and confirm that it is successful - Close the token page and you are done! ### Handover Checklist for Admins The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. - [](guide.team.admin.renew_member_token) ### Moving Data between buckets using bare VMs In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the [](reference.infrastructrue.osn_pod)) it is recommended to use rclone on a VM :::\\{tip} These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of [skyplane](https://github.com/skyplane-project/skyplane) which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below ::: #### Manual spinup of cloud VMs for bulk data transfer Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. :::\\{warning} Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. ::: - Navigate to the [Google Cloud Console](https://console.cloud.google.com) and from there to \"Compute Engine\" and \"VM instances\" - Click on \"Create Instance\" - Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. - Choose a memorable name like \"boatymccloneface\" - Use a region that is close to your storage (for LEAP buckets this is `'us-central1'` and leave the zone on `'Any'` - Choose an `'E2'` machine type preset (here `e2-standard-8`) - In \"OS and Storage\" select the latest \"Ubuntu\" version as Operating System and \"Balanced persistent disk\" as Boot disk type. - Set the size to 20GB - Under \"Observability\" enable \"Install Ops Agent ...\" - (**Only needed when source location is on GCS**) Under \"Security\" change \"Access scopes\" to \"Set access for each API\", and set \"Storage\" to \"Read Only\". - (**Optional cost saving**) Under \"Advanced\" select `'VM provisioning model: Spot'` (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose \"On Demand\", but be aware that this will come at a higher cost). - **Optional but highly Recommended**: Under \"Advanced\" enable \"Set a time limit for the VM\", and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under \"On VM termination\". If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. - Click on \"Create\" - You should now be able to see your instance in the list under \"VM Instances\". Click the SSH button to tunnel into the VM - Install rclone with `sudo -v ; curl https://rclone.org/install.sh | sudo bash` - Start a tmux session with `tmux new` ([cheatsheet for tmux](https://tmuxcheatsheet.com/)) - Set the config via env variables one by one. The exact details might depend on your source/target storage. See the [rclone docs](https://rclone.org/docs/) for more details. This example copies from the LEAP gcs buckets to the OSN pod ``` export RCLONE_CONFIG_SOURCE_TYPE=gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH=true export RCLONE_CONFIG_TARGET_TYPE=s3 export RCLONE_CONFIG_TARGET_PROVIDER=Ceph export RCLONE_CONFIG_TARGET_ENDPOINT=https://nyu1.osn.mghpcc.org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID=XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY=XXX ``` - Run the transfer! `rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix` - Choosing `sync` here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). - The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. - Watch the transfer progress or work on something else ‚òïÔ∏è - You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run `tmux ls`. Pick whatever session you want to re-attach. Then do `tmux attach -d -t <session id>` to re-attach it to a new tmux instance and release it from the old one. - **Important. DO NOT SKIP!**: When your transfer is finished, go back to [Google Cloud Console](https://console.cloud.google.com) and from there to \"Compute Engine\" and \"VM instances\" and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! ## Non-Technical Admin Tasks This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). (guide.team.admin.member_signup_troubleshooting)= ### Member Sign Up Troubleshooting Ideally members should be signed on automatically and well ahead of any event (see [](reference.member_sign_up) for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. ![](../images/member_troubleshooting.png) #### A+G: Check if a user is in the Member Spreadsheet Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet **includes their github username**. Always make sure that the github name exists (search in github), and does not contain extra characters like `\"@\"`. #### B: Add Users to the Member Spreadsheet If this is not time sensitive, make sure that the user has completed the membership application (more details [here](reference.membership.tiers)) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. #### C+D+F: Checking Github Team Membership and invite status Repeat the following steps for all relevant Github Teams (you can find links to the team page [here](reference.membership.tiers)): - C: Navigate to the search bar at the top that says \"Find a member...\" and enter the **github username** of the user. If the name shows up the user is part of this github team - D: Right next to the search bar is a button that says \"... pending members\". Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. - F: **Accepting the invite has to be done by the user!**. Point them to our [FAQ's](faq.where_is_my_invite) for instructions on how to accept the invite. #### E: Manually rerunning github sign up action :::\\{attention} You need to have maintainer access to the private [leap-stc/member_management repo](https://github.com/leap-stc/member_management) in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. [bootcamp-instructor](https://github.com/orgs/leap-stc/teams/bootcamp-instructors)) that has access. ::: - Navigate to [leap-stc/member_management](https://github.com/leap-stc/member_management) - In the top rider click on \"Actions\" - On the left click on \"Parse Member Sheet and Add New Members\" - In the upper right corner, click on \"Run Workflow\", and again on \"Run Workflow\" in the pop up window. - After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). - If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the [](support.data_compute_team) #### H: Clearing Browser Cache etc If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary #### If none of this works :::\\{note} This is a very unlikely scenario and has very rarely happend! ::: Follow the flowchart and reach out to either the [](support.data_compute_team) or [2i2c support](https://docs.2i2c.org/support/). ### M¬≤LInES OSN pod administration All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the [Coldfront Portal](https://coldfront.osn.mghpcc.org/). Please log in with one of your affiliated organizations and make sure to *use the same one each time in case you have multiple affiliations*. The important units of division on the OSN pod are **projects** and **buckets**. Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have *access credentials* and a *storage quota*, both of which might need actions from an admin from time to time. (guide.team.admin.osnpod.check)= #### Check Bucket attributes To check individual buckets' attributes log into the [Coldfront Portal](https://coldfront.osn.mghpcc.org/), click on the relevant project, navigate to the \"Allocations\" section, find the bucket name in the \"Information\" column and click on the folder symbol in the \"Actions\" column. Scroll to the \"Allocation Attributes\" section. You can see all relevant values here. **OSN Anonymous Access**: If False, this data is public, no credentials are needed to read data (writing still requires credentials). **OSN Bucket Quota (TB)**: Shows the currently allocated size. This is the max size, not what is actuall used! **OSN RO/RW Bucket Access/Secret Key**: Credentials for read-only (RO) and read-write (RW) access to the bucket. See [](reference.infrastructrue.osn_pod.credentials) for more details. #### Share bucket credentials :::\\{attention} Some buckets are not meant to be accessible for write by users! Please always refer to [](reference.infrastructrue.osn_pod.organization) and only give access to project specific buckets and the `'leap-pangeo-inbox'` bucket to non-admins. ::: - Navigate to the specific bucket you want to share credentials to (see [above](guide.team.admin.osnpod.check) for detailed steps) - Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. #### Increasing Storage Quota If any of the buckets needs more storage space, follow these steps: - Log into [Coldfront](https://coldfront.osn.mghpcc.org/) - Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) - Scroll to \"Allocations\" and find your bucket in the \"Information\" column. Click on the folder icon in the corresponding \"Actions\" column. - In the top right click on \"Request Change\" and scroll down to \"Allocation Attributes\". Enter the desired new size in TB in the \"Request New Value\" column and the \"OSN Bucket Quota (TB)\" row and enter a short justification (required). - Click the \"Submit\" button. - You should see a green box with \"Allocation change request successfully submitted. \" at the top of the next page. - Wait for email confirmation of the change. #### Provision a new Bucket - Log into [Coldfront](https://coldfront.osn.mghpcc.org/) - Navigate to an existing Project (or create a new one; see below), scroll to \"Allocations\" and click on \"Request Resource Allocation\" - In the following dialog chose \"OSN Bucket (Storage)\", write a short justification which **includes whether you want the bucket to have anonymous (public) access!**, and choose a size in TB. - Click \"Submit\" - Wait for email confirmation. #### Create a new Project :::\\{note} You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. ::: - Log into [Coldfront](https://coldfront.osn.mghpcc.org/) - Click on the \"Projects\" link on the homepage - Click \"Add a Project\" - Choose a title, write a short description of the project, and optionally choose a field of science. - Then click \"Save\" - Wait for email confirmation. #### OSN Support For any questions/functionality not mentioned above, please refer to the [OSN documentation](https://coldfront.osn.mghpcc.org/static/osndocs/index.html) or reach out to the OSN support (`'help@osn.mghpcc.org'`) ### Google Cloud Account Maintenance :::\\{attention} To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. ::: All of the following steps are done from the [Google Cloud Console](https://console.cloud.google.com) #### Billing - Click on the 'burger' symbold on the top-left, and navigate to Billing or search for \"Billing\" in the bar on the top. ##### Check Running Costs (recommended weekly) - Click on \"Reports\" on the left. - From the first drop down on the right, select \"Last 90 days\" (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable \"Promotions & others\" to see the amount spent from credits. - You should now see 3 bars in the center that indicate the monthly spending split into \"Services\" (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. ##### Check remaining credits (recommended monthly) - Click on \"Credits\" on the left to see current and past cloud credits including the total amount, and remaining value. - Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#onboarding",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#checklist-for-new-members",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#relevant-slack-channels",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#relevant-github-repos",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#regular-maintenance",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#monitor-user-directory-useage",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#regularly-updating-the-software-environment",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#offboarding-members",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#admin-tasks",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#renewing-personal-fine-grained-access-token-for-leap-member-management",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#steps",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#handover-checklist-for-admins",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#moving-data-between-buckets-using-bare-vms",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#manual-spinup-of-cloud-vms-for-bulk-data-transfer",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#non-technical-admin-tasks",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#member-sign-up-troubleshooting",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#a-g-check-if-a-user-is-in-the-member-spreadsheet",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#b-add-users-to-the-member-spreadsheet",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#c-d-f-checking-github-team-membership-and-invite-status",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#e-manually-rerunning-github-sign-up-action",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#h-clearing-browser-cache-etc",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#if-none-of-this-works",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#m2lines-osn-pod-administration",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#check-bucket-attributes",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#share-bucket-credentials",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#increasing-storage-quota",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#provision-a-new-bucket",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#create-a-new-project",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#osn-support",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#google-cloud-account-maintenance",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#billing",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#check-running-costs-recommended-weekly",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#check-remaining-credits-recommended-monthly",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#guide-for-data-and-computation-team-members",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#onboarding-slack",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#onboarding-github",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#guide-team-admin-renew-member-token",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/membership.html#reference-member-sign-up",
    "transcript": "Membership ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership Contents Code of Conduct Membership Tiers Administrator and Developer Category Github Teams and Resources Applying for membership Termination of Access Offboarding Process Member Sign Up Procedure Membership ¬∂ Access to LEAP-Pangeo is managed via membership tiers and associated github organization teams. Code of Conduct ¬∂ All users of LEAP-Pangeo must abide by the LEAP Code of Conduct . Membership Tiers ¬∂ The membership tiers are listed in ascending order of access/privileges. PUBLIC MEMBERSHIP(Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHE MEMBERSHIP . Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. For more info see the main LEAP website Administrator and Developer Category ¬∂ The LEAP Manager of Data and Computing may grant access to other participants for the purposes of technical development, debugging, and evaluation of the platform. These members will be added to the leap-pangeo-full-access team to have full access to all resources. Github Teams and Resources ¬∂ The access to the JupyterHub is implemented via Github Teams in the leap-stc GitHub organization. Each Membership tier maps to a single Github Team which determines the resources available to the user. Tier Github Team Resources Available Membership Valid for PUBLIC leap-pangeo-public-access üöß üöß EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. üöß RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. üöß Applying for membership ¬∂ To become a LEAP member please use the Application Form . Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed. Termination of Access ¬∂ Users who violate usage policies will have their access suspended pending investigation. The LEAP Director of Data and Computing decides if a policy has been violated and may suspend or terminate access to LEAP-Pangeo at any time. Offboarding Process ¬∂ Users may also be transferred from e.g. the Education Category to the Community Category when their termed access ends. Removing a user from the corresponding team is sufficient to disable their access to those resources. Removing a user from the leap-pangeo-users group entirely will disable their access completely. An automated process will delete user data from the hub one month after a user is removed from the leap-pangeo-users group. Member Sign Up Procedure ¬∂ All data relevant to LEAP membership is centrally processed in a spreadsheet with access limited to LEAP staff. To allow LEAP members access to the JupyterHub (and the associated storage and compute resources; all managed by 2i2c) the Data and Computation Team maintains a set of Github Actions (the member management repository is private) that parse the relevant data like github usernames and membership expiration, and add users to the appropriate teams . For normal operations users should apply and wait a few days until they are approved, entered, and signed up for access. For urgent situations (like events, classes) that require expedited sign up please refer to Member Sign Up Troubleshooting . previous Infrastructure next Education By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructrue-osn-pod-credentials",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/infrastructure.html#reference-infrastructrue-osn-pod-organization",
    "transcript": "Infrastructure ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure Contents LEAP-Pangeo Data Catalog LEAP-Pangeo JupyterHub Server Managing Servers Your User Directory The Software Environment Preselected Images Custom Images Installing additonal packages Cloud Storage m2lines OSN Pod OSN Pod Organization Credentials LEAP-Pangeo Cloud Storage Buckets Compute Access to LEAP-Pangeo resources without the JupyterHub Temporary Token Persistent Access via Google Groups Service Account Analysis-Ready Cloud-Optimized (ARCO) Data Zarr Pangeo-Forge Infrastructure ¬∂ LEAP-Pangeo Data Catalog ¬∂ Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan For more explanation about the catalog, and its role in the overall vision of LEAP, see LEAP-Pangeo Architecture . See Ingesting Datasets into Cloud Storage for details on how to ingest data and link it into the catalog. LEAP-Pangeo JupyterHub ¬∂ Our team has a cloud-based JupyterHub . For information who can access the hub with which privileges, please refer to Membership Tiers . Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud us-central1 Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap This document goes over the primary technical details of the JupyterHub. For a quick tutorial on basic usage, please check out our Getting Started tutorial. To get an in-depth overview of the LEAP Pangeo Architecture and how the JupyterHub fits into it, please see the Architecture page. Server ¬∂ Managing Servers ¬∂ You can start and stop your server (and even open multiple named servers) from the Hub Control Panel . You can get to the hub control panel by navigating to https://leap.2i2c.cloud/hub/home in your browser or navigating to File > Hub Control Panel from the JupyterLab Interface. Your User Directory ¬∂ When you open your hub, you can navigate to the ‚ÄúFile Browser‚Äù and see all the files in your User Directory Your User Directory behaves very similar to a filestystem on your computer. If you save a file from a notebook, you will see it appear in the File Browser (you might have to wait a few seconds or press refresh) and you can use a terminal to navigate the terminal as you would on a UNIX machine: Note As shown in the picture above, every user will see '/home/jovyan' as their root directory. This is different from many HPC accounts where your home directory will point to a directory with your username. But the functionality is similar. These are your own files and they cannot be seen/modified by other users (except admins). The primary purpose of this directory is to store small files, like github repositories and other code. Warning To accommodate the expanding LEAP community, the data and compute team has instituted a storage quota on individual user directories /home/jovyan . Your home directory is intended only for notebooks, analysis scripts, and small datasets (< 1 GB). It is not an appropriate place to store large datasets. Unlike the cloud buckets, these directories use an underlying storage with a rigid limit. If a single user fills up the space, the Hub crashes for everyone. We recommend users use less than 25GB and enforce a hard limit of 50GB. Users who persistently violate the limit may temporarily get reduced cloud access . To check how much space you are using in your home directory open a terminal window on the hub and run du -h --max-depth=1 ~/ | sort -h . If you want to save larger files for your work use our LEAP-Pangeo Cloud Storage Buckets and consult our Hub Data Guide . See the FAQs for guidance on reducing storage. The Software Environment ¬∂ The software environment you encounter on the Hub is based upon docker images which you can run on other machines (like your laptop or an HPC cluster) for better reproducibility. Upon start up you can choose between A list of preselected images The option of passing a custom docker image via the \"Other...\" option. Preselected Images ¬∂ LEAP-Pangeo uses several full-featured, up-to-date Python environments maintained by Pangeo. You can read all about them at the following URL: https://github.com/pangeo-data/pangeo-docker-images/ There are separate images for pytorch and tensorflow which are available in a drop-down panel when starting up your server. The Hub contains a specific version of the image which can be found here . For example, at the time of writing, the version of pangeo-notebook is 2022.05.10 . A complete list of all packages installed in this environment is located at: https://github.com/pangeo-data/pangeo-docker-images/blob/2022.05.10/pangeo-notebook/packages.txt Attention We regularly update the version of the images provided in the drop-down menu. To ensure full reproducibility you should save the full info of the image you worked with (this is stored in the environment variable JUPYTER_IMAGE_SPEC ) with your work. You could for example print the following in the first cell of a notebook: import os print ( os . environ [ \"JUPYTER_IMAGE_SPEC\" ]) Copy to clipboard You can then use that string with the custom images to reproduce your work with exactly the same environment. Custom Images ¬∂ If you select the Image > Other... Option during server login you can paste an arbitrary reference in the form of docker_registry/organization/image_name:image_version . As an example we can get the 2023.05.08 version of the pangeo tensorflow notebook by pasting quay.io/pangeo/ml-notebook:2023.05.08 . If you want to build your own docker image for your project, take a look at this template and the instructions to learn how to use repo2docker to set up CI workflows to automatically build docker images from your repository. Installing additonal packages ¬∂ You can install additional packages using pip and conda . However, these will disappear when your server shuts down. For a more permanent solution we recommend building project specific dockerfiles and using those as custom images . Cloud Storage ¬∂ m2lines OSN Pod ¬∂ OSN Pod Organization ¬∂ The ~1PB storage on the OSN Pod can be customized into Projects and Buckets. Projects are used to give additional users access to the Coldfront Admin Console, whereas buckets are how storage is administered up on the Pod. A project can have multiple buckets. There are currently 3 principal Projects on the Pod: 'leap-pangeo' : Used for Data Ingestion across the m2lines and LEAP community Buckets: 'leap-pangeo-manual' : No write access for users 'leap-pangeo-pipeline' : No write access for users 'leap-pangeo-inbox' : Write access can be shared with users who want to add data e.g. from an HPC center 'm2lines' : Used for project data and publications from the m2lines project Buckets: 'm2lines-pubs' : No write access for users ‚Ä¶ various project buckets 'leap' : Used for project data and publications from the LEAP project Buckets: 'leap-pubs' : No write access for users ‚Ä¶ various project buckets Credentials ¬∂ Warning All OSN credentials are long lived and should be treated as such. Please do not share them publicly (e.g. in your notebook or a github repository) and when sharing with e.g. collaborators use an encrypted way of sharing (e.g. password manager). Credentials for the OSN Pod are specific to each bucket. There are two types of credentials: ‚ÄúRead-only‚Äù and ‚ÄúRead-Write‚Äù. Exercise caution when sharing/saving secrets, particularly the latter. Each type of credentials consists of two keys (access + secret). Both are required to access the bucket, and they are shared by the OSN Admin. LEAP-Pangeo Cloud Storage Buckets ¬∂ LEAP-Pangeo provides users two cloud buckets to store data. Your Server is automatically authenticated to read from any of these buckets but write access might differ (see below). See Access to LEAP-Pangeo resources without the JupyterHub for details on how to access buckets from ‚Äòoutside‚Äô the JupyterHub. gs://leap-scratch/ - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. More info gs://leap-persistent/ - Persistent Storage. Use this bucket for storing results you want to share with other members. gs://leap-persistent-ro/ - Persistent Storage with read-only access for most users. To upload data to this bucket you need to use this method below. Files stored on each of those buckets can be accessed by any LEAP member, so be concious in the way you use these. Do not put sensitive information (passwords, keys, personal data) into these buckets! When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/funky-user/‚Äô). Compute ¬∂ üöß Access to LEAP-Pangeo resources without the JupyterHub ¬∂ Temporary Token ¬∂ You can generate a temporary (1 hour) token with read/write access as follows: Now start up a LEAP-Pangeo server and open a terminal. Install the Google Cloud SDK using mamba mamba install google-cloud-sdk Copy to clipboard Now you can generate a temporary token (valid for 1 hour) that allows you to upload data to the cloud. gcloud auth print-access-token Copy to clipboard This will print a temporary token in the terminal. You can e.g. copy that to your clipboard. Persistent Access via Google Groups ¬∂ We manage access rights through Google Groups . Please contact the Data and Computation Team to get added to the appropriate group (a gmail address is required for this). Service Account ¬∂ If you want more permanent access to resources, e.g. as part of a repositories CI using a service account, please reach out to the Data and Computation Team to discuss options. Analysis-Ready Cloud-Optimized (ARCO) Data ¬∂ Below you can find some examples of ARCO data formats Zarr ¬∂ zarr Pangeo-Forge ¬∂ You can find more information about Pangeo-Forge here . previous LEAP-Pangeo Infrastructure Policy next Membership By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/team_guide.html#guide-team-admin-osnpod-check",
    "transcript": "Guide for Data and Computation Team Members ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members Contents Onboarding Checklist for new members Relevant Slack channels Relevant github repos Regular Maintenance Monitor User Directory Useage Regularly Updating the Software Environment Offboarding members Admin Tasks Renewing Personal fine grained access token for LEAP member management Steps Handover Checklist for Admins Moving Data between buckets using bare VMs Manual spinup of cloud VMs for bulk data transfer Non-Technical Admin Tasks Member Sign Up Troubleshooting A+G: Check if a user is in the Member Spreadsheet B: Add Users to the Member Spreadsheet C+D+F: Checking Github Team Membership and invite status E: Manually rerunning github sign up action H: Clearing Browser Cache etc If none of this works M¬≤LInES OSN pod administration Check Bucket attributes Share bucket credentials Increasing Storage Quota Provision a new Bucket Create a new Project OSN Support Google Cloud Account Maintenance Billing Check Running Costs (recommended weekly) Check remaining credits (recommended monthly) Guide for Data and Computation Team Members ¬∂ This is a short write up facilitate the spin up of new team members for the Data and Computation Team and describe regular maintenance tasks. Onboarding ¬∂ Checklist for new members ¬∂ Ask to be added to the Data and Computation Github team Ask to be added to the @data-and-compute Slack user group Subscribe to Relevant Slack channels Consider enabling notifications for Relevant github repos Make a PR to the _config.yaml file here in a PR. to add a picture and your personal data to the webpage. Get access to the Grafana Dashboard Request access to a service account to monitor Google Dataflow and Storage from the Google Cloud Console by raising an issue here Instructions for admin: Go to the Google Cloud Console > IAM > Grant Access Add the following permissions: Dataflow Admin Storage Admin Logs Viewer Monitoring Viewer Logs Viewer Compute Viewer Relevant Slack channels ¬∂ data-and-computation-team : Private channel for internal discussions, please contact the Manager for Data and Computing to be added. leap-pangeo : Community wide channel where LEAP-Pangeo users can ask questions. Members of the team should join the channel and regularly engage with issues raised. Relevant github repos ¬∂ leap-stc.github.io : The source for the LEAP-Pangeo technical documentation . This also contains LEAP-Pangeo Discussion Forum data-management : Contains all the pangeo-forge based code/automation to ingest new datasets. Point users to the issue tracker to request new datasets. Regular Maintenance ¬∂ This section documents common tasks performed as part of the Data and Computation Team‚Äôs duties. Monitor User Directory Useage ¬∂ The user directories of all members are on a shared volume of fixed size. We pay for the total size of the volume no matter if it is used or not, so we should strive to keep usage to a minimum. Our current policy is that <50GB per user is acceptable. Unfortunately we do not have a technical way to implement per user quotas, so we need to regularly check two things: How much space is left on the entire shared volume ? If the free space falls below 20% we need to discuss further actions. Which users are taking up more than the allowed 50GB . For users who take up more than 50GB the suggested action is to reach out via slack or email. Regularly Updating the Software Environment ¬∂ We aim to provide users with up-to-date default software environments . This currently requires to change the pangeo-docker-images tag manually. Make sure you are subscribed to release notifications on pangeo-docker-images to recieve Github notification about new releases To bump the version, submit a PR to this config file in the 2i2c infrastructure repo . In that PR you need to change the image tag for all image choices , see an example here . To send emails, a token file is setup as the OAUTH_GMAIL_CREDENTIALS Github Secret in the member management repo; every so often (around biweekly) the action will require re-authentication, generating a new token that should replace the existing secret. This makes use of OAUTH_GMAIL_CLIENT_SECRET, which never needs to change. To update the OAUTH_GMAIL_CREDENTIALS secret, run the generate_emails_token(github_action_mode=False) function from utils.py locally, which will direct you to a confirmation screen. Log in to the leap . pangeo @ gmail . com account and authorize access. You will require a copy of the CLIENT_SECRETS file on your personal machine which can be retreived from Google Cloud Console from the above pangeo support email. Offboarding members ¬∂ [] Delete personal dct-team-<first_name> service account in IAM (needs admin priviliges). Admin Tasks ¬∂ This part of the guide is reserved for team members with admin access to the 'leap-stc' github organization! Renewing Personal fine grained access token for LEAP member management ¬∂ In order to automate member sign up by adding github users from a private Google Sheet to the appropriate github teams (via this gh action ) the github action needs the appropriate priviliges to add/remove members from teams. We are currently handling this by providing a personal access token as the \"ORG_TOKEN\" secret. The person creating the token will usually be the Manager for Data and Computation. Note Ideally we want to remove the dependency on a single user account here, but for now this is the only way I have found this to work properly. Maybe there is a way to establish a ‚Äòdummy‚Äô user? Steps ¬∂ Make sure you have access to set secrets on the private member_management repo Go to the personal account ‚ÄúSettings>Developer Settings‚Äù Tab. From there naviate to ‚ÄúPersonal Access Token>Fine-Grained tokens‚Äù If present click on ‚ÄúLEAP member management token‚Äù, othewise create a new token with that name (the actual name is optional here, but make sure to name it in a memorable way), and authenticate. Generate or regenerate the token The required permissions are ‚ÄúRead and Write access to members‚Äù and ‚ÄúRead Access to actions and metadata‚Äù Set the expiration to a full year (the current limit set on the org level) Make sure to copy the token (leave the page open until the next step is completed, since you will have to recreate the token once the page is closed!) Go to the member_management repo and navigate to ‚ÄúSettings > Secrets and Variables > Actions‚Äù and open the ‚ÄúORG_TOKEN‚Äù to edit Paste the above token from the clipboard and save. Run the Member Add Action and confirm that it is successful Close the token page and you are done! Handover Checklist for Admins ¬∂ The following is a list of tasks that should be done by any new hire in the Data and Computation Manager position to ensure smooth operations. Renewing Personal fine grained access token for LEAP member management Moving Data between buckets using bare VMs ¬∂ In general you need some form of compute to move data between different object store locations, but be aware that the data will be always be streamed to and from that location over the internet, so fast connection speed is key for fast transfers. There are a variety of ways to move data with perhaps the easiest being to run fsspec or rclone on your local computer, but speed is likely limited by your local internet connection. For certain tasks (e.g. moving data to admin only publishing buckets on the m2lines OSN Pod ) it is recommended to use rclone on a VM Tip These instructions should be easy to adapt to VM instances on other clouds, and can likely be automated to a much larger degree, but this is what has worked so far. Ultimately this approach is a somewhat manual implementation of the concept of skyplane which sadly does not seem to be actively maintained anymore. As of the writing of these docs we were able to achieve ~700MB/s transfer speeds with a single VM following the instructions below Manual spinup of cloud VMs for bulk data transfer ¬∂ Following these instructions requires permissions on the LEAP Google Cloud Account. Contact an admin if you run into permission issues. Warning Using VMs this way does not automatically delete instances. Make sure to do that when your transfer is done. Navigate to the Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù Click on ‚ÄúCreate Instance‚Äù Configure your VM instance (this is an example config that worked well in the past, you can modify as needed). If not specified below leave all settings on the default. Choose a memorable name like ‚Äúboatymccloneface‚Äù Use a region that is close to your storage (for LEAP buckets this is 'us-central1' and leave the zone on 'Any' Choose an 'E2' machine type preset (here e2-standard-8 ) In ‚ÄúOS and Storage‚Äù select the latest ‚ÄúUbuntu‚Äù version as Operating System and ‚ÄúBalanced persistent disk‚Äù as Boot disk type. Set the size to 20GB Under ‚ÄúObservability‚Äù enable ‚ÄúInstall Ops Agent ‚Ä¶‚Äù ( Only needed when source location is on GCS ) Under ‚ÄúSecurity‚Äù change ‚ÄúAccess scopes‚Äù to ‚ÄúSet access for each API‚Äù, and set ‚ÄúStorage‚Äù to ‚ÄúRead Only‚Äù. ( Optional cost saving ) Under ‚ÄúAdvanced‚Äù select 'VM provisioning model: Spot' (this means the instance can shut down at any time, and you will have to rerun these steps to pick up the transfer. If you want the job to finish guaranteed, choose ‚ÄúOn Demand‚Äù, but be aware that this will come at a higher cost). Optional but highly Recommended : Under ‚ÄúAdvanced‚Äù enable ‚ÄúSet a time limit for the VM‚Äù, and limit it to the number of hours you expect the transfer to take. You can choose to either stop or delete the VM under ‚ÄúOn VM termination‚Äù. If you choose stop you will keep incurring costs for the storage volume, so unless you expect to restart the instance, choose delete here. Click on ‚ÄúCreate‚Äù You should now be able to see your instance in the list under ‚ÄúVM Instances‚Äù. Click the SSH button to tunnel into the VM Install rclone with sudo -v ; curl https://rclone.org/install.sh | sudo bash Start a tmux session with tmux new ( cheatsheet for tmux ) Set the config via env variables one by one. The exact details might depend on your source/target storage. See the rclone docs for more details. This example copies from the LEAP gcs buckets to the OSN pod export RCLONE_CONFIG_SOURCE_TYPE = gcs export RCLONE_CONFIG_SOURCE_ENV_AUTH = true export RCLONE_CONFIG_TARGET_TYPE = s3 export RCLONE_CONFIG_TARGET_PROVIDER = Ceph export RCLONE_CONFIG_TARGET_ENDPOINT = https : // nyu1 . osn . mghpcc . org export RCLONE_CONFIG_TARGET_ACCESS_KEY_ID = XXX export RCLONE_CONFIG_TARGET_SECRET_ACCESS_KEY = XXX Copy to clipboard Run the transfer! rclone sync --fast-list --s3-chunk-size 128M --s3-upload-concurrency 128 --transfers 128 --checkers 256 -P source:leap-persistent/some/prefix/ target:osn-bucket-name/another/prefix Choosing sync here enables you to restart a transfer if it failed (e.g. due to a spot instance being shut down, or the transfer taking longer than expected). The additional flags passed here seem to work well for past transfers, but they might be tuned for better performance in various scenarios. Watch the transfer progress or work on something else ‚òïÔ∏è You might get disconnected from the SSH browser window after a while (this is why we run the process within tmux!). Simple click on SSH again and run tmux ls . Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one. Important. DO NOT SKIP! : When your transfer is finished, go back to Google Cloud Console and from there to ‚ÄúCompute Engine‚Äù and ‚ÄúVM instances‚Äù and click the three dots to the right of your instance, and delete it. If you forget about this LEAP will keep paying for the instance! Non-Technical Admin Tasks ¬∂ This section describes admin tasks that are necessary for the maintenance of LEAP-Pangeo components (including collaborative efforts lead M¬≤LInES) which require appropriate permissions, but no coding (everything can be achieved on one of several websites). Member Sign Up Troubleshooting ¬∂ Ideally members should be signed on automatically and well ahead of any event (see Member Sign Up Procedure for an overview of our member sign up mechanics). But despite the best efforts situations arise where either staff or the organizers/instructors of an event need to quickly sign on new members, and troubleshoot if certain users do not have access to the JupyterHub. Follow the steps in this Flowchart to quickly resolve any issues in such cases. A+G: Check if a user is in the Member Spreadsheet ¬∂ Request read access or inquire with LEAP staff to confirm that the user(s) who are having trouble, are listed in the Member Data Spreadsheet and make sure the the sheet includes their github username . Always make sure that the github name exists (search in github), and does not contain extra characters like \"@\" . B: Add Users to the Member Spreadsheet ¬∂ If this is not time sensitive, make sure that the user has completed the membership application (more details here ) and inquire with LEAP staff about the status of membership (adding users might take a while, thus always sign up users well in advance). In a time sensitive situation contact LEAP staff to expedite the addition of new users added to the Member Spreadsheet. C+D+F: Checking Github Team Membership and invite status ¬∂ Repeat the following steps for all relevant Github Teams (you can find links to the team page here ): C: Navigate to the search bar at the top that says ‚ÄúFind a member‚Ä¶‚Äù and enter the github username of the user. If the name shows up the user is part of this github team D: Right next to the search bar is a button that says ‚Äú‚Ä¶ pending members‚Äù. Click on that button and scroll down the list. If the username is in that list the member has received an invite, but not yet accepted it. F: Accepting the invite has to be done by the user! . Point them to our FAQ‚Äôs for instructions on how to accept the invite. E: Manually rerunning github sign up action ¬∂ Attention You need to have maintainer access to the private leap-stc/member_management repo in order to follow these steps. If you cannot repeat these steps please ask one of the github organization admins to be added to an appropriate team (e.g. bootcamp-instructor ) that has access. Navigate to leap-stc/member_management In the top rider click on ‚ÄúActions‚Äù On the left click on ‚ÄúParse Member Sheet and Add New Members‚Äù In the upper right corner, click on ‚ÄúRun Workflow‚Äù, and again on ‚ÄúRun Workflow‚Äù in the pop up window. After a short while you should see a yellow circle in the main window, indicating that the github action is in progress. Wait until the action is finished (usually 3-5 minutes). If the circle turns green, you are done. If the circle turns red, try to run it one more time to exclude any random issues. If the error persists, reach out to a member of the Data and Computation Team H: Clearing Browser Cache etc ¬∂ If users are part of a github team, and still have trouble signing on, instruct them to clear their browser cache, or try a different browser. This step is unlikely to be necessary If none of this works ¬∂ Note This is a very unlikely scenario and has very rarely happend! Follow the flowchart and reach out to either the Data and Computation Team or 2i2c support . M¬≤LInES OSN pod administration ¬∂ All administrative tasks pertaining to the M¬≤LInES OSN bucket are handled via the Coldfront Portal . Please log in with one of your affiliated organizations and make sure to use the same one each time in case you have multiple affiliations . The important units of division on the OSN pod are projects and buckets . Each project can have multiple buckets, and you can give others access as guests and admins on a per project basis. Buckets are how the actual storage space is organized, and each bucket will have access credentials and a storage quota , both of which might need actions from an admin from time to time. Check Bucket attributes ¬∂ To check individual buckets‚Äô attributes log into the Coldfront Portal , click on the relevant project, navigate to the ‚ÄúAllocations‚Äù section, find the bucket name in the ‚ÄúInformation‚Äù column and click on the folder symbol in the ‚ÄúActions‚Äù column. Scroll to the ‚ÄúAllocation Attributes‚Äù section. You can see all relevant values here. OSN Anonymous Access : If False, this data is public, no credentials are needed to read data (writing still requires credentials). OSN Bucket Quota (TB) : Shows the currently allocated size. This is the max size, not what is actuall used! OSN RO/RW Bucket Access/Secret Key : Credentials for read-only (RO) and read-write (RW) access to the bucket. See Credentials for more details. Share bucket credentials ¬∂ Attention Some buckets are not meant to be accessible for write by users! Please always refer to OSN Pod Organization and only give access to project specific buckets and the 'leap-pangeo-inbox' bucket to non-admins. Navigate to the specific bucket you want to share credentials to (see above for detailed steps) Copy the relevant Access and Secret keys (either RO or RW depending on the desired use) and share them with the relevant users e.g. by pasting them into a password manager and sharing an authenticated link. Increasing Storage Quota ¬∂ If any of the buckets needs more storage space, follow these steps: Log into Coldfront Navigate to the project that contains the bucket (we currently separate projects for M¬≤LInES, LEAP, and the LEAP-Pangeo Data ingestion) Scroll to ‚ÄúAllocations‚Äù and find your bucket in the ‚ÄúInformation‚Äù column. Click on the folder icon in the corresponding ‚ÄúActions‚Äù column. In the top right click on ‚ÄúRequest Change‚Äù and scroll down to ‚ÄúAllocation Attributes‚Äù. Enter the desired new size in TB in the ‚ÄúRequest New Value‚Äù column and the ‚ÄúOSN Bucket Quota (TB)‚Äù row and enter a short justification (required). Click the ‚ÄúSubmit‚Äù button. You should see a green box with ‚ÄúAllocation change request successfully submitted. ‚Äù at the top of the next page. Wait for email confirmation of the change. Provision a new Bucket ¬∂ Log into Coldfront Navigate to an existing Project (or create a new one; see below), scroll to ‚ÄúAllocations‚Äù and click on ‚ÄúRequest Resource Allocation‚Äù In the following dialog chose ‚ÄúOSN Bucket (Storage)‚Äù, write a short justification which includes whether you want the bucket to have anonymous (public) access! , and choose a size in TB. Click ‚ÄúSubmit‚Äù Wait for email confirmation. Create a new Project ¬∂ Note You need PI status on the pod to create new projects. Reach out to the M¬≤LInES admin to discuss this if you do not have access yet. Log into Coldfront Click on the ‚ÄúProjects‚Äù link on the homepage Click ‚ÄúAdd a Project‚Äù Choose a title, write a short description of the project, and optionally choose a field of science. Then click ‚ÄúSave‚Äù Wait for email confirmation. OSN Support ¬∂ For any questions/functionality not mentioned above, please refer to the OSN documentation or reach out to the OSN support ( 'help@osn.mghpcc.org' ) Google Cloud Account Maintenance ¬∂ Attention To follow these steps you need to have the appropriate priviliges on the LEAP cloud account/organization. Reach out to an organization owner if you need access. All of the following steps are done from the Google Cloud Console Billing ¬∂ Click on the ‚Äòburger‚Äô symbold on the top-left, and navigate to Billing or search for ‚ÄúBilling‚Äù in the bar on the top. Check Running Costs (recommended weekly) ¬∂ Click on ‚ÄúReports‚Äù on the left. From the first drop down on the right, select ‚ÄúLast 90 days‚Äù (you can also select a custom time frame if you want to see longer term costs). Further Down on the right disable ‚ÄúPromotions & others‚Äù to see the amount spent from credits. You should now see 3 bars in the center that indicate the monthly spending split into ‚ÄúServices‚Äù (e.g. compute, storage). You can further refine this partition with the options on the right, but this overview should generally be sufficient for routine cost monitoring e.g. to detect abnormally high spending. Check remaining credits (recommended monthly) ¬∂ Click on ‚ÄúCredits‚Äù on the left to see current and past cloud credits including the total amount, and remaining value. Make sure that enough credits are available for continued operation (based on the monthly costs). Contact Google to arrange for release of next credit round if funds run low. previous Getting access to Cloud VMs next FAQs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/vm_access.md",
    "transcript": "# Getting access to Cloud VMs ```{admonition} Important --- class: important --- This documentation should be considered highly experimental and permissions to follow these instructions are only given to a small subset of testers at the moment. ``` ## Why? We want to test e.g. containerized workflows that run climate simulations (for online testing of ML parametrizations). For that we need a 'bare' VM, and not the JupyterLab interface. ## Admin considerations To fully separate this testing from the Hub etc we have created a separate project in Gcloud (same billing account and Org). To keep an eye on (e.g. accidental) cost overrun, I added a budget alert (in the linked billing account) that applies to all resources for this new project, and set a 500\\$ alert with notifications at various thresholds. New users need the following IAM roles (click \"Grant Access\" with the email as principals): - \"Compute Admin\" - \"Service Account User\" ## How to get access Send your email (private gmail preferred) to Julius Busecke ## Log into google cloud console and start a small test VM - Use the email you sent above to sign into https://console.cloud.google.com - Make sure to chose the project \"ClimSimTesting\" ![](../images/vm_access_project.png) - Now navigate to \"Compute Engine\" and click on \"Create New Instance\". Chose a recognizable name (e.g. `climsim_<your_name>_test` and leave everything else on default for now and click \"Create\" at the bottom. - After a short spin up you should see your VM instance with a green checkmark. - Try to SSH into it via the cloud console (click on SSH on the far right). - Let me know on slack that you were able to create an instance - **Very Important**: Delete your instance after the test. **You should do this everytime you are done using the VM to avoid large costs**. Click on the triple dots on the right, and chose \"Delete\". ![](../images/vm_access_delete.png)"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/vm_access.html#why",
    "transcript": "Getting access to Cloud VMs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs ¬∂ Important This documentation should be considered highly experimental and permissions to follow these instructions are only given to a small subset of testers at the moment. Why? ¬∂ We want to test e.g. containerized workflows that run climate simulations (for online testing of ML parametrizations). For that we need a ‚Äòbare‚Äô VM, and not the JupyterLab interface. Admin considerations ¬∂ To fully separate this testing from the Hub etc we have created a separate project in Gcloud (same billing account and Org). To keep an eye on (e.g. accidental) cost overrun, I added a budget alert (in the linked billing account) that applies to all resources for this new project, and set a 500$ alert with notifications at various thresholds. New users need the following IAM roles (click ‚ÄúGrant Access‚Äù with the email as principals): ‚ÄúCompute Admin‚Äù ‚ÄúService Account User‚Äù How to get access ¬∂ Send your email (private gmail preferred) to Julius Busecke Log into google cloud console and start a small test VM ¬∂ Use the email you sent above to sign into https://console.cloud.google.com Make sure to chose the project ‚ÄúClimSimTesting‚Äù Now navigate to ‚ÄúCompute Engine‚Äù and click on ‚ÄúCreate New Instance‚Äù. Chose a recognizable name (e.g. climsim_<your_name>_test and leave everything else on default for now and click ‚ÄúCreate‚Äù at the bottom. After a short spin up you should see your VM instance with a green checkmark. Try to SSH into it via the cloud console (click on SSH on the far right). Let me know on slack that you were able to create an instance Very Important : Delete your instance after the test. You should do this everytime you are done using the VM to avoid large costs . Click on the triple dots on the right, and chose ‚ÄúDelete‚Äù. previous Bootcamp Guide next Guide for Data and Computation Team Members By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/vm_access.html#admin-considerations",
    "transcript": "Getting access to Cloud VMs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs ¬∂ Important This documentation should be considered highly experimental and permissions to follow these instructions are only given to a small subset of testers at the moment. Why? ¬∂ We want to test e.g. containerized workflows that run climate simulations (for online testing of ML parametrizations). For that we need a ‚Äòbare‚Äô VM, and not the JupyterLab interface. Admin considerations ¬∂ To fully separate this testing from the Hub etc we have created a separate project in Gcloud (same billing account and Org). To keep an eye on (e.g. accidental) cost overrun, I added a budget alert (in the linked billing account) that applies to all resources for this new project, and set a 500$ alert with notifications at various thresholds. New users need the following IAM roles (click ‚ÄúGrant Access‚Äù with the email as principals): ‚ÄúCompute Admin‚Äù ‚ÄúService Account User‚Äù How to get access ¬∂ Send your email (private gmail preferred) to Julius Busecke Log into google cloud console and start a small test VM ¬∂ Use the email you sent above to sign into https://console.cloud.google.com Make sure to chose the project ‚ÄúClimSimTesting‚Äù Now navigate to ‚ÄúCompute Engine‚Äù and click on ‚ÄúCreate New Instance‚Äù. Chose a recognizable name (e.g. climsim_<your_name>_test and leave everything else on default for now and click ‚ÄúCreate‚Äù at the bottom. After a short spin up you should see your VM instance with a green checkmark. Try to SSH into it via the cloud console (click on SSH on the far right). Let me know on slack that you were able to create an instance Very Important : Delete your instance after the test. You should do this everytime you are done using the VM to avoid large costs . Click on the triple dots on the right, and chose ‚ÄúDelete‚Äù. previous Bootcamp Guide next Guide for Data and Computation Team Members By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/vm_access.html#how-to-get-access",
    "transcript": "Getting access to Cloud VMs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs ¬∂ Important This documentation should be considered highly experimental and permissions to follow these instructions are only given to a small subset of testers at the moment. Why? ¬∂ We want to test e.g. containerized workflows that run climate simulations (for online testing of ML parametrizations). For that we need a ‚Äòbare‚Äô VM, and not the JupyterLab interface. Admin considerations ¬∂ To fully separate this testing from the Hub etc we have created a separate project in Gcloud (same billing account and Org). To keep an eye on (e.g. accidental) cost overrun, I added a budget alert (in the linked billing account) that applies to all resources for this new project, and set a 500$ alert with notifications at various thresholds. New users need the following IAM roles (click ‚ÄúGrant Access‚Äù with the email as principals): ‚ÄúCompute Admin‚Äù ‚ÄúService Account User‚Äù How to get access ¬∂ Send your email (private gmail preferred) to Julius Busecke Log into google cloud console and start a small test VM ¬∂ Use the email you sent above to sign into https://console.cloud.google.com Make sure to chose the project ‚ÄúClimSimTesting‚Äù Now navigate to ‚ÄúCompute Engine‚Äù and click on ‚ÄúCreate New Instance‚Äù. Chose a recognizable name (e.g. climsim_<your_name>_test and leave everything else on default for now and click ‚ÄúCreate‚Äù at the bottom. After a short spin up you should see your VM instance with a green checkmark. Try to SSH into it via the cloud console (click on SSH on the far right). Let me know on slack that you were able to create an instance Very Important : Delete your instance after the test. You should do this everytime you are done using the VM to avoid large costs . Click on the triple dots on the right, and chose ‚ÄúDelete‚Äù. previous Bootcamp Guide next Guide for Data and Computation Team Members By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/vm_access.html#log-into-google-cloud-console-and-start-a-small-test-vm",
    "transcript": "Getting access to Cloud VMs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs ¬∂ Important This documentation should be considered highly experimental and permissions to follow these instructions are only given to a small subset of testers at the moment. Why? ¬∂ We want to test e.g. containerized workflows that run climate simulations (for online testing of ML parametrizations). For that we need a ‚Äòbare‚Äô VM, and not the JupyterLab interface. Admin considerations ¬∂ To fully separate this testing from the Hub etc we have created a separate project in Gcloud (same billing account and Org). To keep an eye on (e.g. accidental) cost overrun, I added a budget alert (in the linked billing account) that applies to all resources for this new project, and set a 500$ alert with notifications at various thresholds. New users need the following IAM roles (click ‚ÄúGrant Access‚Äù with the email as principals): ‚ÄúCompute Admin‚Äù ‚ÄúService Account User‚Äù How to get access ¬∂ Send your email (private gmail preferred) to Julius Busecke Log into google cloud console and start a small test VM ¬∂ Use the email you sent above to sign into https://console.cloud.google.com Make sure to chose the project ‚ÄúClimSimTesting‚Äù Now navigate to ‚ÄúCompute Engine‚Äù and click on ‚ÄúCreate New Instance‚Äù. Chose a recognizable name (e.g. climsim_<your_name>_test and leave everything else on default for now and click ‚ÄúCreate‚Äù at the bottom. After a short spin up you should see your VM instance with a green checkmark. Try to SSH into it via the cloud console (click on SSH on the far right). Let me know on slack that you were able to create an instance Very Important : Delete your instance after the test. You should do this everytime you are done using the VM to avoid large costs . Click on the triple dots on the right, and chose ‚ÄúDelete‚Äù. previous Bootcamp Guide next Guide for Data and Computation Team Members By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/vm_access.html#getting-access-to-cloud-vms",
    "transcript": "Getting access to Cloud VMs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs Contents Why? Admin considerations How to get access Log into google cloud console and start a small test VM Getting access to Cloud VMs ¬∂ Important This documentation should be considered highly experimental and permissions to follow these instructions are only given to a small subset of testers at the moment. Why? ¬∂ We want to test e.g. containerized workflows that run climate simulations (for online testing of ML parametrizations). For that we need a ‚Äòbare‚Äô VM, and not the JupyterLab interface. Admin considerations ¬∂ To fully separate this testing from the Hub etc we have created a separate project in Gcloud (same billing account and Org). To keep an eye on (e.g. accidental) cost overrun, I added a budget alert (in the linked billing account) that applies to all resources for this new project, and set a 500$ alert with notifications at various thresholds. New users need the following IAM roles (click ‚ÄúGrant Access‚Äù with the email as principals): ‚ÄúCompute Admin‚Äù ‚ÄúService Account User‚Äù How to get access ¬∂ Send your email (private gmail preferred) to Julius Busecke Log into google cloud console and start a small test VM ¬∂ Use the email you sent above to sign into https://console.cloud.google.com Make sure to chose the project ‚ÄúClimSimTesting‚Äù Now navigate to ‚ÄúCompute Engine‚Äù and click on ‚ÄúCreate New Instance‚Äù. Chose a recognizable name (e.g. climsim_<your_name>_test and leave everything else on default for now and click ‚ÄúCreate‚Äù at the bottom. After a short spin up you should see your VM instance with a green checkmark. Try to SSH into it via the cloud console (click on SSH on the far right). Let me know on slack that you were able to create an instance Very Important : Delete your instance after the test. You should do this everytime you are done using the VM to avoid large costs . Click on the triple dots on the right, and chose ‚ÄúDelete‚Äù. previous Bootcamp Guide next Guide for Data and Computation Team Members By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/bootcamp_guide.md",
    "transcript": "# Bootcamp Guide We collect all bootcamp materials in the [LEAP-Pangeo bootcamp repository](https://github.com/leap-stc/LEAP-bootcamps). Please keep all relevant information and materials in this repository to make it easier for participants to find them. ## Preparing the bootcamp We encourage you to reuse materials from past bootcamps and adapt them to your needs. **Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along.** This ensures the following: - The lecturer substantially more time to write live, ensuring that others can follow along - It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. - It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting \"Clear All Outputs\". > **Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation.** ```{admonition} Pretty please --- class: important --- It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. ``` ## Materials for the bootcamp You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: - Create a new folder in the \"Codes\" and \"Lectures\" folders and add your materials there. *If you are giving the same material as a previous bootcamp, please copy them.* - ```{warning} If you are using materials from other sources, like the [An Introduction to Earth and Environmental Data Science](https://earth-env-data-science.github.io/intro.html) book. Please just follow the past events structure and link to the original materials. ``` - [README.md](https://github.com/leap-stc/LEAP-bootcamps/README.md) Enter a new entry under the \"Events\" section. This should include an [nbgitpuller](https://nbgitpuller.readthedocs.io/en/latest/) link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to [generate these links](https://nbgitpuller.readthedocs.io/en/latest/link.html)). Please also use the LEAP-Pangeo Hub badge by adding code like this: ``` [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) ``` > Add the generated link to the `<>` brackets. ## Running the bootcamp ### Prepare ahead of the event Bootcamp instructors should make sure that they are added to the [Bootcamp Instructors Github Team](https://github.com/orgs/leap-stc/teams/bootcamp-instructors). If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: - Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. - Can the instructors manually trigger the [\"Parse Member Sheet and Add New Members\" action](https://github.com/leap-stc/member_management/actions/workflows/read_sheet.yaml) (by clicking on \"Run Workflow>Run Workflow\" in the top right). - Double Check that all participants github names are present in the [leap-pangeo-base-access](https://github.com/orgs/leap-stc/teams/leap-pangeo-base-access) team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described [here](https://leap-stc.github.io/guides/faq.html#where-is-my-invite)) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting [here](guide.team.admin.member_signup_troubleshooting). We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the [](support.data_compute_team) or 2i2c beforehand. ### Troubleshooting during the event If students have trouble signin into the hub, please refer to the [FAQ](faq.cannot-log-into-hub) and [](guide.team.admin.member_signup_troubleshooting) for troubleshooting steps. ## Debrief Please add your versions of the filled notebooks to a subfolder named `filled_notebooks` in the eventfolder you created above, so participants can access them later."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html#preparing-the-bootcamp",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html#materials-for-the-bootcamp",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html#running-the-bootcamp",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html#prepare-ahead-of-the-event",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html#troubleshooting-during-the-event",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html#debrief",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/bootcamp_guide.html#bootcamp-guide",
    "transcript": "Bootcamp Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide Contents Preparing the bootcamp Materials for the bootcamp Running the bootcamp Prepare ahead of the event Troubleshooting during the event Debrief Bootcamp Guide ¬∂ We collect all bootcamp materials in the LEAP-Pangeo bootcamp repository . Please keep all relevant information and materials in this repository to make it easier for participants to find them. Preparing the bootcamp ¬∂ We encourage you to reuse materials from past bootcamps and adapt them to your needs. Past event have suggested that it is extremely helpful to have the instructor write down code live, so participants can follow along. This ensures the following: The lecturer substantially more time to write live, ensuring that others can follow along It also demonstrates that the lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures. It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process. Ideally you will use the same materials as the participants (see below for how to organize them), and clear all outputs prior to starting the lecture by right clicking on the notebook and selecting ‚ÄúClear All Outputs‚Äù. Of course you are not expected to remember all the code. Ideally you have a second laptop/tablet to use as a cheat sheet during the presentation. Pretty please It would be great if you can commit your filled notebooks after the lecture, so participants can access them later. See below for where to commit them. Materials for the bootcamp ¬∂ You can reuse the materials for your own bootcamps or modify them to your needs. Please do not change the past materials, and instead create new folders/entries in the following places: Create a new folder in the ‚ÄúCodes‚Äù and ‚ÄúLectures‚Äù folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them. Warning If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials. README.md Enter a new entry under the ‚ÄúEvents‚Äù section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links ). Please also use the LEAP-Pangeo Hub badge by adding code like this: [![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&logo=leap-globe)](<generated link>) Copy to clipboard Add the generated link to the <> brackets. Running the bootcamp ¬∂ Prepare ahead of the event ¬∂ Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team . If not please reach out to one of the github organization admins to be added. The goal should be that whoever runs the bootcamp can perform tasks that might be needed the day of (mostly likely signing up folks who either entered wrong data or did not register ahead of time). Before the event check the following: Is somebody available to add new data to the Member Data Google Sheet? LEAPs Managing Director has the ability to add data or give others edit access. Can the instructors manually trigger the ‚ÄúParse Member Sheet and Add New Members‚Äù action (by clicking on ‚ÄúRun Workflow>Run Workflow‚Äù in the top right). Double Check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub! If they are not there they either have not accepted the invite to the team (they will be shown as pending invites; tell them to accept the invite as described here ) or there is an issue with their github username (check their github handles in the Member Data Google Sheet for typos and completeness). Find a more detailed guide for troubleshooting here . We recommend that you familiarize yourself and test these steps ahead of time. If you are anticipating a large demand for an event it might be useful to consult with the Data and Computation Team or 2i2c beforehand. Troubleshooting during the event ¬∂ If students have trouble signin into the hub, please refer to the FAQ and Member Sign Up Troubleshooting for troubleshooting steps. Debrief ¬∂ Please add your versions of the filled notebooks to a subfolder named filled_notebooks in the eventfolder you created above, so participants can access them later. previous Education Guide next Getting access to Cloud VMs By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/faq.html#faq-cannot-log-into-hub",
    "transcript": "FAQs ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs Contents ‚ÄúWhere is my invite?‚Äù I cannot log into the hub üò± I received a warning about space on my User Directory Dask ‚ÄúKilled Workers‚Äù Datasets Chunks too large FAQs ¬∂ ‚ÄúWhere is my invite?‚Äù ¬∂ Please check your email account ( the one you used to sign up for Github - this is independent of the email you use for LEAP) for an invite that will look similar to this: Click the link and accept all invites. Alternatively you can log into your github account, and should see a notification in the top right menu under the ‚ÄúOrganizations‚Äù tab. You can follow that and accept the invitation there aswell. I cannot log into the hub üò± ¬∂ If you are unable to log into the hub, please check the following steps: Check if you are member of the appropriate github teams . If you are not follow these steps: Did you sign up for LEAP membership ? This will be done for you if you sign up for an event like the Momentum bootcamp! Did you receive a github invite? Here is how to check for that. Check again if they are part of the appropriate github teams . If these steps do not work, please reach out to the Data and Computation Team . If you are member of one of the github teams, ask them to try the following steps: Refresh the browser cache Try a different browser Restart the computer If these steps do not work, please reach out to the Data and Computation Team . I received a warning about space on my User Directory ¬∂ If you get a Hub Usage Alert email, this means you are violating the User Directory storage limit (to learn why this limit exists, see read about User Directories ). Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may temporarily get reduced cloud access. Troubleshooting To see which files and directories are taking up the bulk of your storage, run du -h --max-depth=1 ~/ | sort -h in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage. Delete cached files, ipython checkpoints, and any other unwanted files. If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our Data Guide and LEAP-Pangeo Data Policy . Our goal is to accomodate all community members and thus we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Computation Team . Dask ‚ÄúKilled Workers‚Äù ¬∂ The ‚ÄúKilled Worker‚Äù message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message Datasets Chunks too large ¬∂ Issue The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory. Solution You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation: from dask_gateway import Gateway gateway = Gateway () options = gateway . cluster_options () options . worker_memory = 10 # 10 GB of memory per worker. # Create a cluster with those options cluster = gateway . new_cluster ( options ) cluster . scale ( ... ) client = cluster . get_client () Copy to clipboard Example with Solution previous Guide for Data and Computation Team Members next LEAP-Pangeo Architecture By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/education_guide.md",
    "transcript": "# Education Guide This guide is intended for folks who want to run classes using the LEAP Pangeo resources. ## Teaching classes using the LEAP-Pangeo JupyterHub ### Can I use LEAP-Pangeo resources to teach my class? LEAP provides infrastructure support to [](reference.education.leap_affiliated_course). If you are an instructor and want your class to become affiliated with LEAP, you should contact the [](support.data_compute_team) well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. ### Preparing Classes We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for [membership](users.membership.apply) under the **education category** as soon as possible and make sure to check out our ['getting started' tutorial](tutorial.getting_started) to familiarize yourself with the environment. ### Team Membership Contact a gitub organization admin to be added to the [education-instructors github team](https://github.com/orgs/leap-stc/teams/education-instructors) before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. ### Sign up students All students should apply for [membership](users.membership.apply) under the **education category** well ahead of the starting date of the class (ideally a week before). ```{important} A github account is required to gain access to LEAP-Pangeo. It is free to create. ``` Instructors can verify that students have access by checking that their github usernames as listed as members of [this github team](https://github.com/orgs/leap-stc/teams/leap-pangeo-base-access). We suggest that instructors direct students to [this documentation](https://leap-stc.github.io/intro.html) and ask them to test their [access to the Hub](hub:server:login) before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. ### Troubleshooting #### Students cannot sign on Refer students to the [FAQ](faq.cannot-log-into-hub) for troubleshooting steps and follow the [admin troubleshooting steps](guide.team.admin.member_signup_troubleshooting) if problems persist."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#teaching-classes-using-the-leap-pangeo-jupyterhub",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#can-i-use-leap-pangeo-resources-to-teach-my-class",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#preparing-classes",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#team-membership",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#sign-up-students",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#troubleshooting",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#students-cannot-sign-on",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/education_guide.html#education-guide",
    "transcript": "Education Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide Contents Teaching classes using the LEAP-Pangeo JupyterHub Can I use LEAP-Pangeo resources to teach my class? Preparing Classes Team Membership Sign up students Troubleshooting Students cannot sign on Education Guide ¬∂ This guide is intended for folks who want to run classes using the LEAP Pangeo resources. Teaching classes using the LEAP-Pangeo JupyterHub ¬∂ Can I use LEAP-Pangeo resources to teach my class? ¬∂ LEAP provides infrastructure support to LEAP Affiliated Courses . If you are an instructor and want your class to become affiliated with LEAP, you should contact the Data and Computation Team well in advance (ideally a month before the starting date of the class) in order to coordinate efforts efficiently. Preparing Classes ¬∂ We suggest that instructors preprare their materials directly on the hub as much as possible, to ensure an identical experience for the students during the class. To develop/test/edit material instructors should apply for membership under the education category as soon as possible and make sure to check out our ‚Äògetting started‚Äô tutorial to familiarize yourself with the environment. Team Membership ¬∂ Contact a gitub organization admin to be added to the education-instructors github team before the start of the classes. This will enable you to troubleshoot sign up issues (see below) by yourself. Sign up students ¬∂ All students should apply for membership under the education category well ahead of the starting date of the class (ideally a week before). Important A github account is required to gain access to LEAP-Pangeo. It is free to create. Instructors can verify that students have access by checking that their github usernames as listed as members of this github team . We suggest that instructors direct students to this documentation and ask them to test their access to the Hub before the class starts to avoid any technical interruptions. Make sure to familiarize yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary. Troubleshooting ¬∂ Students cannot sign on ¬∂ Refer students to the FAQ for troubleshooting steps and follow the admin troubleshooting steps if problems persist. previous Compute Guide next Bootcamp Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/reference/education.html#reference-education-leap-affiliated-course",
    "transcript": "Education ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents LEAP Affiliated Courses Education Contents LEAP Affiliated Courses Education ¬∂ LEAP Affiliated Courses ¬∂ To become a LEAP Affiliated Course you have to fulfill the following requirements: Acknowledge LEAP‚Äôs support on syllabus. Add the course to LEAP‚Äôs education page. If the course will have a github repository, consider developing it within the LEAP GitHub organization . previous Membership next Getting Help By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/_sources/guides/compute_guide.md",
    "transcript": "# Compute Guide These are a set of guides for using the JupyterHub Compute Environment effectively. ## Dask To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: - <https://pangeo.io/cloud.html#dask> ## Creating Custom Docker Images The LEAP-Pangeo Jupyterhub provides a selection of [software environments](reference.infrastructure.hub.software_env) that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to [simply paste a string into the `Image` selection box upon server startup](reference.infrastructure.hub.image.custom). This guide will not describe how to build a docker image from scratch, but instead rely on [repo2docker](https://github.com/jupyterhub/repo2docker) which is implemented in template repositories. Generally you have two options to generate images: - Inheret from a community maintained existing image and add additional dependencies (**recommended**) - Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to [Customize a community-maintained upstream image](https://docs.2i2c.org/admin/howto/environment/customize-image/index.html) with some modification specific to LEAP. ### Creating a repository Create a repository from [2i2c-org/example-inherit-from-community-image](https://github.com/2i2c-org/example-inherit-from-community-image/tree/main) by clicking the `Use this Template` button in the upper right corner. ```{warning} We highly encourage - creating this repo under the `leap-stc` organization - making the repo public This will ensure all steps below work as easily as possible. ``` (guides.compute.custom_image.container_registry)= ### Set up the Container registry We recommend hosting the image under the [`leap-stc` quay.io account](https://quay.io/user/leap-stc/). This requires actions from a member of the [](support.data_compute_team). Please open an issue titled `Set up quay.io` in your newly created repo, and add a message like this: ``` @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo ``` ```{note} If you decide to host the image under your own account, you should follow the instructions in the [2i2c docs](https://docs.2i2c.org/admin/howto/environment/customize-image/index.html#set-up-the-github-repository-and-connect-it-to-quay-io). ``` ### Edit repository files Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: - In `Dockerfile` edit the text after `\"FROM\"` with the image tag of the upstream image you want to build opon. The tag usually is composed as ``` <registry>/<username>/<repo_name>:<hash> ``` To inheret from the latest pangeo-notebook image you should paste: ``` quay.io/pangeo/pangeo-notebook:2024.08.18 ``` ```{note} At the time of writing `2024.08.18` was the latest tag, but you should always check for the latest hash [here](https://github.com/pangeo-data/pangeo-docker-images/tags) and use that one. ``` - Now we can specify the additional requirements that should be installed on top. Edit the `environment.yml` file to your liking (you can remove the example packages from the template!). - Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder `image-tests`, add `.ipynb` and/or `.py` files for your added dependencies. Don't forget to remove the existing examples, since they will fail if you removed the `'otter-grader'` from `environment.yaml`! - Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c [here](https://docs.2i2c.org/admin/howto/environment/customize-image/index.html#build-base-image) to modify the github workflows and push changes back to Github. ```{important} If you chose to host the image via LEAP (as recommended in [](guides.compute.custom_image.container_registry)), make sure you replace the `IMAGE_NAME` value in `.github/workflows/build.yaml` and `.github/workflows/test.yaml` with `leap-stc/<your repository name>`. ``` ### Test the image You can test the built image either on the [hub](https://docs.2i2c.org/admin/howto/environment/customize-image/index.html#test-the-custom-image-with-a-2i2c-hub) or via \\[Binder\\](<username>/jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the `environment.yaml`, push to Github, and test until you are happy with the environment."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#dask",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#creating-custom-docker-images",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#creating-a-repository",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#set-up-the-container-registry",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#edit-repository-files",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#test-the-image",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#compute-guide",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/guides/compute_guide.html#guides-compute-custom-image-container-registry",
    "transcript": "Compute Guide ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide Contents Dask Creating Custom Docker Images Creating a repository Set up the Container registry Edit repository files Test the image Compute Guide ¬∂ These are a set of guides for using the JupyterHub Compute Environment effectively. Dask ¬∂ To help you scale up calculations using a cluster, the Hub is configured with Dask Gateway. For a quick guide on how to start a Dask Cluster, consult this page from the Pangeo docs: https://pangeo.io/cloud.html#dask Creating Custom Docker Images ¬∂ The LEAP-Pangeo Jupyterhub provides a selection of software environments that enable many workflows using the Pangeo Software stack, but often users will need to install custom dependencies. Installing dependencies every time you start a server is usually fine for testing, but can become cumbersome over the long run and makes reproducibility much harder. The solution to this is to build a custom docker image, publish it on a registry, and then you will be able to simply paste a string into the Image selection box upon server startup . This guide will not describe how to build a docker image from scratch, but instead rely on repo2docker which is implemented in template repositories. Generally you have two options to generate images: Inheret from a community maintained existing image and add additional dependencies ( recommended ) Build an entirely new environment, for instance based on a conda environment file. In most cases we recommend the first approach due to the lower maintenance burden (configurations from upstream can be adopted by a simple tag change). This guide will describe the particular steps needed to generate your own image based on one of the pangeo docker images. These instructions are based mostly on the 2i2c documentation of how to Customize a community-maintained upstream image with some modification specific to LEAP. Creating a repository ¬∂ Create a repository from 2i2c-org/example-inherit-from-community-image by clicking the Use this Template button in the upper right corner. Warning We highly encourage creating this repo under the leap-stc organization making the repo public This will ensure all steps below work as easily as possible. Set up the Container registry ¬∂ We recommend hosting the image under the leap-stc quay.io account . This requires actions from a member of the Data and Computation Team . Please open an issue titled Set up quay.io in your newly created repo, and add a message like this: @leap-stc/data-and-compute can you please: - share the quay.io secrets with this repo - set up a matching repo under the `leap-stc`quay.io organization - give the robot account access to that new repo Copy to clipboard Note If you decide to host the image under your own account, you should follow the instructions in the 2i2c docs . Edit repository files ¬∂ Now we edit the files in the repository. Chose your preferred way (e.g. in the browser, or by cloning the repositoriy locally or to the hub), and make the following changes: In Dockerfile edit the text after \"FROM\" with the image tag of the upstream image you want to build opon. The tag usually is composed as < registry >/< username >/< repo_name > : < hash > Copy to clipboard To inheret from the latest pangeo-notebook image you should paste: quay . io / pangeo / pangeo - notebook : 2024.08.18 Copy to clipboard Note At the time of writing 2024.08.18 was the latest tag, but you should always check for the latest hash here and use that one. Now we can specify the additional requirements that should be installed on top. Edit the environment.yml file to your liking (you can remove the example packages from the template!). Edit the tests to make sure that we can import the dependencies properly. Check out the examples in the folder image-tests , add .ipynb and/or .py files for your added dependencies. Don‚Äôt forget to remove the existing examples, since they will fail if you removed the 'otter-grader' from environment.yaml ! Finally set up the Github actions so that we can build the image in the next step. You can follow the instructions from 2i2c here to modify the github workflows and push changes back to Github. Important If you chose to host the image via LEAP (as recommended in Set up the Container registry ), make sure you replace the IMAGE_NAME value in .github/workflows/build.yaml and .github/workflows/test.yaml with leap-stc/<your repository name> . Test the image ¬∂ You can test the built image either on the hub or via [Binder]( /jupyter-scipy-xarray). If things work as expected, you are done! If you find that you are still missing dependencies, repeat the steps above to edit the environment.yaml , push to Github, and test until you are happy with the environment. previous Data Guide next Education Guide By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/intro.html#dashboard",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/intro.html#motivation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/intro.html#contents",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/intro.html#leap-technical-documentation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/index.html#dashboard",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/index.html#motivation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/index.html#contents",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io/index.html#leap-technical-documentation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io#dashboard",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io#motivation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io#contents",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap-stc.github.io#leap-technical-documentation",
    "transcript": "LEAP Technical Documentation ‚Äî Technical Documentation Technical Documentation LEAP Technical Documentation Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References Powered by Jupyter Book .md .pdf repository open issue Contents Dashboard Motivation Contents LEAP Technical Documentation Contents Dashboard Motivation Contents LEAP Technical Documentation ¬∂ This website is the home for all technical documentation related to LEAP and LEAP-Pangeo. Dashboard ¬∂ Update Status Contributors Deployment Status Motivation ¬∂ The motivation and justification for developing LEAP-Pangeo are laid out in several recent peer-reviewed publications: [ Abernathey et al. , 2021 ] and [ Gentemann et al. , 2021 ] . To summarize these arguments, a shared data and computing platform will: Facilitate seamless collaboration between project members around data-intensive science, accelerating research progress. Empower LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives. This access is provided through our JupyterHub platform . Place actionable data in the hands of LEAP partners to support knowledge transfer.using the LEAP-Pangeo Data Catalog . Enable rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research. Contents ¬∂ Tutorials Getting Started Guides Code Guide Data Guide Compute Guide Education Guide Bootcamp Guide Getting access to Cloud VMs Guide for Data and Computation Team Members FAQs Explanation LEAP-Pangeo Architecture LEAP-Pangeo Implementation Plan LEAP-Pangeo Code Policy LEAP-Pangeo Data Policy LEAP-Pangeo Infrastructure Policy Reference Infrastructure Membership Education Miscellaneous Getting Help How to cite LEAP-Pangeo References next Getting Started By LEAP Community ¬© Copyright 2021."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu",
    "transcript": "LEAP - An NSF Science and Technology Center Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Harnessing data to revolutionize climate projections LEAP‚Äôs Strategy Learning the Earth with Artificial Intelligence and Physics (LEAP) is an NSF Science and Technology Center (STC) launched in 2021. LEAP‚Äôs primary research strategy is to improve near-term climate projections by merging physical modeling with machine learning across a continuum from expertise in climate science and climate modeling to cutting-edge machine learning algorithms. The benefits will be significant for both the climate and data sciences communities. Climate scientists and modelers struggle to fully integrate the wealth of existing datasets into their models, while machine learning algorithms have been good at emulating and interpolating but have difficulties extrapolating or predicting extremes. By combining both approaches, LEAP will trigger a significant advancement for data science algorithms applied to physical problems. LEAP will incorporate physics and causal mechanisms into machine learning algorithms for better generalization and extrapolation, while optimally using the wealth of data available to climate science, in order to better predict the future. LATEST NEWS Applications Open for LEAP‚Äôs 2025 Summer Institute LEAP‚Äôs Winter 2025 Momentum Bootcamp Receives Approval to Issue Continuing Teacher + Leader Education (CTLE) Credits LEAP KT Research: Understanding Climate Modeling Across Societal Sectors MORE NEWS CONNECT WITH LEAP Linkedin Instagram Youtube SUBSCRIBE TO LEAP DONATE Focus Button Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu#content",
    "transcript": "LEAP - An NSF Science and Technology Center Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Harnessing data to revolutionize climate projections LEAP‚Äôs Strategy Learning the Earth with Artificial Intelligence and Physics (LEAP) is an NSF Science and Technology Center (STC) launched in 2021. LEAP‚Äôs primary research strategy is to improve near-term climate projections by merging physical modeling with machine learning across a continuum from expertise in climate science and climate modeling to cutting-edge machine learning algorithms. The benefits will be significant for both the climate and data sciences communities. Climate scientists and modelers struggle to fully integrate the wealth of existing datasets into their models, while machine learning algorithms have been good at emulating and interpolating but have difficulties extrapolating or predicting extremes. By combining both approaches, LEAP will trigger a significant advancement for data science algorithms applied to physical problems. LEAP will incorporate physics and causal mechanisms into machine learning algorithms for better generalization and extrapolation, while optimally using the wealth of data available to climate science, in order to better predict the future. LATEST NEWS Applications Open for LEAP‚Äôs 2025 Summer Institute LEAP‚Äôs Winter 2025 Momentum Bootcamp Receives Approval to Issue Continuing Teacher + Leader Education (CTLE) Credits LEAP KT Research: Understanding Climate Modeling Across Societal Sectors MORE NEWS CONNECT WITH LEAP Linkedin Instagram Youtube SUBSCRIBE TO LEAP DONATE Focus Button Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/",
    "transcript": "About - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP About Learning the Earth with Artificial Intelligence and Physics (LEAP) is an NSF Science Technology Center (STC) launched in 2021. Context Shortcomings in both climate science and computing capacities limit the accuracy of climate projections needed for optimal adaptation and resilience to climate change. Our team sees tremendous promise in developing a convergent approach integrating machine learning and climate simulations that can substantially reduce uncertainty in climate projections through 2050. This ambitious endeavor engages our nation‚Äôs top climate research laboratories (e.g., NCAR and NASA GISS) with major computing organizations (e.g., Google) and a diverse array of stakeholders, from companies to community organizations, that can benefit from more accurate projections. Ameliorating this critical shortcoming is a worthy and necessary venture for our nation and the world, and could improve life for generations to come. Such ambitious goals are only achievable through a large-scale and long-term effort covering research, education, and knowledge transfer, such as provided by the National Science Foundation Science and Technology Center (STC). Strategic Vision LEAP will revolutionize climate projections for informed climate adaptation. Achieving this vision should ensure that a large and broad range of public and private stakeholders have the required tools necessary for informed decision making when facing climate change. Mission LEAP‚Äôs mission is to increase the reliability, utility, and reach of climate projections through the integration of climate and data science. Values Our Mission will be achieved through the following Center Values: Innovation : Integrate new knowledge and methods to explore, learn and create; Legacy : Building lasting scientific knowledge and a new research community. These values will help us reach our final key value: Impact : Our work should make a difference in the adaptation to climate change. Strategic Priorities/Goals Harness data to improve the reliability of the Community Earth System Model Establish and deploy a modern cloud computing infrastructure for climate data, LEAPangeo, in support of research, education, and knowledge transfer Create a new discipline: Climate Data Science Establish bidirectional knowledge transfer regarding climate projections with a broad range of public and private stakeholders Strategic and Implementation Plan LEAP‚Äôs Strategic and Implementation Plan 2021-2026 (Version 1, April 28, 2022) is publicly available on Zenodo . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/#content",
    "transcript": "About - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP About Learning the Earth with Artificial Intelligence and Physics (LEAP) is an NSF Science Technology Center (STC) launched in 2021. Context Shortcomings in both climate science and computing capacities limit the accuracy of climate projections needed for optimal adaptation and resilience to climate change. Our team sees tremendous promise in developing a convergent approach integrating machine learning and climate simulations that can substantially reduce uncertainty in climate projections through 2050. This ambitious endeavor engages our nation‚Äôs top climate research laboratories (e.g., NCAR and NASA GISS) with major computing organizations (e.g., Google) and a diverse array of stakeholders, from companies to community organizations, that can benefit from more accurate projections. Ameliorating this critical shortcoming is a worthy and necessary venture for our nation and the world, and could improve life for generations to come. Such ambitious goals are only achievable through a large-scale and long-term effort covering research, education, and knowledge transfer, such as provided by the National Science Foundation Science and Technology Center (STC). Strategic Vision LEAP will revolutionize climate projections for informed climate adaptation. Achieving this vision should ensure that a large and broad range of public and private stakeholders have the required tools necessary for informed decision making when facing climate change. Mission LEAP‚Äôs mission is to increase the reliability, utility, and reach of climate projections through the integration of climate and data science. Values Our Mission will be achieved through the following Center Values: Innovation : Integrate new knowledge and methods to explore, learn and create; Legacy : Building lasting scientific knowledge and a new research community. These values will help us reach our final key value: Impact : Our work should make a difference in the adaptation to climate change. Strategic Priorities/Goals Harness data to improve the reliability of the Community Earth System Model Establish and deploy a modern cloud computing infrastructure for climate data, LEAPangeo, in support of research, education, and knowledge transfer Create a new discipline: Climate Data Science Establish bidirectional knowledge transfer regarding climate projections with a broad range of public and private stakeholders Strategic and Implementation Plan LEAP‚Äôs Strategic and Implementation Plan 2021-2026 (Version 1, April 28, 2022) is publicly available on Zenodo . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/about-our-center/",
    "transcript": "About Our Center - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP About Learning the Earth with Artificial Intelligence and Physics (LEAP) is an NSF Science Technology Center (STC) launched in 2021. Context Shortcomings in both climate science and computing capacities limit the accuracy of climate projections needed for optimal adaptation and resilience to climate change. Our team sees tremendous promise in developing a convergent approach integrating machine learning and climate simulations that can substantially reduce uncertainty in climate projections through 2050. This ambitious endeavor engages our nation‚Äôs top climate research laboratories (e.g., NCAR and NASA GISS) with major computing organizations (e.g., Google) and a diverse array of stakeholders, from companies to community organizations, that can benefit from more accurate projections. Ameliorating this critical shortcoming is a worthy and necessary venture for our nation and the world, and could improve life for generations to come. Such ambitious goals are only achievable through a large-scale and long-term effort covering research, education, and knowledge transfer, such as provided by the National Science Foundation Science and Technology Center (STC). Strategic Vision LEAP will revolutionize climate projections for informed climate adaptation. Achieving this vision should ensure that a large and broad range of public and private stakeholders have the required tools necessary for informed decision making when facing climate change. Mission LEAP‚Äôs mission is to increase the reliability, utility, and reach of climate projections through the integration of climate and data science. Values Our Mission will be achieved through the following Center Values: Innovation : Integrate new knowledge and methods to explore, learn and create; Legacy : Building lasting scientific knowledge and a new research community. These values will help us reach our final key value: Impact : Our work should make a difference in the adaptation to climate change. Strategic Priorities/Goals Harness data to improve the reliability of the Community Earth System Model Establish and deploy a modern cloud computing infrastructure for climate data, LEAPangeo, in support of research, education, and knowledge transfer Create a new discipline: Climate Data Science Establish bidirectional knowledge transfer regarding climate projections with a broad range of public and private stakeholders Strategic and Implementation Plan LEAP‚Äôs Strategic and Implementation Plan 2021-2026 (Version 1, April 28, 2022) is publicly available on Zenodo . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/about-our-center/#content",
    "transcript": "About Our Center - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP About Learning the Earth with Artificial Intelligence and Physics (LEAP) is an NSF Science Technology Center (STC) launched in 2021. Context Shortcomings in both climate science and computing capacities limit the accuracy of climate projections needed for optimal adaptation and resilience to climate change. Our team sees tremendous promise in developing a convergent approach integrating machine learning and climate simulations that can substantially reduce uncertainty in climate projections through 2050. This ambitious endeavor engages our nation‚Äôs top climate research laboratories (e.g., NCAR and NASA GISS) with major computing organizations (e.g., Google) and a diverse array of stakeholders, from companies to community organizations, that can benefit from more accurate projections. Ameliorating this critical shortcoming is a worthy and necessary venture for our nation and the world, and could improve life for generations to come. Such ambitious goals are only achievable through a large-scale and long-term effort covering research, education, and knowledge transfer, such as provided by the National Science Foundation Science and Technology Center (STC). Strategic Vision LEAP will revolutionize climate projections for informed climate adaptation. Achieving this vision should ensure that a large and broad range of public and private stakeholders have the required tools necessary for informed decision making when facing climate change. Mission LEAP‚Äôs mission is to increase the reliability, utility, and reach of climate projections through the integration of climate and data science. Values Our Mission will be achieved through the following Center Values: Innovation : Integrate new knowledge and methods to explore, learn and create; Legacy : Building lasting scientific knowledge and a new research community. These values will help us reach our final key value: Impact : Our work should make a difference in the adaptation to climate change. Strategic Priorities/Goals Harness data to improve the reliability of the Community Earth System Model Establish and deploy a modern cloud computing infrastructure for climate data, LEAPangeo, in support of research, education, and knowledge transfer Create a new discipline: Climate Data Science Establish bidirectional knowledge transfer regarding climate projections with a broad range of public and private stakeholders Strategic and Implementation Plan LEAP‚Äôs Strategic and Implementation Plan 2021-2026 (Version 1, April 28, 2022) is publicly available on Zenodo . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/",
    "transcript": "Meet Our Team - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Meet the LEAP Team LEAP is committed to nurturing collaboration throughout our transdisciplinary team in order to revolutionize climate projections by converging geoscience with machine learning, training a new generation of scientists and engineers proficient in climate science and data science, and leveraging many perspectives, experiences and ideas to produce rigorous climate data science and impactful community engagement initiatives. Executive Committee + Staff Senior Personnel Students + Postdocs Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/#content",
    "transcript": "Meet Our Team - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Meet the LEAP Team LEAP is committed to nurturing collaboration throughout our transdisciplinary team in order to revolutionize climate projections by converging geoscience with machine learning, training a new generation of scientists and engineers proficient in climate science and data science, and leveraging many perspectives, experiences and ideas to produce rigorous climate data science and impactful community engagement initiatives. Executive Committee + Staff Senior Personnel Students + Postdocs Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/executive-committee-staff/",
    "transcript": "Executive Committee + Staff - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Executive Committee + Staff EXECUTIVE COMMITTEE Pierre Gentine Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University Tian Zheng Deputy Director, Chief Convergence Officer & Education Director, LEAP; Professor & Chair of the Department of Statistics, Columbia University; Affiliate Member of the Data Science Institute Vanessa Burbano Corporate Engagement Director, LEAP; Sidney Taurel Associate Professor of Management at Columbia Business School, Columbia University Courtney D. Cogburn Chief Broadening Participation Officer & Knowledge Transfer Director, LEAP; Associate Professor of Social Work at Columbia University; Affiliate Member of the Data Science Institute David Lawrence NCAR Model Development Liaison, LEAP; Senior Scientist and Section Head at NCAR Michael Pritchard Institutional Integration Director, LEAP; Associate Professor of Earth Systems Science at University of California at Irvine; Director of Climate Simulation Research, NVIDIA Carl Vondrick Data Science Director, LEAP; Assistant Professor of Computer Science at Columbia University Laure Zanna Geoscience Director, LEAP; Professor of Mathematics & Atmosphere/Ocean Science at the Courant Institute of NYU; Affiliate Member of the Center for Data Science at NYU RESEARCH STAFF pending Manager of Data and Computing Wayne Chuang Integration Engineer Geneva List Senior Staff Associate, Knowledge Transfer ADMINISTRATIVE STAFF Catherine Cha Sr. Manager of Communications and Knowledge Transfer Molly Lopez Managing Director James Lu Manager of Finance and Operations pending Climate Data Science Education + Outreach Coordinator Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/executive-committee-staff/#content",
    "transcript": "Executive Committee + Staff - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Executive Committee + Staff EXECUTIVE COMMITTEE Pierre Gentine Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University Tian Zheng Deputy Director, Chief Convergence Officer & Education Director, LEAP; Professor & Chair of the Department of Statistics, Columbia University; Affiliate Member of the Data Science Institute Vanessa Burbano Corporate Engagement Director, LEAP; Sidney Taurel Associate Professor of Management at Columbia Business School, Columbia University Courtney D. Cogburn Chief Broadening Participation Officer & Knowledge Transfer Director, LEAP; Associate Professor of Social Work at Columbia University; Affiliate Member of the Data Science Institute David Lawrence NCAR Model Development Liaison, LEAP; Senior Scientist and Section Head at NCAR Michael Pritchard Institutional Integration Director, LEAP; Associate Professor of Earth Systems Science at University of California at Irvine; Director of Climate Simulation Research, NVIDIA Carl Vondrick Data Science Director, LEAP; Assistant Professor of Computer Science at Columbia University Laure Zanna Geoscience Director, LEAP; Professor of Mathematics & Atmosphere/Ocean Science at the Courant Institute of NYU; Affiliate Member of the Center for Data Science at NYU RESEARCH STAFF pending Manager of Data and Computing Wayne Chuang Integration Engineer Geneva List Senior Staff Associate, Knowledge Transfer ADMINISTRATIVE STAFF Catherine Cha Sr. Manager of Communications and Knowledge Transfer Molly Lopez Managing Director James Lu Manager of Finance and Operations pending Climate Data Science Education + Outreach Coordinator Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/sr-personnel/",
    "transcript": "Senior Personnel - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Senior Personnel Viviana Acquaviva Professor of Physics, CUNY NYC College of Technology and CUNY Graduate Center Candace Agonafir Associate Research Scientist, Data Science + Civil Engineering, Columbia University Dhruv Balwada Lamont Assistant Research Professor, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Katie Dagon Project Scientist II - Climate Change Research Section, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Gregory Elsaesser Research Scientist, Columbia University / NASA Goddard Institute for Space Studies (NASA GISS) David John Gagne Machine Learning Scientist II & Head of the Machine Integration and Learning for Earth Systems Group, National Center for Atmospheric Research (NSF NCAR) Marco Giometto Assistant Professor of Civil Engineering & Engineering Mechanics, Columbia University Linnia Hawkins Associate Research Scientist, Earth + Environmental Engineering, Columbia University Jonathan Kingslake Associate Professor, Department of Earth & Environmental Sciences (DEES), Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Samory Kpotufe Associate Professor, Department of Statistics, Columbia University Vipin Kumar Regents Professor and William Norris Endowed Chair, Department of Computer Science & Engineering, University of Minnesota; Director of the CSE Data Science Initiative Kara Lamb Associate Research Scientist, Earth + Environmental Engineering, Columbia University Charles Lang Senior Executive Director of the Digital Futures Institute, Teachers College of Columbia University Stephan Mandt Associate Professor of Computer Science and Statistics, University of California, Irvine Brian Medeiros Project Scientist, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Robert Pincus Lamont Research Professor, Ocean & Climate Physics, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School; Adjunct Professor, Department of Applied Mathematics & Applied Physics, School of Engineering & Applied Science, Columbia University Oren Pizmony-Levy Associate Professor of International & Comparative Education, Teachers College of Columbia University; Director of the Center for Sustainable Futures David Porter Associate Research Scientist, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Addisu Semie Associate Research Scientist, Columbia University; National Center for Atmospheric Research (NSF NCAR) Sara Shamekh Assistant Professor, Courant Institute of Mathematical Sciences, New York University Marcus van Lier-Walqui Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Ensheng Weng Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/sr-personnel/#content",
    "transcript": "Senior Personnel - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Senior Personnel Viviana Acquaviva Professor of Physics, CUNY NYC College of Technology and CUNY Graduate Center Candace Agonafir Associate Research Scientist, Data Science + Civil Engineering, Columbia University Dhruv Balwada Lamont Assistant Research Professor, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Katie Dagon Project Scientist II - Climate Change Research Section, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Gregory Elsaesser Research Scientist, Columbia University / NASA Goddard Institute for Space Studies (NASA GISS) David John Gagne Machine Learning Scientist II & Head of the Machine Integration and Learning for Earth Systems Group, National Center for Atmospheric Research (NSF NCAR) Marco Giometto Assistant Professor of Civil Engineering & Engineering Mechanics, Columbia University Linnia Hawkins Associate Research Scientist, Earth + Environmental Engineering, Columbia University Jonathan Kingslake Associate Professor, Department of Earth & Environmental Sciences (DEES), Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Samory Kpotufe Associate Professor, Department of Statistics, Columbia University Vipin Kumar Regents Professor and William Norris Endowed Chair, Department of Computer Science & Engineering, University of Minnesota; Director of the CSE Data Science Initiative Kara Lamb Associate Research Scientist, Earth + Environmental Engineering, Columbia University Charles Lang Senior Executive Director of the Digital Futures Institute, Teachers College of Columbia University Stephan Mandt Associate Professor of Computer Science and Statistics, University of California, Irvine Brian Medeiros Project Scientist, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Robert Pincus Lamont Research Professor, Ocean & Climate Physics, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School; Adjunct Professor, Department of Applied Mathematics & Applied Physics, School of Engineering & Applied Science, Columbia University Oren Pizmony-Levy Associate Professor of International & Comparative Education, Teachers College of Columbia University; Director of the Center for Sustainable Futures David Porter Associate Research Scientist, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Addisu Semie Associate Research Scientist, Columbia University; National Center for Atmospheric Research (NSF NCAR) Sara Shamekh Assistant Professor, Courant Institute of Mathematical Sciences, New York University Marcus van Lier-Walqui Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Ensheng Weng Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/studentsandpostdocs/",
    "transcript": "Students + Postdocs - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Students + Postdocs Gabriele Accarino Postdoc (Columbia University) Guillaume Bertoli Postdoctoral Fellow (Columbia University) Matthieu Blanke Postdoc, Courant Institute of Mathematical Sciences (NYU) Da Fan Postdoc, Meteorology + Atmospheric Science (NSF NCAR) Savannah Ferretti ** Graduate Research Assistant, Earth System Science (UC Irvine) Dion Ho Graduate Research Assistant, Applied Physics + Applied Mathematics (Columbia University) Francesco Immorlano Postdoc, Computer Science (UC Irvine) Jaeyoung Jung Postdoc, Civil Engineering + Engineering Mechanics (Columbia University) Joseph Ko ** Postdoc, Climate School (Columbia University) Michelle Krakora Bridge-to-PhD Scholar (Columbia University) Aya Lahlou Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Shuolin (Shawn) Li ** Postdoc, Data Science Institute (Columbia University) Jerry Lin Graduate Research Assistant, Earth System Science (UC Irvine) Kaitlyn Loftus ** Postdoc, Climate School (Columbia University) Racheet Matai ** Postdoc, Climate School (Columbia University) Yongquan Francis Qu ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Arvind Renganathan Graduate Research Assistant, Computer Science (Univ. of Minnesota) Julia Simpson ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Abby Shaum Graduate Staff Associate, Climate School (Columbia University) Christina Torres ** Graduate Research Assistant, Science Education (Teachers College, Columbia University) Jiarong Wu Postdoc, Courant Institute of Mathematical Sciences (NYU) ** These LEAPers are also members of the LEAP Junior Scientists Advisory Group (JSAG). Read more about JSAG here . Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/meet-our-team/studentsandpostdocs/#content",
    "transcript": "Students + Postdocs - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Students + Postdocs Gabriele Accarino Postdoc (Columbia University) Guillaume Bertoli Postdoctoral Fellow (Columbia University) Matthieu Blanke Postdoc, Courant Institute of Mathematical Sciences (NYU) Da Fan Postdoc, Meteorology + Atmospheric Science (NSF NCAR) Savannah Ferretti ** Graduate Research Assistant, Earth System Science (UC Irvine) Dion Ho Graduate Research Assistant, Applied Physics + Applied Mathematics (Columbia University) Francesco Immorlano Postdoc, Computer Science (UC Irvine) Jaeyoung Jung Postdoc, Civil Engineering + Engineering Mechanics (Columbia University) Joseph Ko ** Postdoc, Climate School (Columbia University) Michelle Krakora Bridge-to-PhD Scholar (Columbia University) Aya Lahlou Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Shuolin (Shawn) Li ** Postdoc, Data Science Institute (Columbia University) Jerry Lin Graduate Research Assistant, Earth System Science (UC Irvine) Kaitlyn Loftus ** Postdoc, Climate School (Columbia University) Racheet Matai ** Postdoc, Climate School (Columbia University) Yongquan Francis Qu ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Arvind Renganathan Graduate Research Assistant, Computer Science (Univ. of Minnesota) Julia Simpson ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Abby Shaum Graduate Staff Associate, Climate School (Columbia University) Christina Torres ** Graduate Research Assistant, Science Education (Teachers College, Columbia University) Jiarong Wu Postdoc, Courant Institute of Mathematical Sciences (NYU) ** These LEAPers are also members of the LEAP Junior Scientists Advisory Group (JSAG). Read more about JSAG here . Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/org-chart/",
    "transcript": "Organization Chart - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Organizational Chart Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/org-chart/#content",
    "transcript": "Organization Chart - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Organizational Chart Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/jsag/",
    "transcript": "Junior Scientists Advisory Group - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Junior Scientists Advisory Group In 2023, LEAP established the Junior Scientists Advisory Group (JSAG) to foster transparency and communication throughout the LEAP community, as well as promote a deeper sense of community within our Center.¬† Current JSAG members are listed below. JSAG activities and responsibilities include: Liaison representation at a portion of the monthly LEAP Executive Committee meeting Providing input regarding Center events, such as Journal Club meetings Spearheading community-building events and social activities If you are interested in joining LEAP‚Äôs Junior Scientist Advisory Group, please contact Molly Lopez, Managing Director, at ml3122@columbia.edu . Jatan Buch Postdoc, Earth + Environmental Engineering (Columbia University) Savannah Ferretti Graduate Research Assistant, Earth System Science (UC Irvine) Linnia Hawkins Postdoc, Earth + Environmental Engineering (Columbia University) Thea Heimdal Associate Research Scientist, Geochemistry (Columbia University) Arthur Hu Postdoc, Center for Climate Systems Research (Columbia University) Joseph Ko Postdoc, Ocean + Climate Physics (Columbia University) Shuolin (Shawn) Li Postdoc, Data Science Institute (Columbia University) Kaitlyn Loftus Postdoc, Climate School (Columbia University) Racheet Matai Postdoc, Marine and Polar Geophysics (Columbia University) Yongquan (Francis) Qu Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Christina Torres Graduate Research Assistant, Science Education (Teachers College, Columbia University) Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/jsag/#content",
    "transcript": "Junior Scientists Advisory Group - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Junior Scientists Advisory Group In 2023, LEAP established the Junior Scientists Advisory Group (JSAG) to foster transparency and communication throughout the LEAP community, as well as promote a deeper sense of community within our Center.¬† Current JSAG members are listed below. JSAG activities and responsibilities include: Liaison representation at a portion of the monthly LEAP Executive Committee meeting Providing input regarding Center events, such as Journal Club meetings Spearheading community-building events and social activities If you are interested in joining LEAP‚Äôs Junior Scientist Advisory Group, please contact Molly Lopez, Managing Director, at ml3122@columbia.edu . Jatan Buch Postdoc, Earth + Environmental Engineering (Columbia University) Savannah Ferretti Graduate Research Assistant, Earth System Science (UC Irvine) Linnia Hawkins Postdoc, Earth + Environmental Engineering (Columbia University) Thea Heimdal Associate Research Scientist, Geochemistry (Columbia University) Arthur Hu Postdoc, Center for Climate Systems Research (Columbia University) Joseph Ko Postdoc, Ocean + Climate Physics (Columbia University) Shuolin (Shawn) Li Postdoc, Data Science Institute (Columbia University) Kaitlyn Loftus Postdoc, Climate School (Columbia University) Racheet Matai Postdoc, Marine and Polar Geophysics (Columbia University) Yongquan (Francis) Qu Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Christina Torres Graduate Research Assistant, Science Education (Teachers College, Columbia University) Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/eac/",
    "transcript": "External Advisory Committee - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP External Advisory Committee Rafael Bras Professor, School of Civil and Environmental Engineering and School of Earth and Atmospheric Sciences, Georgia Institute of Technology Melissa Burt Associate Professor and Associate Dean for Diversity and Inclusion, Walter Scott, Jr. College of Engineering, Colorado State University Montserrat Fuentes President, St. Edward‚Äôs University Amy McGovern Lloyd G. and Joyce Austin Presidential Professor, School of Computer Science and School of Meteorology, University of Oklahoma Rebecca Nugent Fienberg Professor of Statistics & Data Science, Associate Department Head, and Co-Director of Undergraduate Studies, Carnegie Mellon University Sonia Seneviratne Professor Department of Environmental Systems Science Eidgen√∂ssische Technische Hochschule (ETH) Z√ºrich, Institute for Atmospheric and Climate Science Bin Yu Chancellor's Distinguished Professor, Class of 1936, Second Chair in the College of Letters and Science, Departments of Statistics and of Electrical Engineering & Computer Sciences, University of California - Berkeley Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/eac/#content",
    "transcript": "External Advisory Committee - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP External Advisory Committee Rafael Bras Professor, School of Civil and Environmental Engineering and School of Earth and Atmospheric Sciences, Georgia Institute of Technology Melissa Burt Associate Professor and Associate Dean for Diversity and Inclusion, Walter Scott, Jr. College of Engineering, Colorado State University Montserrat Fuentes President, St. Edward‚Äôs University Amy McGovern Lloyd G. and Joyce Austin Presidential Professor, School of Computer Science and School of Meteorology, University of Oklahoma Rebecca Nugent Fienberg Professor of Statistics & Data Science, Associate Department Head, and Co-Director of Undergraduate Studies, Carnegie Mellon University Sonia Seneviratne Professor Department of Environmental Systems Science Eidgen√∂ssische Technische Hochschule (ETH) Z√ºrich, Institute for Atmospheric and Climate Science Bin Yu Chancellor's Distinguished Professor, Class of 1936, Second Chair in the College of Letters and Science, Departments of Statistics and of Electrical Engineering & Computer Sciences, University of California - Berkeley Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/partners-supporters/",
    "transcript": "Partners + Supporters - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Partners + Supporters SUPPORT LEAP Academic Partners New York University University of California at Irvine University of Minnesota Teachers College, Columbia University Climate Modeling Lab Partners National Center for Atmospheric Research (NCAR) NASA Goddard Institute for Space Studies Private + Public Partners CarbonPlan Google New York City Public Schools Climatematch Academy American Museum of Natural History Amazon Web Services NVIDIA LEAP Supporters Darren Manelski Gift to support LEAP‚Äôs work to improve climate projections through the integration of climate and data science. David Wallerstein Gift to establish the Wallerstein AI and Climate Communications Fund, dedicated to supporting communication efforts to educate and disseminate information about advancements of AI for climate change solutions. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/partners-supporters/#content",
    "transcript": "Partners + Supporters - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Partners + Supporters SUPPORT LEAP Academic Partners New York University University of California at Irvine University of Minnesota Teachers College, Columbia University Climate Modeling Lab Partners National Center for Atmospheric Research (NCAR) NASA Goddard Institute for Space Studies Private + Public Partners CarbonPlan Google New York City Public Schools Climatematch Academy American Museum of Natural History Amazon Web Services NVIDIA LEAP Supporters Darren Manelski Gift to support LEAP‚Äôs work to improve climate projections through the integration of climate and data science. David Wallerstein Gift to establish the Wallerstein AI and Climate Communications Fund, dedicated to supporting communication efforts to educate and disseminate information about advancements of AI for climate change solutions. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/strategic-partnership/",
    "transcript": "Strategic Partnership - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Strategic Partnership Engage With LEAP Driven by a collaboration between Columbia University , NYU , University of California Irvine , Teachers College , University of Minnesota Twin Cities , NCAR , NASA-GISS , and CarbonPlan , LEAP works with the nation‚Äôs top climate research laboratories and major computing organizations to optimally utilize the wealth of data available for climate modeling in order to better predict the future. Through financial contributions, in-kind collaborations, and bidirectional knowledge transfer, our strategic partners provide critical funding to seed, grow, and sustain LEAP, as well as help us understand the climate adaptation needs throughout various fields of the private and public sectors . Strategic partners may connect with the wide array of experts at the LEAP Center ‚Äì spanning the fields of artificial intelligence, machine learning, physics, education, climate science, data science, social work, statistics, business, and more ‚Äì and participate in events such as LEAP‚Äôs Annual Meeting, Bootcamps, Lectures in Climate Data Science, and other educational and networking programs. Together, we can ensure that the broadest range of private and public stakeholders will have access to the tools necessary for informed decision-making on behalf of their communities in the face of climate change. Impact of Strategic Partnership Strategic partnership directly impacts our Center‚Äôs growth and the expansion of our activities in order to accomplish these goals. Your engagement with LEAP will help us to: identify valuable industry perspectives, current issues, and prospects for research engagement and community outreach; scale up the number and scope of the Center‚Äôs research projects in response to this information; recruit more undergraduate / graduate students and postdoctoral fellows to innovate and lead in the new discipline of Climate Data Science; present additional Bootcamps and trainings to equip the next generations of climate data scientists, educators, and community leaders + members; and connect with, learn from, and inform a greater breadth of community, non-profit, and governmental organizations. Become a Strategic Partner If you are an industry or community organization who would like to financially invest in the growth of our Center and/or contribute¬† to our work by investing your unique perspective, knowledge, and experience in addressing climate change, we would love to have a conversation with you . We look forward to learning how LEAP‚Äôs research may benefit your work, and sharing how your investment in our Center can best serve our communities. For more information, please contact Molly Lopez, LEAP‚Äôs Managing Director, at ml3122@columbia.edu . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/strategic-partnership/#content",
    "transcript": "Strategic Partnership - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Strategic Partnership Engage With LEAP Driven by a collaboration between Columbia University , NYU , University of California Irvine , Teachers College , University of Minnesota Twin Cities , NCAR , NASA-GISS , and CarbonPlan , LEAP works with the nation‚Äôs top climate research laboratories and major computing organizations to optimally utilize the wealth of data available for climate modeling in order to better predict the future. Through financial contributions, in-kind collaborations, and bidirectional knowledge transfer, our strategic partners provide critical funding to seed, grow, and sustain LEAP, as well as help us understand the climate adaptation needs throughout various fields of the private and public sectors . Strategic partners may connect with the wide array of experts at the LEAP Center ‚Äì spanning the fields of artificial intelligence, machine learning, physics, education, climate science, data science, social work, statistics, business, and more ‚Äì and participate in events such as LEAP‚Äôs Annual Meeting, Bootcamps, Lectures in Climate Data Science, and other educational and networking programs. Together, we can ensure that the broadest range of private and public stakeholders will have access to the tools necessary for informed decision-making on behalf of their communities in the face of climate change. Impact of Strategic Partnership Strategic partnership directly impacts our Center‚Äôs growth and the expansion of our activities in order to accomplish these goals. Your engagement with LEAP will help us to: identify valuable industry perspectives, current issues, and prospects for research engagement and community outreach; scale up the number and scope of the Center‚Äôs research projects in response to this information; recruit more undergraduate / graduate students and postdoctoral fellows to innovate and lead in the new discipline of Climate Data Science; present additional Bootcamps and trainings to equip the next generations of climate data scientists, educators, and community leaders + members; and connect with, learn from, and inform a greater breadth of community, non-profit, and governmental organizations. Become a Strategic Partner If you are an industry or community organization who would like to financially invest in the growth of our Center and/or contribute¬† to our work by investing your unique perspective, knowledge, and experience in addressing climate change, we would love to have a conversation with you . We look forward to learning how LEAP‚Äôs research may benefit your work, and sharing how your investment in our Center can best serve our communities. For more information, please contact Molly Lopez, LEAP‚Äôs Managing Director, at ml3122@columbia.edu . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/opportunities/",
    "transcript": "Opportunities - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Job Opportunities LEAP Center Opportunities **NEW** Climate Data Science Education + Outreach Coordinator Deadline : open until filled, with ideal start in February 2025. Student Opportunities Please check back for future opportunities! Postdoctoral Opportunities **NEW** Postdoctoral Researcher, Quantitative Ecosystem Dynamics Lab (UC Berkeley) Deadline : December 22, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Extreme Ocean Events) (UC Irvine) Deadline : December 31, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Marine Heat Waves) (UC Irvine) Deadline : December 31, 2024 **NEW** Postdoctoral Research Associate, Atmospheric Sciences + Machine Learning (NYU Courant Institute) Deadline : January 15, 2025, with anticipated start date of March 2025. **NEW** Postdoctoral Research Scientist, Physical Oceanography (LDEO/Columbia) Deadline : ASAP, with anticipated start date of January 1, 2025 Faculty + Research Opportunities **NEW** Software Engineer, Global Ocean And Land Alkalinization DoE Earth Shot Project (Yale School of the Environment) **NEW** Geospatial Data Engineer (Planette AI) **NEW** Assistant Professor (limited term), Statistics (Columbia University) Deadline : review of applications begins December 1, 2024, and will continue until position is filled. **NEW** Lecturer in Discipline, Statistics (Columbia University) Deadline : review of applications begins February 1, 2025, and will continue until position is filled. Anticipated start date is Fall 2024. Faculty Positions (multiple) : Marine, Atmospheric, Environmental or Earth Sciences with Data Science Expertise (Univ. of Miami) Deadline : Review of applications is ongoing, and each appointment is expected to start as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about-3/opportunities/#content",
    "transcript": "Opportunities - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Job Opportunities LEAP Center Opportunities **NEW** Climate Data Science Education + Outreach Coordinator Deadline : open until filled, with ideal start in February 2025. Student Opportunities Please check back for future opportunities! Postdoctoral Opportunities **NEW** Postdoctoral Researcher, Quantitative Ecosystem Dynamics Lab (UC Berkeley) Deadline : December 22, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Extreme Ocean Events) (UC Irvine) Deadline : December 31, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Marine Heat Waves) (UC Irvine) Deadline : December 31, 2024 **NEW** Postdoctoral Research Associate, Atmospheric Sciences + Machine Learning (NYU Courant Institute) Deadline : January 15, 2025, with anticipated start date of March 2025. **NEW** Postdoctoral Research Scientist, Physical Oceanography (LDEO/Columbia) Deadline : ASAP, with anticipated start date of January 1, 2025 Faculty + Research Opportunities **NEW** Software Engineer, Global Ocean And Land Alkalinization DoE Earth Shot Project (Yale School of the Environment) **NEW** Geospatial Data Engineer (Planette AI) **NEW** Assistant Professor (limited term), Statistics (Columbia University) Deadline : review of applications begins December 1, 2024, and will continue until position is filled. **NEW** Lecturer in Discipline, Statistics (Columbia University) Deadline : review of applications begins February 1, 2025, and will continue until position is filled. Anticipated start date is Fall 2024. Faculty Positions (multiple) : Marine, Atmospheric, Environmental or Earth Sciences with Data Science Expertise (Univ. of Miami) Deadline : Review of applications is ongoing, and each appointment is expected to start as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/leap-policies/",
    "transcript": "LEAP Policies - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Policies Acknowledgements Acknowledging LEAP Support in Publications All papers supported in full or in part by LEAP should acknowledge LEAP funding in the Acknowledgements section as follows: ‚ÄúWe acknowledge funding from NSF through the Learning the Earth with Artificial intelligence and Physics (LEAP) Science and Technology Center (STC) (Award #2019625).‚Äù Center Documents LEAP Ethics Handbook Other LEAP Center documents are currently being updated. Types of Membership LEAP‚Äôs Executive Committee invites you to APPLY TO the following types of LEAP Membership: PUBLIC MEMBERSHIP. (Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP. Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP. Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHER MEMBERSHIP. Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. Innovation Hub Space Use Policy LEAP‚Äôs Executive Committee is pleased to share the Innovation Hub Space Use Policy portion of our Operations Manual. As no written policy can cover all situations, we expect that questions will arise. Please contact LEAP‚Äôs Managing Director, Molly Lopez , with any questions or concerns. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/leap-policies/#content",
    "transcript": "LEAP Policies - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Policies Acknowledgements Acknowledging LEAP Support in Publications All papers supported in full or in part by LEAP should acknowledge LEAP funding in the Acknowledgements section as follows: ‚ÄúWe acknowledge funding from NSF through the Learning the Earth with Artificial intelligence and Physics (LEAP) Science and Technology Center (STC) (Award #2019625).‚Äù Center Documents LEAP Ethics Handbook Other LEAP Center documents are currently being updated. Types of Membership LEAP‚Äôs Executive Committee invites you to APPLY TO the following types of LEAP Membership: PUBLIC MEMBERSHIP. (Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP. Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP. Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHER MEMBERSHIP. Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. Innovation Hub Space Use Policy LEAP‚Äôs Executive Committee is pleased to share the Innovation Hub Space Use Policy portion of our Operations Manual. As no written policy can cover all situations, we expect that questions will arise. Please contact LEAP‚Äôs Managing Director, Molly Lopez , with any questions or concerns. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/news/",
    "transcript": "News - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP News Applications Open for LEAP‚Äôs 2025 Summer Institute LEAP‚Äôs Winter 2025 Momentum Bootcamp Receives Approval to Issue Continuing Teacher + Leader Education (CTLE) Credits LEAP KT Research: Understanding Climate Modeling Across Societal Sectors In Brief: Winter 2025 LEAP Momentum Bootcamp: Call for Applications In Brief: January 2025 Hackathon: Call for Applications 1st ClimateML Methods Workshop Hosted by LEAP's JSAG Pierre Gentine Answers the Question: \"Can AI Help Save Our Planet?\" (Columbia Engineering News) Sara Shamekh Named a U.S. DoE 2024 Early Career Research Program (ECRP) Awardee: \"UNSHADE: Understanding and Modeling of Shallow to Deep Convection Transition\" Kaitlyn Loftus, Co-Investigator on DOE ASR-Funded Project: \"Identifying and Reducing Structural Errors in Parameterized Warm Rain Processes Using ARM EPCAPE and ENA Observations\" Julia Simpson Named a 2024 National Defense Science + Engineering Graduate (NDSEG) Fellowship Program Awardee Dhruv Balwada Named as Recipient of 2025 America Meteorological Society (AMS) Nicholas P. Fofonoff Award LEAP's NYC Summer Institute: Teachers Empower the Next Generation to Address Climate Change News Archive Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/news/#content",
    "transcript": "News - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP News Applications Open for LEAP‚Äôs 2025 Summer Institute LEAP‚Äôs Winter 2025 Momentum Bootcamp Receives Approval to Issue Continuing Teacher + Leader Education (CTLE) Credits LEAP KT Research: Understanding Climate Modeling Across Societal Sectors In Brief: Winter 2025 LEAP Momentum Bootcamp: Call for Applications In Brief: January 2025 Hackathon: Call for Applications 1st ClimateML Methods Workshop Hosted by LEAP's JSAG Pierre Gentine Answers the Question: \"Can AI Help Save Our Planet?\" (Columbia Engineering News) Sara Shamekh Named a U.S. DoE 2024 Early Career Research Program (ECRP) Awardee: \"UNSHADE: Understanding and Modeling of Shallow to Deep Convection Transition\" Kaitlyn Loftus, Co-Investigator on DOE ASR-Funded Project: \"Identifying and Reducing Structural Errors in Parameterized Warm Rain Processes Using ARM EPCAPE and ENA Observations\" Julia Simpson Named a 2024 National Defense Science + Engineering Graduate (NDSEG) Fellowship Program Awardee Dhruv Balwada Named as Recipient of 2025 America Meteorological Society (AMS) Nicholas P. Fofonoff Award LEAP's NYC Summer Institute: Teachers Empower the Next Generation to Address Climate Change News Archive Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/news/news-sub/",
    "transcript": "LEAP News - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP News Applications Open for LEAP‚Äôs 2025 Summer Institute LEAP‚Äôs Winter 2025 Momentum Bootcamp Receives Approval to Issue Continuing Teacher + Leader Education (CTLE) Credits LEAP KT Research: Understanding Climate Modeling Across Societal Sectors In Brief: Winter 2025 LEAP Momentum Bootcamp: Call for Applications In Brief: January 2025 Hackathon: Call for Applications 1st ClimateML Methods Workshop Hosted by LEAP's JSAG Pierre Gentine Answers the Question: \"Can AI Help Save Our Planet?\" (Columbia Engineering News) Sara Shamekh Named a U.S. DoE 2024 Early Career Research Program (ECRP) Awardee: \"UNSHADE: Understanding and Modeling of Shallow to Deep Convection Transition\" Kaitlyn Loftus, Co-Investigator on DOE ASR-Funded Project: \"Identifying and Reducing Structural Errors in Parameterized Warm Rain Processes Using ARM EPCAPE and ENA Observations\" Julia Simpson Named a 2024 National Defense Science + Engineering Graduate (NDSEG) Fellowship Program Awardee Dhruv Balwada Named as Recipient of 2025 America Meteorological Society (AMS) Nicholas P. Fofonoff Award LEAP's NYC Summer Institute: Teachers Empower the Next Generation to Address Climate Change News Archive Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/news/news-sub/#content",
    "transcript": "LEAP News - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP News Applications Open for LEAP‚Äôs 2025 Summer Institute LEAP‚Äôs Winter 2025 Momentum Bootcamp Receives Approval to Issue Continuing Teacher + Leader Education (CTLE) Credits LEAP KT Research: Understanding Climate Modeling Across Societal Sectors In Brief: Winter 2025 LEAP Momentum Bootcamp: Call for Applications In Brief: January 2025 Hackathon: Call for Applications 1st ClimateML Methods Workshop Hosted by LEAP's JSAG Pierre Gentine Answers the Question: \"Can AI Help Save Our Planet?\" (Columbia Engineering News) Sara Shamekh Named a U.S. DoE 2024 Early Career Research Program (ECRP) Awardee: \"UNSHADE: Understanding and Modeling of Shallow to Deep Convection Transition\" Kaitlyn Loftus, Co-Investigator on DOE ASR-Funded Project: \"Identifying and Reducing Structural Errors in Parameterized Warm Rain Processes Using ARM EPCAPE and ENA Observations\" Julia Simpson Named a 2024 National Defense Science + Engineering Graduate (NDSEG) Fellowship Program Awardee Dhruv Balwada Named as Recipient of 2025 America Meteorological Society (AMS) Nicholas P. Fofonoff Award LEAP's NYC Summer Institute: Teachers Empower the Next Generation to Address Climate Change News Archive Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/news/news-archive/",
    "transcript": "Archive - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP News Archive PLOS Climate Article Published: \"Ethics n Climate AI: From Theory to Practice\" LEAP + Climatematch Academy: Supporting Climate Impact Scholars How Much Carbon Can the Ocean Hold? Lamont Researchers Aim to Find out The Ocean Carbon Sink: A Data Story Tian Zheng Elected as a Fellow of the American Association for the Advancement of Science (AAAS) LEAP Collaborates on \"Carbon Catchers\" Experience at the American Museum of Natural History's EarthFest 2024 Kara Lamb: Ice-Cloud Puzzle Master Summer Institute 2023-2024 Showcase: Climate Change Education Marco Giometto Receives NSF CAREER Award Christina Torres Discusses Integration of Climate Change Education in NYC Public Schools A New Paper with University of Lausanne: \"Climate-Invariant Machine Learning\" 2024 Winter Momentum Bootcamp on Climate Data Science Hosted at Teachers College‚Äã In Brief: Winter 2024 LEAP Momentum Bootcamp: Call for Applications Kara Lamb and Pierre Gentine Awarded 3-Year Zegar Family Foundation Grant for ‚ÄúAI Cloud Seeding Project‚Äù In Brief: 2023 Summer REU Concluding Reflections Hechinger Report || 2023 Summer Institute: Integrating Climate Education in NYC Public Schools In Brief: Meet the 2023 Summer Momentum Fellows In Brief: Sungduk Yu Leads \"ClimSim\" Project, Pushing the Limits of Hybrid AI-Physics Climate Modeling Vanessa Burbano Receives 2023 SMS Emerging Scholar Award A New Study Finds That a Majority of Americans Want Climate Change Education for K-12 Students A Leap Towards a Sustainable Earth: Q+A with Climate Expert Pierre Gentine In Brief: Announcing the 2023 NYC Lab-to-School Summer Institute In Brief: Summer 2023 LEAP Momentum Fellowship: Call for Applications In Brief: LEAP Announces Andr√© Hoffmann Fellowship with the World Economic Forum (WEF) In Brief: LEAP Announces 2023 Summer REU Program Compound Extreme Heat and Drought Will Hit 90% of World Population 2023 Train-the-Trainer Bootcamp on Climate Data Science Hosted at Teachers College Climate Modeling Reaches for the Next Level of Precision In Brief: LEAP Announces January 2023 Train-the-Trainer Bootcamp on Climate Data Science LEAP's Director, Pierre Gentine, is 2022 AGU James B. Macelwane Medal Recipient A Global Teach-in Explores Paths from Classrooms to Climate Change Impact | Meta Bulletin In Brief: LEAP Announces May 2022 Momentum Bootcamp on Climate Data Science In Brief: LEAP Partners with NCAR for TAI4ES Summer School In Brief: LEAP Launches Summer REU Program Building Better Climate Models Columbia to Launch $25 Million AI-based Climate Modeling Center How Next-Generation Models Will Leverage Big Data and AI for More Accurate Estimates of Future Climate Columbia Engineering, Arts and Sciences, and Columbia Climate School to Launch $25M Climate Modeling Center Climate Intelligence Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/news/news-archive/#content",
    "transcript": "Archive - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP News Archive PLOS Climate Article Published: \"Ethics n Climate AI: From Theory to Practice\" LEAP + Climatematch Academy: Supporting Climate Impact Scholars How Much Carbon Can the Ocean Hold? Lamont Researchers Aim to Find out The Ocean Carbon Sink: A Data Story Tian Zheng Elected as a Fellow of the American Association for the Advancement of Science (AAAS) LEAP Collaborates on \"Carbon Catchers\" Experience at the American Museum of Natural History's EarthFest 2024 Kara Lamb: Ice-Cloud Puzzle Master Summer Institute 2023-2024 Showcase: Climate Change Education Marco Giometto Receives NSF CAREER Award Christina Torres Discusses Integration of Climate Change Education in NYC Public Schools A New Paper with University of Lausanne: \"Climate-Invariant Machine Learning\" 2024 Winter Momentum Bootcamp on Climate Data Science Hosted at Teachers College‚Äã In Brief: Winter 2024 LEAP Momentum Bootcamp: Call for Applications Kara Lamb and Pierre Gentine Awarded 3-Year Zegar Family Foundation Grant for ‚ÄúAI Cloud Seeding Project‚Äù In Brief: 2023 Summer REU Concluding Reflections Hechinger Report || 2023 Summer Institute: Integrating Climate Education in NYC Public Schools In Brief: Meet the 2023 Summer Momentum Fellows In Brief: Sungduk Yu Leads \"ClimSim\" Project, Pushing the Limits of Hybrid AI-Physics Climate Modeling Vanessa Burbano Receives 2023 SMS Emerging Scholar Award A New Study Finds That a Majority of Americans Want Climate Change Education for K-12 Students A Leap Towards a Sustainable Earth: Q+A with Climate Expert Pierre Gentine In Brief: Announcing the 2023 NYC Lab-to-School Summer Institute In Brief: Summer 2023 LEAP Momentum Fellowship: Call for Applications In Brief: LEAP Announces Andr√© Hoffmann Fellowship with the World Economic Forum (WEF) In Brief: LEAP Announces 2023 Summer REU Program Compound Extreme Heat and Drought Will Hit 90% of World Population 2023 Train-the-Trainer Bootcamp on Climate Data Science Hosted at Teachers College Climate Modeling Reaches for the Next Level of Precision In Brief: LEAP Announces January 2023 Train-the-Trainer Bootcamp on Climate Data Science LEAP's Director, Pierre Gentine, is 2022 AGU James B. Macelwane Medal Recipient A Global Teach-in Explores Paths from Classrooms to Climate Change Impact | Meta Bulletin In Brief: LEAP Announces May 2022 Momentum Bootcamp on Climate Data Science In Brief: LEAP Partners with NCAR for TAI4ES Summer School In Brief: LEAP Launches Summer REU Program Building Better Climate Models Columbia to Launch $25 Million AI-based Climate Modeling Center How Next-Generation Models Will Leverage Big Data and AI for More Accurate Estimates of Future Climate Columbia Engineering, Arts and Sciences, and Columbia Climate School to Launch $25M Climate Modeling Center Climate Intelligence Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/",
    "transcript": "Research - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research @ LEAP Overview LEAP research aims to reduce errors in near-future climate projections by developing novel machine learning (ML) algorithms which will better extrapolate by including domain and causal knowledge, while aggressively leveraging the wealth of recent datasets. Research Focus LEAP will focus on hybridizing ML by integrating physical knowledge for implementation with the open-source Community Earth System Model (CESM) by: Reducing the existing model structural errors related to the lack of comprehension of the process at play (e.g., clouds + microphysics); Optimally estimating the model parameters using a Bayesian approach; and Developing new observational products which will be used to evaluate the CESM skill. Research Impacts The Problem Climate scientists and modelers struggle to fully integrate the wealth of existing datasets into their models, while ML algorithms have been good at replacing and interpolating, but less targeted at extrapolating. Our Solution LEAP‚Äôs research triggers a significant advancement for data science applied to physical problems, by incorporating physics into ML algorithms to develop a next-generation CESM for better generalization and extrapolation, while optimally using the wealth of data available to climate science. Our scientists merge physical modeling with ML across a ‚ÄúKnowledge-Data Continuum,‚Äù benefiting both scientific communities by providing a template for other climate modeling centers, as well as offering a more accurate approach to predicting the future of our climate. The Problem Compounding historical shortcomings in climate science and computing capacities that limit the accuracy of climate projections, inaccessibility to trustworthy, relevant, and accurate climate-related information makes it challenging for communities to optimally adapt and develop resiliency in the face of climate change. Our Solution By pursuing deep collaboration and engagement between climate data scientists, national research labs, public and private stakeholders, and other partners, LEAP will develop tailored and relevant climate-related information that will provide improved climate projections 10 to 40 years into the future. Communicated in ways that are more digestible and relevant, this information will benefit industry, government, and local communities with increased mitigation knowledge, more precise risk quantification, and the ability to better adapt to climate change to improve life for generations to come. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/#content",
    "transcript": "Research - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research @ LEAP Overview LEAP research aims to reduce errors in near-future climate projections by developing novel machine learning (ML) algorithms which will better extrapolate by including domain and causal knowledge, while aggressively leveraging the wealth of recent datasets. Research Focus LEAP will focus on hybridizing ML by integrating physical knowledge for implementation with the open-source Community Earth System Model (CESM) by: Reducing the existing model structural errors related to the lack of comprehension of the process at play (e.g., clouds + microphysics); Optimally estimating the model parameters using a Bayesian approach; and Developing new observational products which will be used to evaluate the CESM skill. Research Impacts The Problem Climate scientists and modelers struggle to fully integrate the wealth of existing datasets into their models, while ML algorithms have been good at replacing and interpolating, but less targeted at extrapolating. Our Solution LEAP‚Äôs research triggers a significant advancement for data science applied to physical problems, by incorporating physics into ML algorithms to develop a next-generation CESM for better generalization and extrapolation, while optimally using the wealth of data available to climate science. Our scientists merge physical modeling with ML across a ‚ÄúKnowledge-Data Continuum,‚Äù benefiting both scientific communities by providing a template for other climate modeling centers, as well as offering a more accurate approach to predicting the future of our climate. The Problem Compounding historical shortcomings in climate science and computing capacities that limit the accuracy of climate projections, inaccessibility to trustworthy, relevant, and accurate climate-related information makes it challenging for communities to optimally adapt and develop resiliency in the face of climate change. Our Solution By pursuing deep collaboration and engagement between climate data scientists, national research labs, public and private stakeholders, and other partners, LEAP will develop tailored and relevant climate-related information that will provide improved climate projections 10 to 40 years into the future. Communicated in ways that are more digestible and relevant, this information will benefit industry, government, and local communities with increased mitigation knowledge, more precise risk quantification, and the ability to better adapt to climate change to improve life for generations to come. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/research-sub/",
    "transcript": "LEAP Research - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research @ LEAP Overview LEAP research aims to reduce errors in near-future climate projections by developing novel machine learning (ML) algorithms which will better extrapolate by including domain and causal knowledge, while aggressively leveraging the wealth of recent datasets. Research Focus LEAP will focus on hybridizing ML by integrating physical knowledge for implementation with the open-source Community Earth System Model (CESM) by: Reducing the existing model structural errors related to the lack of comprehension of the process at play (e.g., clouds + microphysics); Optimally estimating the model parameters using a Bayesian approach; and Developing new observational products which will be used to evaluate the CESM skill. Research Impacts The Problem Climate scientists and modelers struggle to fully integrate the wealth of existing datasets into their models, while ML algorithms have been good at replacing and interpolating, but less targeted at extrapolating. Our Solution LEAP‚Äôs research triggers a significant advancement for data science applied to physical problems, by incorporating physics into ML algorithms to develop a next-generation CESM for better generalization and extrapolation, while optimally using the wealth of data available to climate science. Our scientists merge physical modeling with ML across a ‚ÄúKnowledge-Data Continuum,‚Äù benefiting both scientific communities by providing a template for other climate modeling centers, as well as offering a more accurate approach to predicting the future of our climate. The Problem Compounding historical shortcomings in climate science and computing capacities that limit the accuracy of climate projections, inaccessibility to trustworthy, relevant, and accurate climate-related information makes it challenging for communities to optimally adapt and develop resiliency in the face of climate change. Our Solution By pursuing deep collaboration and engagement between climate data scientists, national research labs, public and private stakeholders, and other partners, LEAP will develop tailored and relevant climate-related information that will provide improved climate projections 10 to 40 years into the future. Communicated in ways that are more digestible and relevant, this information will benefit industry, government, and local communities with increased mitigation knowledge, more precise risk quantification, and the ability to better adapt to climate change to improve life for generations to come. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/research-sub/#content",
    "transcript": "LEAP Research - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research @ LEAP Overview LEAP research aims to reduce errors in near-future climate projections by developing novel machine learning (ML) algorithms which will better extrapolate by including domain and causal knowledge, while aggressively leveraging the wealth of recent datasets. Research Focus LEAP will focus on hybridizing ML by integrating physical knowledge for implementation with the open-source Community Earth System Model (CESM) by: Reducing the existing model structural errors related to the lack of comprehension of the process at play (e.g., clouds + microphysics); Optimally estimating the model parameters using a Bayesian approach; and Developing new observational products which will be used to evaluate the CESM skill. Research Impacts The Problem Climate scientists and modelers struggle to fully integrate the wealth of existing datasets into their models, while ML algorithms have been good at replacing and interpolating, but less targeted at extrapolating. Our Solution LEAP‚Äôs research triggers a significant advancement for data science applied to physical problems, by incorporating physics into ML algorithms to develop a next-generation CESM for better generalization and extrapolation, while optimally using the wealth of data available to climate science. Our scientists merge physical modeling with ML across a ‚ÄúKnowledge-Data Continuum,‚Äù benefiting both scientific communities by providing a template for other climate modeling centers, as well as offering a more accurate approach to predicting the future of our climate. The Problem Compounding historical shortcomings in climate science and computing capacities that limit the accuracy of climate projections, inaccessibility to trustworthy, relevant, and accurate climate-related information makes it challenging for communities to optimally adapt and develop resiliency in the face of climate change. Our Solution By pursuing deep collaboration and engagement between climate data scientists, national research labs, public and private stakeholders, and other partners, LEAP will develop tailored and relevant climate-related information that will provide improved climate projections 10 to 40 years into the future. Communicated in ways that are more digestible and relevant, this information will benefit industry, government, and local communities with increased mitigation knowledge, more precise risk quantification, and the ability to better adapt to climate change to improve life for generations to come. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/research-projects-years-3and4/",
    "transcript": "Research Projects || Years 3 ‚Äì 4 - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research Projects || Years 3 - 4 FOUNDATION MODELS FOR GENERATIVE CLIMATE FORECASTING AND DOWNSCALING Stephan Mandt (PI, UC Irvine) Francesco Immorlano (Postdoc, UC Irvine) THE METRICS RELOADED: IMPROVED SIMILARITY ASSESSMENT FOR CLIMATE MAPS Viviana Acquaviva (PI, CUNY / Columbia) Gabriele Accarino (Postdoc, Columbia) IMPROVING THE PARAMETERIZATION OF ATMOSPHERIC BOUNDARY LAYER USING PHYSICS-INFORMED MACHINE LEARNING Sara Shamekh (PI, NYU) Matthieu Blanke (Postdoc, NYU) TASK AWARE MODELING FOR ENVIRONMENTAL SYSTEMS Vipin Kumar (PI, Univ. of Minnesota) Arvind Renganathan (Postdoc, Univ. of Minnesota) LEAP: PHYSICS-IMPLEMENTATION EXPERIENCE WITH CESM (PIECES) Brian Medeiros (PI, NCAR) Addisu Semie (Associate Research Scientist, NCAR) Qingyuan Yang (Associate Research Scientist, Columbia) ATTRIBUTING CLIMATE MODEL DIVERGENCE WITH HIERARCHICAL REPRESENTATION TRANSFORMERS David John Gagne (PI, NCAR) Da Fan (Postdoc, NCAR) LEVERAGING ML TO ADVANCE LAND MODELING THROUGH PARAMETER ESTIMATION AND ML PARAMETERIZATIONS Dave Lawrence (PI, NCAR) Linnia Hawkins (Associate Research Scientist, Columbia) ML-ENHANCED UNIFIED ICE MICROPHYSICS SCHEME (ML-ICE) DEVELOPMENT Kara Lamb (PI, Columbia) Joseph Ko (Postdoc, Columbia) WARM RAIN MICROPHYSICS Marcus van Lier-Walqui (PI, Columbia) Kaitlyn Loftus (Postdoc, Columbia) IMPLEMENTATION OF AUTOMATED CALIBRATION FRAMEWORK INTO CAM AND ANALYSIS OF CALIBRATED ENSEMBLES Greg Elsaesser (PI, Columbia) PARAMETERIZING IMPACTS OF HETEROGENEITY AT THE AIR-SEA INTERFACE Dhruv Balwada (PI, Columbia) Prani Nalluri (Graduate Research Assistant, Columbia) TRANSFER LEARNING FOR CLIMATE SCIENCE Samory Kpotufe (PI, Columbia) NEW DATA ASSIMILATION Pierre Gentine (Co-PI, Columbia) Tian Zheng (Co-PI, Columbia) Shuolin ‚ÄúShawn‚Äù Li (Postdoc, Columbia) Joseph Lockwood (Postdoc, Columbia) IDEALIZED MODELS OF SPECTROSCOPY FOR ATMOSPHERIC MODELING Pierre Gentine (Co-PI, Columbia) Robert Pincus (Co-PI, Columbia) Guillaume Bertoli (Postdoctoral Fellow, Columbia) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/research-projects-years-3and4/#content",
    "transcript": "Research Projects || Years 3 ‚Äì 4 - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research Projects || Years 3 - 4 FOUNDATION MODELS FOR GENERATIVE CLIMATE FORECASTING AND DOWNSCALING Stephan Mandt (PI, UC Irvine) Francesco Immorlano (Postdoc, UC Irvine) THE METRICS RELOADED: IMPROVED SIMILARITY ASSESSMENT FOR CLIMATE MAPS Viviana Acquaviva (PI, CUNY / Columbia) Gabriele Accarino (Postdoc, Columbia) IMPROVING THE PARAMETERIZATION OF ATMOSPHERIC BOUNDARY LAYER USING PHYSICS-INFORMED MACHINE LEARNING Sara Shamekh (PI, NYU) Matthieu Blanke (Postdoc, NYU) TASK AWARE MODELING FOR ENVIRONMENTAL SYSTEMS Vipin Kumar (PI, Univ. of Minnesota) Arvind Renganathan (Postdoc, Univ. of Minnesota) LEAP: PHYSICS-IMPLEMENTATION EXPERIENCE WITH CESM (PIECES) Brian Medeiros (PI, NCAR) Addisu Semie (Associate Research Scientist, NCAR) Qingyuan Yang (Associate Research Scientist, Columbia) ATTRIBUTING CLIMATE MODEL DIVERGENCE WITH HIERARCHICAL REPRESENTATION TRANSFORMERS David John Gagne (PI, NCAR) Da Fan (Postdoc, NCAR) LEVERAGING ML TO ADVANCE LAND MODELING THROUGH PARAMETER ESTIMATION AND ML PARAMETERIZATIONS Dave Lawrence (PI, NCAR) Linnia Hawkins (Associate Research Scientist, Columbia) ML-ENHANCED UNIFIED ICE MICROPHYSICS SCHEME (ML-ICE) DEVELOPMENT Kara Lamb (PI, Columbia) Joseph Ko (Postdoc, Columbia) WARM RAIN MICROPHYSICS Marcus van Lier-Walqui (PI, Columbia) Kaitlyn Loftus (Postdoc, Columbia) IMPLEMENTATION OF AUTOMATED CALIBRATION FRAMEWORK INTO CAM AND ANALYSIS OF CALIBRATED ENSEMBLES Greg Elsaesser (PI, Columbia) PARAMETERIZING IMPACTS OF HETEROGENEITY AT THE AIR-SEA INTERFACE Dhruv Balwada (PI, Columbia) Prani Nalluri (Graduate Research Assistant, Columbia) TRANSFER LEARNING FOR CLIMATE SCIENCE Samory Kpotufe (PI, Columbia) NEW DATA ASSIMILATION Pierre Gentine (Co-PI, Columbia) Tian Zheng (Co-PI, Columbia) Shuolin ‚ÄúShawn‚Äù Li (Postdoc, Columbia) Joseph Lockwood (Postdoc, Columbia) IDEALIZED MODELS OF SPECTROSCOPY FOR ATMOSPHERIC MODELING Pierre Gentine (Co-PI, Columbia) Robert Pincus (Co-PI, Columbia) Guillaume Bertoli (Postdoctoral Fellow, Columbia) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/projects-yrs1and2/",
    "transcript": "Research Projects || Years 1 ‚Äì 2 - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research Projects || Years 1 - 2 OPEN-SOURCE PROCESS FOR ROUTINE PARAMETER ESTIMATION + UNCERTAINTY QUANTIFICATION FOR THE CLM Dave Lawrence (Co-PI, NCAR) Katie Dagon (Co-PI, NCAR) Linnia Hawkins (Associate Research Scientist, Columbia) STATISTICAL METHODS FOR FUNCTIONAL REGRESSION FOR SPATIAL TEMPORAL DYNAMICS Tian Zheng (Co-PI, Columbia) Galen McKinley (Co-PI, Columbia) Candace Agonafir (Associate Research Scientist, Columbia) CONSTRAINING PHENOLOGY WITH MACHINE LEARNING Pierre Gentine (Co-PI, Columbia) Dave Lawrence (Co-PI, NCAR) Aya Lahlou (Graduate Research Assistant, Columbia) LOW-CLOUD TURBULENCE USING MACHINE LEARNING Pierre Gentine (Co-PI, Columbia) Mike Pritchard (Co-PI, UC Irvine / NVIDIA) Yongquan Qu (Graduate Research Assistant, Columbia) METRICS FOR CESM: MACHINE-LEARNING BASED AIR-SEA CO2 FLUXES Galen McKinley (Co-PI, Columbia) Matt Long (Co-PI, NCAR) Abby Shaum (Graduate Research Assistant, Columbia) LEARNING OPTIMAL RELATIONSHIPS FOR SOLAR RADIATION Robert Pincus (Co-PI, Columbia) Carl Vondrick (Co-PI, Columbia) Dion Ho (Graduate Research Assistant, Columbia) WARM RAIN MICROPHYSICS EMULATION Marcus van Lier-Walqui (PI, Columbia) Kaitlyn Loftus (Postdoc, Columbia) DEEP LEARNING BASED INVERSE MODELING FOR PARAMETER ESTIMATION Vipin Kumar (Co-PI, Univ. of Minnesota) Dave Lawrence (Co-PI, NCAR) Arvind Renganathan (Postdoc, Univ. of Minnesota) UNIFICATION OF LABORATORY + OBSERVATIONAL DATA VIA LEARNING ALGORITHMS FOR ROBUST MODELS OF ICE MICROPHYSICS Kara Lamb (PI, Columbia) Joseph Ko (Postdoc, Columbia) OPTIMIZED SYSTEMATIC CALIBRATION OF EARTH SYSTEM MODEL PHYSICS Greg Elsaesser (PI, NASA-GISS) Qingyuan Yang (Associate Research Scientist, Columbia) PHYSICS DATA DRIVEN SURROGATE MODELS FOR VEGETATION-ATMOSPHERE INTERACTION: IMPROVED PARAMETERIZATION OF TURBULENT EXCHANGES BETWEEN PLANT CANOPIES AND THE ATMOSPHERE Marco Giometto (Co-PI, Columbia) Ensheng Weng (Co-PI, Columbia) Jaeyoung Jung (Postdoc, Columbia) LEAP CRYO: LEARNING NEW ICE SHEET SLIDING LAWS WITH NEURAL NETS + HIGH-RESOLUTION ICE MODELS Jonny Kingslake (Co-PI, Columbia) David Porter (Co-PI, Columbia) Racheet Matai (Postdoc, Columbia) TOWARDS ROBUST, RELIABLE, AND OPERATIONAL SUPER-PARAMETERIZATION EMULATORS Mike Pritchard (Co-PI, UC Irvine / NVIDIA) Pierre Gentine (Co-PI, Columbia) Sungduk Yu (Postdoc, UC Irvine) Savannah Ferretti (Graduate Research Assistant, UC Irvine) Jerry Lin (Graduate Research Assistant, UC Irvine) RETHINKING AIR-SEA FLUXES PARAMETERIZATION Laure Zanna (Co-PI, NYU) David John Gagne (Co-PI, NCAR) Jiarong Wu (Postdoc, NYU) CORPORATE STAKEHOLDERS‚Äô RESPONSES TO CLIMATE INITIATIVES AND CLIMATE-BASED COMMUNICATIONS + INFORMATION Vanessa Burbano (PI, Columbia) Florencio Portocarrero (Postdoc, Columbia) LAB TO SCHOOL: SUMMER INSTITUTE FOR K-12 TEACHERS IN NYC Oren Pizmony-Levy (Co-PI, Teachers College) Charles Lang (Co-PI, Teachers College) Christina Torres (Graduate Research Assistant, Teachers College) NEURO-SYMBOLIC MACHINE LEARNING FOR CLIMATE Carl Vondrick (PI, Columbia) REVISITING AIR-SEA FLUX INTERACTIONS WITH ML Pierre Gentine (Co-PI, Columbia) Laure Zanna (Co-PI, NYU) Julia Simpson (Graduate Research Assistant, Columbia) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/projects-yrs1and2/#content",
    "transcript": "Research Projects || Years 1 ‚Äì 2 - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Research Projects || Years 1 - 2 OPEN-SOURCE PROCESS FOR ROUTINE PARAMETER ESTIMATION + UNCERTAINTY QUANTIFICATION FOR THE CLM Dave Lawrence (Co-PI, NCAR) Katie Dagon (Co-PI, NCAR) Linnia Hawkins (Associate Research Scientist, Columbia) STATISTICAL METHODS FOR FUNCTIONAL REGRESSION FOR SPATIAL TEMPORAL DYNAMICS Tian Zheng (Co-PI, Columbia) Galen McKinley (Co-PI, Columbia) Candace Agonafir (Associate Research Scientist, Columbia) CONSTRAINING PHENOLOGY WITH MACHINE LEARNING Pierre Gentine (Co-PI, Columbia) Dave Lawrence (Co-PI, NCAR) Aya Lahlou (Graduate Research Assistant, Columbia) LOW-CLOUD TURBULENCE USING MACHINE LEARNING Pierre Gentine (Co-PI, Columbia) Mike Pritchard (Co-PI, UC Irvine / NVIDIA) Yongquan Qu (Graduate Research Assistant, Columbia) METRICS FOR CESM: MACHINE-LEARNING BASED AIR-SEA CO2 FLUXES Galen McKinley (Co-PI, Columbia) Matt Long (Co-PI, NCAR) Abby Shaum (Graduate Research Assistant, Columbia) LEARNING OPTIMAL RELATIONSHIPS FOR SOLAR RADIATION Robert Pincus (Co-PI, Columbia) Carl Vondrick (Co-PI, Columbia) Dion Ho (Graduate Research Assistant, Columbia) WARM RAIN MICROPHYSICS EMULATION Marcus van Lier-Walqui (PI, Columbia) Kaitlyn Loftus (Postdoc, Columbia) DEEP LEARNING BASED INVERSE MODELING FOR PARAMETER ESTIMATION Vipin Kumar (Co-PI, Univ. of Minnesota) Dave Lawrence (Co-PI, NCAR) Arvind Renganathan (Postdoc, Univ. of Minnesota) UNIFICATION OF LABORATORY + OBSERVATIONAL DATA VIA LEARNING ALGORITHMS FOR ROBUST MODELS OF ICE MICROPHYSICS Kara Lamb (PI, Columbia) Joseph Ko (Postdoc, Columbia) OPTIMIZED SYSTEMATIC CALIBRATION OF EARTH SYSTEM MODEL PHYSICS Greg Elsaesser (PI, NASA-GISS) Qingyuan Yang (Associate Research Scientist, Columbia) PHYSICS DATA DRIVEN SURROGATE MODELS FOR VEGETATION-ATMOSPHERE INTERACTION: IMPROVED PARAMETERIZATION OF TURBULENT EXCHANGES BETWEEN PLANT CANOPIES AND THE ATMOSPHERE Marco Giometto (Co-PI, Columbia) Ensheng Weng (Co-PI, Columbia) Jaeyoung Jung (Postdoc, Columbia) LEAP CRYO: LEARNING NEW ICE SHEET SLIDING LAWS WITH NEURAL NETS + HIGH-RESOLUTION ICE MODELS Jonny Kingslake (Co-PI, Columbia) David Porter (Co-PI, Columbia) Racheet Matai (Postdoc, Columbia) TOWARDS ROBUST, RELIABLE, AND OPERATIONAL SUPER-PARAMETERIZATION EMULATORS Mike Pritchard (Co-PI, UC Irvine / NVIDIA) Pierre Gentine (Co-PI, Columbia) Sungduk Yu (Postdoc, UC Irvine) Savannah Ferretti (Graduate Research Assistant, UC Irvine) Jerry Lin (Graduate Research Assistant, UC Irvine) RETHINKING AIR-SEA FLUXES PARAMETERIZATION Laure Zanna (Co-PI, NYU) David John Gagne (Co-PI, NCAR) Jiarong Wu (Postdoc, NYU) CORPORATE STAKEHOLDERS‚Äô RESPONSES TO CLIMATE INITIATIVES AND CLIMATE-BASED COMMUNICATIONS + INFORMATION Vanessa Burbano (PI, Columbia) Florencio Portocarrero (Postdoc, Columbia) LAB TO SCHOOL: SUMMER INSTITUTE FOR K-12 TEACHERS IN NYC Oren Pizmony-Levy (Co-PI, Teachers College) Charles Lang (Co-PI, Teachers College) Christina Torres (Graduate Research Assistant, Teachers College) NEURO-SYMBOLIC MACHINE LEARNING FOR CLIMATE Carl Vondrick (PI, Columbia) REVISITING AIR-SEA FLUX INTERACTIONS WITH ML Pierre Gentine (Co-PI, Columbia) Laure Zanna (Co-PI, NYU) Julia Simpson (Graduate Research Assistant, Columbia) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/publications-papers-conferences/",
    "transcript": "Publications, Papers, Conferences - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Publications, Papers + Conferences Acknowledgements Acknowledging LEAP Support in Publications All papers supported in full or in part by LEAP should acknowledge LEAP funding in the Acknowledgements section as follows: ‚ÄúWe acknowledge funding from NSF through the Learning the Earth with Artificial intelligence and Physics (LEAP) Science and Technology Center (STC) (Award #2019625).‚Äù Center Publications PEER REVIEWED PUBLICATIONS Behrens, Gunnar, Tom Beucler, Pierre Gentine, et al. ‚ÄúNon-Linear Dimensionality Reduction With a Variational Encoder Decoder to Understand Convective Processes in Climate Models.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 8, 2022, p. e2022MS003130, https://doi.org/10.1029/2022MS003130 . Bennington, Val, Tomislav Galjanic, et al. ‚ÄúExplicit Physical Knowledge in Machine Learning for Ocean Carbon Flux Reconstruction: The PCO2-Residual Method.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 10, 2022, p. e2021MS002960, https://doi.org/10.1029/2021MS002960 . Bennington, Val, Lucas Gloege, et al. ‚ÄúVariability in the Global Ocean Carbon Sink From 1959 to 2020 by Correcting Models With Observations.‚Äù Geophysical Research Letters , vol. 49, no. 14, 2022, p. e2022GL098632, https://doi.org/10.1029/2022GL098632 . Beucler, Tom, et al. ‚ÄúClimate-Invariant Machine Learning.‚Äù Science Advances , vol. 10, no. 6, Feb. 2024, p. eadj7250, https://doi.org/10.1126/sciadv.adj7250 . Bhouri, Mohamed Aziz, and Pierre Gentine. ‚ÄúMemory-Based Parameterization with Differentiable Solver: Application to Lorenz ‚Äô96.‚Äù Chaos: An Interdisciplinary Journal of Nonlinear Science, vol. 33, no. 7, July 2023, p. 073116, https://doi.org/10.1063/5.0131929 . see more... Bu, Jingyi, et al. ‚ÄúDryland Evapotranspiration from Remote Sensing Solar-Induced Chlorophyll Fluorescence: Constraining an Optimal Stomatal Model within a Two-Source Energy Balance Model.‚Äù Remote Sensing of Environment, vol. 303, Mar. 2024, p. 113999, https://doi.org/10.1016/j.rse.2024.113999 . Buch, J., Williams, A., Juang, C., Hansen, W., & Gentine, P. (2023). SMLFire1.0: a stochastic machine learning (SML) model for wildfire activity in the western United States. Geoscientific Model Development , 16(12), 3407-3433. http://dx.doi.org/10.5194/gmd-16-3407-2023 . Chen, Tse-Chun, et al. ‚ÄúCorrecting Systematic and State-Dependent Errors in the NOAA FV3-GFS Using Neural Networks.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 11, 2022, p. e2022MS003309, https://doi.org/10.1029/2022MS003309 . Cheng, Y., Giometto, M. G., Kauffmann, P., Lin, L., Cao, C., Zupnick, C., Abernathey, R. & Gentine, P. Deep learning for subgrid-scale turbulence modeling in large-eddy simulations of the convective atmospheric boundary layer. Journal of Advances in Modeling Earth Systems, 14, e2021MS002847, May 2022, https://doi.org/10.1029/2021MS002847 . Eidhammer, Trude & Gettelman, Andrew & Thayer-Calder, Katherine & Watson-Parris, Duncan & Elsaesser, Gregory & Morrison, Hugh & van Lier-Walqui, Marcus & Song, Ci & Mccoy, Daniel. (2024). An extensible perturbed parameter ensemble for the Community Atmosphere Model version 6. Geoscientific Model Development. 17. 7835-7853. https://doi.org/10.5194/gmd-17-7835-2024 . Eyring, V., Gentine, P., Camps-Valls, G. et al. AI-empowered next-generation multiscale climate modelling for mitigation and adaptation. Nat. Geosci. 17, 963‚Äì971 (2024). https://doi.org/10.1038/s41561-024-01527-w . Eyring, V., Collins, W.D., Gentine, P. et al. Pushing the frontiers in climate modelling and analysis with machine learning. Nat. Clim. Chang. 14, 916‚Äì928 (2024). https://doi.org/10.1038/s41558-024-02095-y . Heimdal, T.H., McKinley, G.A. The importance of adding unbiased Argo observations to the ocean carbon observing system. Sci Rep 14 , 19763 (2024). https://doi.org/10.1038/s41598-024-70617-x . Islam, Ariful, et al. Enhancing Satellite Data Coverage: Leveraging Multiple Sensors to Bridge Information Gaps in Urban Flood Mapping. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1353707 . Giardina, Francesco, et al. Groundwater Rivals Aridity in Determining Global Photosynthesis . 13 Mar. 2024, https://doi.org/10.21203/rs.3.rs-3793488/v1 . Grundner, Arthur, Tom Beucler, Pierre Gentine, and Veronika Eyring. ‚ÄúData-Driven Equation Discovery of a Cloud Cover Parameterization.‚Äù Journal of Advances in Modeling Earth Systems , vol. 16, no. 3, 2024, p. e2023MS003763, https://doi.org/10.1029/2023MS003763 . Grundner, Arthur, Tom Beucler, Pierre Gentine, Fernando Iglesias-Suarez, et al. ‚ÄúDeep Learning Based Cloud Cover Parameterization for ICON.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 12, 2022, p. e2021MS002959, https://doi.org/10.1029/2021MS002959 . Iglesias-Suarez, Fernando, et al. ‚ÄúCausally-Informed Deep Learning to Improve Climate Models and Projections.‚Äù Journal of Geophysical Research: Atmospheres , vol. 129, no. 4, 2024, p. e2023JD039202, https://doi.org/10.1029/2023JD039202 . Improving the Modeling and Analysis of Tropical Convection and Precipitation Through Machine Learning Methods ‚Äì ProQuest . https://www.proquest.com/openview/b21c84cc107ccd37cba0f0a7e7c0c30f/1?pq-origsite=gscholar&cbl=18750&diss=y Accessed 15 Apr. 2024. Li, Xing, et al. ‚ÄúNew-Generation Geostationary Satellite Reveals Widespread Midday Depression in Dryland Photosynthesis during 2020 Western U.S. Heatwave.‚Äù Science Advances, vol. 9, no. 31, Aug. 2023, p. eadi0775, https://doi.org/10.1126/sciadv.adi0775 . Lockwood, J. W., Gori, A., & Gentine, P. (2024). A generative super-resolution model for enhancing tropical cyclone wind field intensity and resolution. Journal of Geophysical Research: Machine Learning and Computation , 1, e2024JH000375. https://doi.org/10.1029/2024JH000375 . McKinley, Galen A., et al. ‚ÄúModern Air-Sea Flux Distributions Reduce Uncertainty in the Future Ocean Carbon Sink.‚Äù Environmental Research Letters , vol. 18, no. 4, Mar. 2023, p. 044011, https://doi.org/10.1088/1748-9326/acc195 . Mooers, Griffin, Mike Pritchard, et al. ‚ÄúComparing Storm Resolving Models and Climates via Unsupervised Machine Learning.‚Äù Scientific Reports , vol. 13, no. 1, Dec. 2023, p. 22365, https://doi.org/10.1038/s41598-023-49455-w . Mooers, Griffin S., et al. ‚ÄúAn Unsupervised Learning Perspective on the Dynamic Contribution to Extreme Precipitation Changes.‚Äù Climate Change AI, Climate Change AI, 2022, https://www.climatechange.ai/papers/neurips2022/81 . Nathaniel, Juan, Jiangong Liu, et al. ‚ÄúMetaFlux: Meta-Learning Global Carbon Fluxes from Sparse Spatiotemporal Observations.‚Äù Scientific Data , vol. 10, no. 1, July 2023, p. 440, https://doi.org/10.1038/s41597-023-02349-y . Pla, Paula, et al. ‚ÄúHydrogenation of C24 Carbon Clusters: Structural Diversity and Energetic Properties.‚Äù The Journal of Physical Chemistry A , vol. 125, no. 24, June 2021, pp. 5273‚Äì88, https://doi.org/10.1021/acs.jpca.1c02359 . Portocarrero, Florencio F., and Vanessa C. Burbano. ‚ÄúThe Effects of a Short-Term Corporate Social Impact Activity on Employee Turnover: Field Experimental Evidence.‚Äù Management Science , Oct. 2023, https://doi.org/10.1287/mnsc.2022.01517 . Schmidt, Gavin. A., et al. ‚ÄúAnomalous Meltwater From Ice Sheets and Ice Shelves Is a Historical Forcing.‚Äù Geophysical Research Letters , vol. 50, no. 24, 2023, p. e2023GL106530, https://doi.org/10.1029/2023GL106530 . Shamekh, Sara, et al. ‚ÄúImplicit Learning of Convective Organization Explains Precipitation Stochasticity.‚Äù Proceedings of the National Academy of Sciences , vol. 120, no. 20, May 2023, p. e2216158120, https://doi.org/10.1073/pnas.2216158120 . Shen, Chaopeng, et al. ‚ÄúDifferentiable Modeling to Unify Machine Learning and Physical Models and Advance Geosciences.‚Äù Nature Reviews Earth & Environment , vol. 4, no. 8, July 2023, pp. 552‚Äì67, https://doi.org/10.1038/s43017-023-00450-9 . Skulovich, Olya, and Pierre Gentine. ‚ÄúA Long-Term Consistent Artificial Intelligence and Remote Sensing-Based Soil Moisture Dataset.‚Äù Scientific Data , vol. 10, no. 1, Mar. 2023, p. 154, https://doi.org/10.1038/s41597-023-02053-x . Wong, S. C., McKinley, G. A., & Seager, R. Equatorial Pacific pCO2 interannual variability in CMIP6 models. Journal of Geophysical Research: Biogeosciences , e2022JG007243, Dec. 2022, https://doi.org/10.1029/2022JG007243 . Yang, Qingyuan, and Susanna F. Jenkins. ‚ÄúTwo Sources of Uncertainty in Estimating Tephra Volumes from Isopachs: Perspectives and Quantification.‚Äù Bulletin of Volcanology , vol. 85, no. 8, July 2023, p. 44, https://doi.org/10.1007/s00445-023-01652-1 . Yu, Sungduk, Walter Hannah, Liran Peng, Jerry Lin, Mike Pritchard, et al. ‚ÄúClimSim: A Large Multi-Scale Dataset for Hybrid Physics-ML Climate Emulation.‚Äù Advances in Neural Information Processing Systems , vol. 36, Dec. 2023, pp. 22070‚Äì84, https://proceedings.neurips.cc/paper_files/paper/2023/hash/45fbcc01349292f5e059a0b8b02c8c3f-Abstract-Datasets_and_Benchmarks.html Zhan, W., Yang, X., Ryu, Y., Dechant, B., Huang, Y., Goulas, Y., ‚Ä¶ & Gentine, P. Two for one: Partitioning CO2 fluxes and understanding the relationship between solar-induced chlorophyll fluorescence and gross primary productivity using machine learning. Agricultural and Forest Meteorology, 321, 108980, Jun. 2022, https://doi.org/10.1016/j.agrformet.2022.108980 . Zhao, Wenli, Biqing Zhu, Steven J. Davis, Philippe Ciais, Chaopeng Hong, Zhu Liu and Pierre Gentine. ‚ÄúReliance on Fossil Fuels Increases during Extreme Temperature Events in the Continental United States.‚Äù Communications Earth & Environment, vol. 4, no. 1, Dec. 2023, pp. 1‚Äì11, https://doi.org/10.1038/s43247-023-01147-z . NON-PEER REVIEWED PUBLICATIONS Agonafir, Candace, and Tian Zheng. ‚ÄúStructured Exploration of Machine Learning Model¬† Complexity for Spatio-Temporal Forecasting of Urban Flooding.‚Äù EGUsphere, Mar. 2024, pp. 1‚Äì32, https://doi.org/10.5194/egusphere-2024-551 . Behrens, Gunnar, Tom Beucler, Fernando Iglesias-Suarez, et al. Improving Atmospheric Processes in Earth System Models with Deep Learning Ensembles and Stochastic Parameterizations . arXiv:2402.03079, arXiv, 5 Feb. 2024, https://doi.org/10.48550/arXiv.2402.03079 . Bhouri, Mohamed Aziz, et al. Multi-Fidelity Climate Model Parameterization for Better Generalization and Extrapolation. arXiv:2309.10231, arXiv, 18 Sept. 2023, https://doi.org/10.48550/arXiv.2309.10231 . Bhouri, Mohamed Aziz, and Pierre Gentine. History-Based, Bayesian, Closure for Stochastic Parameterization: Application to Lorenz ‚Äô96 . arXiv:2210.14488, arXiv, 26 Oct. 2022, https://doi.org/10.48550/arXiv.2210.14488 . Hoffman, Andrew O., et al. ‚ÄúInland Migration of Near-Surface Crevasses in the Amundsen Sea Sector, West Antarctica.‚Äù EGUsphere , Jan. 2024, pp. 1‚Äì29, https://doi.org/10.5194/egusphere-2023-2956 . see more ... Karpatne, Anuj, Xiaowei Jia, and Vipin Kumar (2024). Knowledge-guided Machine Learning: Current Trends and Future Prospects. arXiv:2403.15989, arXiv, 24 Mar. 2024, https://doi.org/10.48550/arXiv.2403.15989 . Kodner, Jordan, et al. Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023) . arXiv:2308.03228, arXiv, 6 Aug. 2023, https://doi.org/10.48550/arXiv.2308.03228 . Lamb, Kara, Marcus van Lier Walqui, Sean Santos, Hugh Morrison. Reduced Order Modeling for Linear Representations of Microphysical Process Rates. https://essopenarchive.org/users/553026/articles/653967-reduced-order-modeling-for-linearized-representations-of-microphysical-process-rates . Langlois, Gabriel P., et al. Efficient First-Order Algorithms for Large-Scale, Non-Smooth Maximum Entropy Models with Application to Wildfire Science . arXiv:2403.06816, arXiv, 11 Mar. 2024, https://doi.org/10.48550/arXiv.2403.06816 . Liao, Kyleen, et al. Simulating the Air Quality Impact of Prescribed Fires Using a Graph Neural Network-Based PM$_{2.5}$ Emissions Forecasting System . arXiv:2312.04291, arXiv, 7 Dec. 2023, https://doi.org/10.48550/arXiv.2312.04291 . Lin, Jerry, et al. Stress-Testing the Coupled Behavior of Hybrid Physics-Machine Learning Climate Simulations on an Unseen, Warmer Climate. arXiv:2401.02098, arXiv, 4 Jan. 2024, https://doi.org/10.48550/arXiv.2401.02098 . Lin, Jerry, et al. Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models . arXiv:2309.16177, arXiv, 28 Sept. 2023, https://doi.org/10.48550/arXiv.2309.16177 . Mooers, Griffin, Tom Beucler, et al. Understanding Extreme Precipitation Changes through Unsupervised Machine Learning . arXiv:2211.01613, arXiv, 1 Dec. 2023, https://doi.org/10.48550/arXiv.2211.01613 . PythonicDISORT: A Python Reimplementation of the Discrete Ordinate Radiative TransferPackage DISORT. Journal of Open Source Software, https://joss.theoj.org/papers/ec19f2796dac7303ea3bd476455b641f . Accessed 22 Apr. 2024. Nathaniel, Juan, Yongquan Qu, et al. ChaosBench: A Multi-Channel, Physics-Based Benchmarkfor Subseasonal-to-Seasonal Climate Prediction . arXiv:2402.00712, arXiv, 1 Mar. 2024, https://doi.org/10.48550/arXiv.2402.00712 . Qu, Yongquan, Juan Nathaniel, Shuolin Li, Pierre Gentine. Deep Generative Data Assimilation in Multimodal Setting. arXiv:2404.06665, arXiv, 9 Apr. 2024, https://doi.org/10.48550/arXiv.2404.06665 . Qu, Yongquan, Mohamed Aziz Bhouri, Pierre Gentine. Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming . 2024, https://doi.org/10.48550/ARXIV.2403.02215 . Renganathan, Arvind, et al. Task Aware Modulation Using Representation Learning: An Approach for Few Shot Learning in Heterogeneous Systems. arXiv:2310.04727, arXiv, 7 Oct. 2023, http://arxiv.org/abs/2310.04727 . Shamekh, Sara, P. Gentine. Learning Atmospheric Boundary Layer Turbulence . 23 Jun. 2023. https://essopenarchive.org/doi/full/10.22541/essoar.168748456.60017486 . Shi, Haiyang. Global Pattern and Mechanism of Terrestrial Evapotranspiration Change Indicated by Weather Stations . arXiv:2309.06822, arXiv, 13 Sept. 2023, https://doi.org/10.48550/arXiv.2309.06822 . Will, Justus C., Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt. Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds. In Machine Learning and the Physical Sciences Workshop, Neural Information Processing Conference 2023. https://arxiv.org/abs/2310.20168 . Xi, Xuan. Pierre Gentine, Qianlai Zhuang and Seungbum Kim. Evaluating the Effects of Precipitation and Evapotranspiration on Soil Moisture Variability. https://essopenarchive.org/doi/full/10.1002/essoar.10511220.1 . Accessed 23 Apr. 2024. Yu, Sungduk, Walter Hannah, Liran Peng, Jerry Lin, Michael Pritchard, et al. ClimSim: A Large Multi-Scale Dataset for Hybrid Physics-ML Climate Emulation . arXiv:2306.08754, arXiv, 6 Feb. 2024, https://doi.org/10.48550/arXiv.2306.08754 . Yu, Sungduk et al. ‚ÄúClimSim-Online: A Large Multi-scale Dataset and Framework for Hybrid ML-physics Climate Emulation.‚Äù 8 Jul. 2024, https://doi.org/10.48550/arXiv.2306.08754 . Synergistic Publications PEER REVIEWED PUBLICATIONS Agonafir, Candace, et al. ‚ÄúA Review of Recent Advances in Urban Flood Research.‚Äù Water Security, vol. 19, Aug. 2023, p. 100141, https://doi.org/10.1016/j.wasec.2023.100141 . Biass, S√©bastien, et al. ‚ÄúHow Well Do Concentric Radii Approximate Population Exposure to Volcanic Hazards?‚Äù Bulletin of Volcanology, vol. 86, no. 1, Dec. 2023, p. 3, https://doi.org/10.1007/s00445-023-01686-5 . Burbano, Vanessa C., et al. ‚ÄúThe Gender Gap in Meaningful Work.‚Äù Management Science, Dec. 2023, https://doi.org/10.1287/mnsc.2022.01807 . Burbano, Vanessa, et al. ‚ÄúThe Past and Future of Corporate Sustainability Research.‚Äù Organization & Environment, Articles in Advance, Dec. 2023, pp.1-20, https://doi.org/10.1177/10860266231213105 . Camps-Valls, Gustau, Andreas Gerhardus, Urmi Ninad, Gherardo Varando, Georg Martius, Emili Balaguer-Ballester, Ricardo Vinuesa, Emiliano Diaz, Laure Zanna, Jakob Runge. ‚ÄúDiscovering Causal Relations and Equations from Data.‚Äù Physics Reports, vol. 1044, Dec. 2023, pp. 1‚Äì68, https://doi.org/10.1016/j.physrep.2023.10.005 . see more ... Chang, Chiung-Yin, et al. ‚ÄúRemote Versus Local Impacts of Energy Backscatter on the North Atlantic SST Biases in a Global Ocean Model.‚Äù Geophysical Research Letters, vol. 50, no. 21, 2023, p. e2023GL105757, https://doi.org/10.1029/2023GL105757 . Falasca, Fabrizio, et al. ‚ÄúData-Driven Dimensionality Reduction and Causal Inference for Spatiotemporal Climate Fields.‚Äù Physical Review E, vol. 109, no. 4, Apr. 2024, p. 044202, https://doi.org/10.1103/PhysRevE.109.044202 . Friedlingstein, Pierre, et al. ‚ÄúGlobal Carbon Budget 2023.‚Äù Earth System Science Data, vol. 15, no. 12, Dec. 2023, pp. 5301‚Äì69, https://doi.org/10.5194/essd-15-5301-2023 . Gregory, William, et al. ‚ÄúDeep Learning of Systematic Sea Ice Model Errors From DataAssimilation Increments.‚Äù Journal of Advances in Modeling Earth Systems, vol. 15, no. 10, 2023, p. e2023MS003757, https://doi.org/10.1029/2023MS003757 . Gregory, William, et al. ‚ÄúMachine Learning for Online Sea Ice Bias Correction Within Global Ice-Ocean Simulations.‚Äù Geophysical Research Letters, vol. 51, no. 3, 2024, p.e2023GL106776, https://doi.org/10.1029/2023GL106776 . Heimdal, Thea Hatlen, et al. ‚ÄúAssessing Improvements in Global Ocean PCO2 Machine Learning Reconstructions with Southern Ocean Autonomous Sampling.‚Äù Biogeosciences Discussions,¬† Oct. 2023, pp. 1‚Äì35, https://doi.org/10.5194/bg-2023-160 . Hu, Zeyuan, Akshay Subramaniam, Zhiming Kuang, Jerry Lin, Sungduk Yu, Walter M. Hannah, Noah D. Brenowitz, Josh Romero and Michael S. Pritchard. ‚ÄúStable Machine-Learning Parameterization of Subgrid Processes with Real Geography and Full-physics Emulation.‚Äù 5 Aug. 2024, https://doi.org/10.48550/arXiv.2407.00124 . Lamb, K. D., and P. Gentine. ‚ÄúZero-Shot Learning of Aerosol Optical Properties with Graph Neural Networks.‚Äù Scientific Reports, vol. 13, no. 1, Oct. 2023, p. 18777, https://doi.org/10.1038/s41598-023-45235-8 . Newsom, Emily, et al. ‚ÄúBackground Pycnocline Depth Constrains Future Ocean Heat Uptake Efficiency.‚Äù Geophysical Research Letters, vol. 50, no. 22, 2023, p. e2023GL105673, https://doi.org/10.1029/2023GL105673 . Olivarez, Holly C., et al. ‚ÄúHow Does the Pinatubo Eruption Influence Our Understanding of Long-Term Changes in Ocean Biogeochemistry?‚Äù Geophysical Research Letters, vol. 51, no. 2, 2024, p. e2023GL105431, https://doi.org/10.1029/2023GL105431 . Sane, Aakash, et al. ‚ÄúParameterizing Vertical Mixing Coefficients in the Ocean Surface Boundary Layer Using Neural Networks.‚Äù Journal of Advances in Modeling Earth Systems, vol. 15, no. 10, 2023, p. e2023MS003890, https://doi.org/10.1029/2023MS003890 . Zhang, Cheng, et al. ‚ÄúImplementation and Evaluation of a Machine Learned Mesoscale Eddy Parameterization Into a Numerical Ocean Circulation Model.‚Äù Journal of Advances in Modeling Earth Systems, vol. 15, no. 10, 2023, p. e2023MS003697, https://doi.org/10.1029/2023MS003697 . NON-PEER REVIEWED PUBLICATIONS Bodner, Abigail, et al. A Data-Driven Approach for Parameterizing Submesoscale VerticalBuoyancy Fluxes in the Ocean Mixed Layer. arXiv:2312.06972, arXiv, 11 Dec. 2023, https://doi.org/10.48550/arXiv.2312.06972 . Heimdal, T.H. and G.A. McKinley (2024) Using observing system simulation experiments to assess impacts of observational uncertainties in surface ocean pCO2 machine learning reconstructions, Scientific Rep., in review. Hermans, Tim H. J., et al. Projecting Changes in the Drivers of Compound Flooding in Europe Using CMIP6 Models. 27 Oct. 2023, https://doi.org/10.22541/essoar.169841704.46464014/v1 . Langlois, Gabriel P., et al. Efficient First-Order Algorithms for Large-Scale, Non-Smooth Maximum Entropy Models with Application to Wildfire Science. arXiv:2403.06816, arXiv, 11 Mar. 2024, https://doi.org/10.48550/arXiv.2403.06816 . Pedersen, Christian, et al. Reliable Coarse-Grained Turbulent Simulations through Combined Offline Learning and Neural Emulation. arXiv:2307.13144, arXiv, 24 July 2023, https://doi.org/10.48550/arXiv.2307.13144 . see more ... Perezhogin, Pavel, et al. Implementation of a Data-Driven Equation-Discovery Mesoscale Parameterization into an Ocean Model. arXiv:2311.02517, arXiv, 4 Nov. 2023, https://doi.org/10.48550/arXiv.2311.02517 . Books + Book Chapters Beucler, Tom, et al. ‚ÄúMachine Learning for Clouds and Climate.‚Äù Clouds and Their Climatic Impacts, American Geophysical Union (AGU), 2023, pp. 325‚Äì45, https://doi.org/10.1002/9781119700357.ch16 . Reyes, Nicole Alia Salis, et al. ‚Äú(Re)Wiring Settler Colonial Practices in Higher Education: Creating Indigenous Centered Futures Through Considerations of Power, the Social, Place, and Space.‚Äù Higher Education: Handbook of Theory and Research: Volume 39 , edited by Laura W. Perna, Springer Nature Switzerland, 2024, pp. 187‚Äì263, https://doi.org/10.1007/978-3-031-38077-8_5 . Conferences Acquaviva, Viviana, Barnes, Elizabeth, Gagne, John David II, McKinley, Galen, Thais, Savannah, ‚ÄúEthics and Explainability in Climate AI: from theory to practice), panel discussion, Climate Informatics 2024. https://alan-turing-institute.github.io/climate-informatics-2024/schedule/ . Acquaviva, Viviana, ‚ÄúFrom ML x Astrophysics to ML x Climate: A journey across disciplines‚Äù, PIVOT fellowship symposium, April 2024. Dagon, Katherine, et al. Perturbed Parameter Ensembles (PPEs) for Understanding Processes and Quantifying Uncertainty in Earth System Models I Oral. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Session/202455 . Elsaesser, Gregory, et al. ‚ÄúUsing Machine Learning to Generate a NASA GISS ESM Calibrated Physics Ensemble‚Äù, AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1437342 . Elsaesser, Gregory, and. M. McGraw. AI Advances in Tropical Meteorology: Tropical Cyclones, Sub-Seasonal Phenomena, and More.¬† AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Session/65400 . Gentine, Pierre. Differentiable land-surface model. Cornell University 6th Training Course on New Advances in Land Carbon Cycle Modeling. Cornell University, May 2023 (invited) https://ecolab.cals.cornell.edu/?training_course_2023 . Gentine, Pierre. How can AI help with climate adaptation and resilience? United Nations Geneva, July 2023. (invited) see more... Gentine, Pierre; LEAPing across scales for climate modeling. Multiscale Physics Symposium. Simons Foundation, August 2023. (invited) https://www.simonsfoundation.org/event/multi-scale-physics-2023/ . Gentine, Pierre, and Bhouri, M. Aziz. Multifidelity climate model parameterizations. AGU, 2023. (invited) https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1290385 . Gentine, Pierre, and Nathaniel Juan. Intrinsic dimension of spatio-temporal chaotic systems. AGU, 2023. (invited) https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1378633 . Gentine, Pierre. Next generation climate models with new tools. ETH Zurich, December 2023. Gentine, Pierre. AI for weather and climate forecasting. EESI, Congress AI bill on AI for weather. February 2024. (invited) https://www.eesi.org/briefings/view/021524weather . Gentine, Pierre. AI for climate: from emulation to new discoveries. Simons Foundation Presidential Lecture. March 2024. (invited) https://www.simonsfoundation.org/event/ai-in-climate-science-from-emulation-to-new-discoveries/#:~:text=Simulating%20Earth‚Äôs%20climate%20is%20a,tool%20in%20 overcoming%20 these%20 roadblocks. Gentine, Pierre. Machine learning in climate science, from emulation to discoveries. Berkeley Atmospheric Sciences Center, UC Berkeley. March 2024 (invited) https://atmosphere.berkeley.edu/symposium2024.php . Gentine, Pierre. Next generation climate models. Stanford University, March 2024. (invited) Gentine, Pierre. Interdependencies between migration and climate: a need for a transdisciplinary Earth System approach. Workshop on Climate Change and Human Migration: An Earth Systems Science Perspective. National Academies, March 2024. (invited) https://www.nationalacademies.org/event/41814_03-2024_workshop-on-climate-change-and-human-migration-an-earth-systems-science-perspective . Gentine, Pierre. Keynote: Harnessing AI to Confront Environmental Challenges. Bezos Earth Fund AI For Climate and Nature Spring Convening. Columbia University, April 2024. https://www.climate.columbia.edu/events/ai-climate-and-nature-spring-convening . Gentine, Pierre. Machine learning for climate science: from emulation to new discoveries. Seoul National University, April 2024 (invited) https://calslab.snu.ac.kr/agronomy/board.read?mcode=1910&id=188 Gentine, Pierre, V. Eyring, M. Reichstein,G. Camps-Valls, Gudstau. Pushing Frontier Research in Climate Modelling and Understanding with AI for Urgent Mitigation and Adaptation Needs. United Nations, May 2024. Gentine, Pierre. Climate model refactorization to JAX. Microsoft, May 2024 Jung, Jaeyoung, et al. Canopy Flow Modeling Using Multiscale Homogenization Techniques. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1444637 . Ko, Joseph, et al. Classification of Cloud Particle Imagery Using Variational Autoencoders and Unsupervised Clustering. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/432580 . Ko, Joseph, et al. Informing Depositional Ice Growth Models Through 3-D Reconstruction of Ice Crystal Images Using Machine Learning. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/432551 . Kumar, Vipin. Role of Big Data and Machine Learning for Addressing Global Environmental ¬† ¬† ¬† ¬† ¬† Challenges. AAAI 2023 Fall Symposium Series. https://www.climatechange.ai/events/aaaifss2023#schedule Lamb, Kara D. J. Mikhaeil, J. Harrington, M. van Lier Walqui. Cloud chamber constraints on depositional ice growth models. AGU Fall Meeting 2023. San Francisco, CA, December 2023. Lamb, Kara D. and P. Gentine. Exploring Phase Transitions and Dynamical Processes in Tropical Moist Convection Using Machine Learning. AGU Fall Meeting, San Francisco, CA, December 2023 [invited] Lamb, Kara D., et al. Learning Constraints on Depositional Ice Growth Models from Cloud ¬† Chamber Experiments with Physics Informed Neural Networks. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/437022 . Lamb, Kara D., M. van Lier Walqui, S. Santos, H. Morrison. Reduced Order Modeling to Reduce Structural Uncertainty in Representing Cloud Microphysical Process Rates. AGU Fall Meeting 2023. San Francisco, CA, December 2023. Lamb, Kara D. and P. Gentine. Zero shot learning of aerosol optical properties with graph neural networks. EGU General Assembly, April 2024. [invited] Loftus, Kaitlyn, et al. Parameterizing Cloud Microphysics with Machine Learning-Enabled Bayesian Parameter Inference. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/435624 . Loftus, Kaitlyn, et al. Parameterizing Cloud Microphysics with Machine Learning-Enabled Bayesian Parameter Inference. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1429616 . McKinley, et al. Constraining historical ocean carbon uptake with models, machine learning and data. World Climate Research Program Open Science Conference. Rwanda, Oct. 2023. Mikhaeil, Jonas Magdy, et al. Bayesian Workflow for the Evaluation of Constraints on Depositional Ice Growth Models with Cloud Chamber Observations. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/436623 . Pizmony-Levy, Oren, Rivet, Ann, Torres, Chris, and Urbach, Noa. New York City Educators‚Äô Perceptions of Students‚Äô Engagement with Climate Change. Annual Meeting of the Comparative and International Education Society (CIES). Miami, Florida (2024). Presenter: Urbach, Noa Pizmony-Levy, Oren, Rivet, Ann, Torres, Chris, and Urbach, Noa. Tik Tok, Cheese Sticks, and Our Future: How Educators Perceive Students‚Äô Engagement with Climate Change. Annual Meeting of the American Educational Research Association (AERA). Philadelphia, Pennsylvania (2024). Presenters: Torres, Chris, and Urbach, Noa Will, Justus C., Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt. Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds. In Machine Learning and the Physical Sciences Workshop, Neural Information Processing Conference 2023. https://arxiv.org/abs/2310.20168 . Yang, Qingyuan, et al. Flexible Use of Additive Gaussian Processes as a Powerful Tool for More Interpretable Analysis and Emulation of Climate Model PPEs. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1341071 . Yang, Qingyuan, and Susanna Jenkins. Two Sources of Uncertainty in Estimating Tephra Volumes from Isopachs: Perspectives and Quantification. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1343724 . Zanna, Laure. The New Generation of Global Climate Models Enhanced by Machine Learning (Invited). AGU 2023. https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1366219 . Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research-home/publications-papers-conferences/#content",
    "transcript": "Publications, Papers, Conferences - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Publications, Papers + Conferences Acknowledgements Acknowledging LEAP Support in Publications All papers supported in full or in part by LEAP should acknowledge LEAP funding in the Acknowledgements section as follows: ‚ÄúWe acknowledge funding from NSF through the Learning the Earth with Artificial intelligence and Physics (LEAP) Science and Technology Center (STC) (Award #2019625).‚Äù Center Publications PEER REVIEWED PUBLICATIONS Behrens, Gunnar, Tom Beucler, Pierre Gentine, et al. ‚ÄúNon-Linear Dimensionality Reduction With a Variational Encoder Decoder to Understand Convective Processes in Climate Models.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 8, 2022, p. e2022MS003130, https://doi.org/10.1029/2022MS003130 . Bennington, Val, Tomislav Galjanic, et al. ‚ÄúExplicit Physical Knowledge in Machine Learning for Ocean Carbon Flux Reconstruction: The PCO2-Residual Method.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 10, 2022, p. e2021MS002960, https://doi.org/10.1029/2021MS002960 . Bennington, Val, Lucas Gloege, et al. ‚ÄúVariability in the Global Ocean Carbon Sink From 1959 to 2020 by Correcting Models With Observations.‚Äù Geophysical Research Letters , vol. 49, no. 14, 2022, p. e2022GL098632, https://doi.org/10.1029/2022GL098632 . Beucler, Tom, et al. ‚ÄúClimate-Invariant Machine Learning.‚Äù Science Advances , vol. 10, no. 6, Feb. 2024, p. eadj7250, https://doi.org/10.1126/sciadv.adj7250 . Bhouri, Mohamed Aziz, and Pierre Gentine. ‚ÄúMemory-Based Parameterization with Differentiable Solver: Application to Lorenz ‚Äô96.‚Äù Chaos: An Interdisciplinary Journal of Nonlinear Science, vol. 33, no. 7, July 2023, p. 073116, https://doi.org/10.1063/5.0131929 . see more... Bu, Jingyi, et al. ‚ÄúDryland Evapotranspiration from Remote Sensing Solar-Induced Chlorophyll Fluorescence: Constraining an Optimal Stomatal Model within a Two-Source Energy Balance Model.‚Äù Remote Sensing of Environment, vol. 303, Mar. 2024, p. 113999, https://doi.org/10.1016/j.rse.2024.113999 . Buch, J., Williams, A., Juang, C., Hansen, W., & Gentine, P. (2023). SMLFire1.0: a stochastic machine learning (SML) model for wildfire activity in the western United States. Geoscientific Model Development , 16(12), 3407-3433. http://dx.doi.org/10.5194/gmd-16-3407-2023 . Chen, Tse-Chun, et al. ‚ÄúCorrecting Systematic and State-Dependent Errors in the NOAA FV3-GFS Using Neural Networks.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 11, 2022, p. e2022MS003309, https://doi.org/10.1029/2022MS003309 . Cheng, Y., Giometto, M. G., Kauffmann, P., Lin, L., Cao, C., Zupnick, C., Abernathey, R. & Gentine, P. Deep learning for subgrid-scale turbulence modeling in large-eddy simulations of the convective atmospheric boundary layer. Journal of Advances in Modeling Earth Systems, 14, e2021MS002847, May 2022, https://doi.org/10.1029/2021MS002847 . Eidhammer, Trude & Gettelman, Andrew & Thayer-Calder, Katherine & Watson-Parris, Duncan & Elsaesser, Gregory & Morrison, Hugh & van Lier-Walqui, Marcus & Song, Ci & Mccoy, Daniel. (2024). An extensible perturbed parameter ensemble for the Community Atmosphere Model version 6. Geoscientific Model Development. 17. 7835-7853. https://doi.org/10.5194/gmd-17-7835-2024 . Eyring, V., Gentine, P., Camps-Valls, G. et al. AI-empowered next-generation multiscale climate modelling for mitigation and adaptation. Nat. Geosci. 17, 963‚Äì971 (2024). https://doi.org/10.1038/s41561-024-01527-w . Eyring, V., Collins, W.D., Gentine, P. et al. Pushing the frontiers in climate modelling and analysis with machine learning. Nat. Clim. Chang. 14, 916‚Äì928 (2024). https://doi.org/10.1038/s41558-024-02095-y . Heimdal, T.H., McKinley, G.A. The importance of adding unbiased Argo observations to the ocean carbon observing system. Sci Rep 14 , 19763 (2024). https://doi.org/10.1038/s41598-024-70617-x . Islam, Ariful, et al. Enhancing Satellite Data Coverage: Leveraging Multiple Sensors to Bridge Information Gaps in Urban Flood Mapping. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1353707 . Giardina, Francesco, et al. Groundwater Rivals Aridity in Determining Global Photosynthesis . 13 Mar. 2024, https://doi.org/10.21203/rs.3.rs-3793488/v1 . Grundner, Arthur, Tom Beucler, Pierre Gentine, and Veronika Eyring. ‚ÄúData-Driven Equation Discovery of a Cloud Cover Parameterization.‚Äù Journal of Advances in Modeling Earth Systems , vol. 16, no. 3, 2024, p. e2023MS003763, https://doi.org/10.1029/2023MS003763 . Grundner, Arthur, Tom Beucler, Pierre Gentine, Fernando Iglesias-Suarez, et al. ‚ÄúDeep Learning Based Cloud Cover Parameterization for ICON.‚Äù Journal of Advances in Modeling Earth Systems , vol. 14, no. 12, 2022, p. e2021MS002959, https://doi.org/10.1029/2021MS002959 . Iglesias-Suarez, Fernando, et al. ‚ÄúCausally-Informed Deep Learning to Improve Climate Models and Projections.‚Äù Journal of Geophysical Research: Atmospheres , vol. 129, no. 4, 2024, p. e2023JD039202, https://doi.org/10.1029/2023JD039202 . Improving the Modeling and Analysis of Tropical Convection and Precipitation Through Machine Learning Methods ‚Äì ProQuest . https://www.proquest.com/openview/b21c84cc107ccd37cba0f0a7e7c0c30f/1?pq-origsite=gscholar&cbl=18750&diss=y Accessed 15 Apr. 2024. Li, Xing, et al. ‚ÄúNew-Generation Geostationary Satellite Reveals Widespread Midday Depression in Dryland Photosynthesis during 2020 Western U.S. Heatwave.‚Äù Science Advances, vol. 9, no. 31, Aug. 2023, p. eadi0775, https://doi.org/10.1126/sciadv.adi0775 . Lockwood, J. W., Gori, A., & Gentine, P. (2024). A generative super-resolution model for enhancing tropical cyclone wind field intensity and resolution. Journal of Geophysical Research: Machine Learning and Computation , 1, e2024JH000375. https://doi.org/10.1029/2024JH000375 . McKinley, Galen A., et al. ‚ÄúModern Air-Sea Flux Distributions Reduce Uncertainty in the Future Ocean Carbon Sink.‚Äù Environmental Research Letters , vol. 18, no. 4, Mar. 2023, p. 044011, https://doi.org/10.1088/1748-9326/acc195 . Mooers, Griffin, Mike Pritchard, et al. ‚ÄúComparing Storm Resolving Models and Climates via Unsupervised Machine Learning.‚Äù Scientific Reports , vol. 13, no. 1, Dec. 2023, p. 22365, https://doi.org/10.1038/s41598-023-49455-w . Mooers, Griffin S., et al. ‚ÄúAn Unsupervised Learning Perspective on the Dynamic Contribution to Extreme Precipitation Changes.‚Äù Climate Change AI, Climate Change AI, 2022, https://www.climatechange.ai/papers/neurips2022/81 . Nathaniel, Juan, Jiangong Liu, et al. ‚ÄúMetaFlux: Meta-Learning Global Carbon Fluxes from Sparse Spatiotemporal Observations.‚Äù Scientific Data , vol. 10, no. 1, July 2023, p. 440, https://doi.org/10.1038/s41597-023-02349-y . Pla, Paula, et al. ‚ÄúHydrogenation of C24 Carbon Clusters: Structural Diversity and Energetic Properties.‚Äù The Journal of Physical Chemistry A , vol. 125, no. 24, June 2021, pp. 5273‚Äì88, https://doi.org/10.1021/acs.jpca.1c02359 . Portocarrero, Florencio F., and Vanessa C. Burbano. ‚ÄúThe Effects of a Short-Term Corporate Social Impact Activity on Employee Turnover: Field Experimental Evidence.‚Äù Management Science , Oct. 2023, https://doi.org/10.1287/mnsc.2022.01517 . Schmidt, Gavin. A., et al. ‚ÄúAnomalous Meltwater From Ice Sheets and Ice Shelves Is a Historical Forcing.‚Äù Geophysical Research Letters , vol. 50, no. 24, 2023, p. e2023GL106530, https://doi.org/10.1029/2023GL106530 . Shamekh, Sara, et al. ‚ÄúImplicit Learning of Convective Organization Explains Precipitation Stochasticity.‚Äù Proceedings of the National Academy of Sciences , vol. 120, no. 20, May 2023, p. e2216158120, https://doi.org/10.1073/pnas.2216158120 . Shen, Chaopeng, et al. ‚ÄúDifferentiable Modeling to Unify Machine Learning and Physical Models and Advance Geosciences.‚Äù Nature Reviews Earth & Environment , vol. 4, no. 8, July 2023, pp. 552‚Äì67, https://doi.org/10.1038/s43017-023-00450-9 . Skulovich, Olya, and Pierre Gentine. ‚ÄúA Long-Term Consistent Artificial Intelligence and Remote Sensing-Based Soil Moisture Dataset.‚Äù Scientific Data , vol. 10, no. 1, Mar. 2023, p. 154, https://doi.org/10.1038/s41597-023-02053-x . Wong, S. C., McKinley, G. A., & Seager, R. Equatorial Pacific pCO2 interannual variability in CMIP6 models. Journal of Geophysical Research: Biogeosciences , e2022JG007243, Dec. 2022, https://doi.org/10.1029/2022JG007243 . Yang, Qingyuan, and Susanna F. Jenkins. ‚ÄúTwo Sources of Uncertainty in Estimating Tephra Volumes from Isopachs: Perspectives and Quantification.‚Äù Bulletin of Volcanology , vol. 85, no. 8, July 2023, p. 44, https://doi.org/10.1007/s00445-023-01652-1 . Yu, Sungduk, Walter Hannah, Liran Peng, Jerry Lin, Mike Pritchard, et al. ‚ÄúClimSim: A Large Multi-Scale Dataset for Hybrid Physics-ML Climate Emulation.‚Äù Advances in Neural Information Processing Systems , vol. 36, Dec. 2023, pp. 22070‚Äì84, https://proceedings.neurips.cc/paper_files/paper/2023/hash/45fbcc01349292f5e059a0b8b02c8c3f-Abstract-Datasets_and_Benchmarks.html Zhan, W., Yang, X., Ryu, Y., Dechant, B., Huang, Y., Goulas, Y., ‚Ä¶ & Gentine, P. Two for one: Partitioning CO2 fluxes and understanding the relationship between solar-induced chlorophyll fluorescence and gross primary productivity using machine learning. Agricultural and Forest Meteorology, 321, 108980, Jun. 2022, https://doi.org/10.1016/j.agrformet.2022.108980 . Zhao, Wenli, Biqing Zhu, Steven J. Davis, Philippe Ciais, Chaopeng Hong, Zhu Liu and Pierre Gentine. ‚ÄúReliance on Fossil Fuels Increases during Extreme Temperature Events in the Continental United States.‚Äù Communications Earth & Environment, vol. 4, no. 1, Dec. 2023, pp. 1‚Äì11, https://doi.org/10.1038/s43247-023-01147-z . NON-PEER REVIEWED PUBLICATIONS Agonafir, Candace, and Tian Zheng. ‚ÄúStructured Exploration of Machine Learning Model¬† Complexity for Spatio-Temporal Forecasting of Urban Flooding.‚Äù EGUsphere, Mar. 2024, pp. 1‚Äì32, https://doi.org/10.5194/egusphere-2024-551 . Behrens, Gunnar, Tom Beucler, Fernando Iglesias-Suarez, et al. Improving Atmospheric Processes in Earth System Models with Deep Learning Ensembles and Stochastic Parameterizations . arXiv:2402.03079, arXiv, 5 Feb. 2024, https://doi.org/10.48550/arXiv.2402.03079 . Bhouri, Mohamed Aziz, et al. Multi-Fidelity Climate Model Parameterization for Better Generalization and Extrapolation. arXiv:2309.10231, arXiv, 18 Sept. 2023, https://doi.org/10.48550/arXiv.2309.10231 . Bhouri, Mohamed Aziz, and Pierre Gentine. History-Based, Bayesian, Closure for Stochastic Parameterization: Application to Lorenz ‚Äô96 . arXiv:2210.14488, arXiv, 26 Oct. 2022, https://doi.org/10.48550/arXiv.2210.14488 . Hoffman, Andrew O., et al. ‚ÄúInland Migration of Near-Surface Crevasses in the Amundsen Sea Sector, West Antarctica.‚Äù EGUsphere , Jan. 2024, pp. 1‚Äì29, https://doi.org/10.5194/egusphere-2023-2956 . see more ... Karpatne, Anuj, Xiaowei Jia, and Vipin Kumar (2024). Knowledge-guided Machine Learning: Current Trends and Future Prospects. arXiv:2403.15989, arXiv, 24 Mar. 2024, https://doi.org/10.48550/arXiv.2403.15989 . Kodner, Jordan, et al. Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023) . arXiv:2308.03228, arXiv, 6 Aug. 2023, https://doi.org/10.48550/arXiv.2308.03228 . Lamb, Kara, Marcus van Lier Walqui, Sean Santos, Hugh Morrison. Reduced Order Modeling for Linear Representations of Microphysical Process Rates. https://essopenarchive.org/users/553026/articles/653967-reduced-order-modeling-for-linearized-representations-of-microphysical-process-rates . Langlois, Gabriel P., et al. Efficient First-Order Algorithms for Large-Scale, Non-Smooth Maximum Entropy Models with Application to Wildfire Science . arXiv:2403.06816, arXiv, 11 Mar. 2024, https://doi.org/10.48550/arXiv.2403.06816 . Liao, Kyleen, et al. Simulating the Air Quality Impact of Prescribed Fires Using a Graph Neural Network-Based PM$_{2.5}$ Emissions Forecasting System . arXiv:2312.04291, arXiv, 7 Dec. 2023, https://doi.org/10.48550/arXiv.2312.04291 . Lin, Jerry, et al. Stress-Testing the Coupled Behavior of Hybrid Physics-Machine Learning Climate Simulations on an Unseen, Warmer Climate. arXiv:2401.02098, arXiv, 4 Jan. 2024, https://doi.org/10.48550/arXiv.2401.02098 . Lin, Jerry, et al. Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models . arXiv:2309.16177, arXiv, 28 Sept. 2023, https://doi.org/10.48550/arXiv.2309.16177 . Mooers, Griffin, Tom Beucler, et al. Understanding Extreme Precipitation Changes through Unsupervised Machine Learning . arXiv:2211.01613, arXiv, 1 Dec. 2023, https://doi.org/10.48550/arXiv.2211.01613 . PythonicDISORT: A Python Reimplementation of the Discrete Ordinate Radiative TransferPackage DISORT. Journal of Open Source Software, https://joss.theoj.org/papers/ec19f2796dac7303ea3bd476455b641f . Accessed 22 Apr. 2024. Nathaniel, Juan, Yongquan Qu, et al. ChaosBench: A Multi-Channel, Physics-Based Benchmarkfor Subseasonal-to-Seasonal Climate Prediction . arXiv:2402.00712, arXiv, 1 Mar. 2024, https://doi.org/10.48550/arXiv.2402.00712 . Qu, Yongquan, Juan Nathaniel, Shuolin Li, Pierre Gentine. Deep Generative Data Assimilation in Multimodal Setting. arXiv:2404.06665, arXiv, 9 Apr. 2024, https://doi.org/10.48550/arXiv.2404.06665 . Qu, Yongquan, Mohamed Aziz Bhouri, Pierre Gentine. Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming . 2024, https://doi.org/10.48550/ARXIV.2403.02215 . Renganathan, Arvind, et al. Task Aware Modulation Using Representation Learning: An Approach for Few Shot Learning in Heterogeneous Systems. arXiv:2310.04727, arXiv, 7 Oct. 2023, http://arxiv.org/abs/2310.04727 . Shamekh, Sara, P. Gentine. Learning Atmospheric Boundary Layer Turbulence . 23 Jun. 2023. https://essopenarchive.org/doi/full/10.22541/essoar.168748456.60017486 . Shi, Haiyang. Global Pattern and Mechanism of Terrestrial Evapotranspiration Change Indicated by Weather Stations . arXiv:2309.06822, arXiv, 13 Sept. 2023, https://doi.org/10.48550/arXiv.2309.06822 . Will, Justus C., Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt. Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds. In Machine Learning and the Physical Sciences Workshop, Neural Information Processing Conference 2023. https://arxiv.org/abs/2310.20168 . Xi, Xuan. Pierre Gentine, Qianlai Zhuang and Seungbum Kim. Evaluating the Effects of Precipitation and Evapotranspiration on Soil Moisture Variability. https://essopenarchive.org/doi/full/10.1002/essoar.10511220.1 . Accessed 23 Apr. 2024. Yu, Sungduk, Walter Hannah, Liran Peng, Jerry Lin, Michael Pritchard, et al. ClimSim: A Large Multi-Scale Dataset for Hybrid Physics-ML Climate Emulation . arXiv:2306.08754, arXiv, 6 Feb. 2024, https://doi.org/10.48550/arXiv.2306.08754 . Yu, Sungduk et al. ‚ÄúClimSim-Online: A Large Multi-scale Dataset and Framework for Hybrid ML-physics Climate Emulation.‚Äù 8 Jul. 2024, https://doi.org/10.48550/arXiv.2306.08754 . Synergistic Publications PEER REVIEWED PUBLICATIONS Agonafir, Candace, et al. ‚ÄúA Review of Recent Advances in Urban Flood Research.‚Äù Water Security, vol. 19, Aug. 2023, p. 100141, https://doi.org/10.1016/j.wasec.2023.100141 . Biass, S√©bastien, et al. ‚ÄúHow Well Do Concentric Radii Approximate Population Exposure to Volcanic Hazards?‚Äù Bulletin of Volcanology, vol. 86, no. 1, Dec. 2023, p. 3, https://doi.org/10.1007/s00445-023-01686-5 . Burbano, Vanessa C., et al. ‚ÄúThe Gender Gap in Meaningful Work.‚Äù Management Science, Dec. 2023, https://doi.org/10.1287/mnsc.2022.01807 . Burbano, Vanessa, et al. ‚ÄúThe Past and Future of Corporate Sustainability Research.‚Äù Organization & Environment, Articles in Advance, Dec. 2023, pp.1-20, https://doi.org/10.1177/10860266231213105 . Camps-Valls, Gustau, Andreas Gerhardus, Urmi Ninad, Gherardo Varando, Georg Martius, Emili Balaguer-Ballester, Ricardo Vinuesa, Emiliano Diaz, Laure Zanna, Jakob Runge. ‚ÄúDiscovering Causal Relations and Equations from Data.‚Äù Physics Reports, vol. 1044, Dec. 2023, pp. 1‚Äì68, https://doi.org/10.1016/j.physrep.2023.10.005 . see more ... Chang, Chiung-Yin, et al. ‚ÄúRemote Versus Local Impacts of Energy Backscatter on the North Atlantic SST Biases in a Global Ocean Model.‚Äù Geophysical Research Letters, vol. 50, no. 21, 2023, p. e2023GL105757, https://doi.org/10.1029/2023GL105757 . Falasca, Fabrizio, et al. ‚ÄúData-Driven Dimensionality Reduction and Causal Inference for Spatiotemporal Climate Fields.‚Äù Physical Review E, vol. 109, no. 4, Apr. 2024, p. 044202, https://doi.org/10.1103/PhysRevE.109.044202 . Friedlingstein, Pierre, et al. ‚ÄúGlobal Carbon Budget 2023.‚Äù Earth System Science Data, vol. 15, no. 12, Dec. 2023, pp. 5301‚Äì69, https://doi.org/10.5194/essd-15-5301-2023 . Gregory, William, et al. ‚ÄúDeep Learning of Systematic Sea Ice Model Errors From DataAssimilation Increments.‚Äù Journal of Advances in Modeling Earth Systems, vol. 15, no. 10, 2023, p. e2023MS003757, https://doi.org/10.1029/2023MS003757 . Gregory, William, et al. ‚ÄúMachine Learning for Online Sea Ice Bias Correction Within Global Ice-Ocean Simulations.‚Äù Geophysical Research Letters, vol. 51, no. 3, 2024, p.e2023GL106776, https://doi.org/10.1029/2023GL106776 . Heimdal, Thea Hatlen, et al. ‚ÄúAssessing Improvements in Global Ocean PCO2 Machine Learning Reconstructions with Southern Ocean Autonomous Sampling.‚Äù Biogeosciences Discussions,¬† Oct. 2023, pp. 1‚Äì35, https://doi.org/10.5194/bg-2023-160 . Hu, Zeyuan, Akshay Subramaniam, Zhiming Kuang, Jerry Lin, Sungduk Yu, Walter M. Hannah, Noah D. Brenowitz, Josh Romero and Michael S. Pritchard. ‚ÄúStable Machine-Learning Parameterization of Subgrid Processes with Real Geography and Full-physics Emulation.‚Äù 5 Aug. 2024, https://doi.org/10.48550/arXiv.2407.00124 . Lamb, K. D., and P. Gentine. ‚ÄúZero-Shot Learning of Aerosol Optical Properties with Graph Neural Networks.‚Äù Scientific Reports, vol. 13, no. 1, Oct. 2023, p. 18777, https://doi.org/10.1038/s41598-023-45235-8 . Newsom, Emily, et al. ‚ÄúBackground Pycnocline Depth Constrains Future Ocean Heat Uptake Efficiency.‚Äù Geophysical Research Letters, vol. 50, no. 22, 2023, p. e2023GL105673, https://doi.org/10.1029/2023GL105673 . Olivarez, Holly C., et al. ‚ÄúHow Does the Pinatubo Eruption Influence Our Understanding of Long-Term Changes in Ocean Biogeochemistry?‚Äù Geophysical Research Letters, vol. 51, no. 2, 2024, p. e2023GL105431, https://doi.org/10.1029/2023GL105431 . Sane, Aakash, et al. ‚ÄúParameterizing Vertical Mixing Coefficients in the Ocean Surface Boundary Layer Using Neural Networks.‚Äù Journal of Advances in Modeling Earth Systems, vol. 15, no. 10, 2023, p. e2023MS003890, https://doi.org/10.1029/2023MS003890 . Zhang, Cheng, et al. ‚ÄúImplementation and Evaluation of a Machine Learned Mesoscale Eddy Parameterization Into a Numerical Ocean Circulation Model.‚Äù Journal of Advances in Modeling Earth Systems, vol. 15, no. 10, 2023, p. e2023MS003697, https://doi.org/10.1029/2023MS003697 . NON-PEER REVIEWED PUBLICATIONS Bodner, Abigail, et al. A Data-Driven Approach for Parameterizing Submesoscale VerticalBuoyancy Fluxes in the Ocean Mixed Layer. arXiv:2312.06972, arXiv, 11 Dec. 2023, https://doi.org/10.48550/arXiv.2312.06972 . Heimdal, T.H. and G.A. McKinley (2024) Using observing system simulation experiments to assess impacts of observational uncertainties in surface ocean pCO2 machine learning reconstructions, Scientific Rep., in review. Hermans, Tim H. J., et al. Projecting Changes in the Drivers of Compound Flooding in Europe Using CMIP6 Models. 27 Oct. 2023, https://doi.org/10.22541/essoar.169841704.46464014/v1 . Langlois, Gabriel P., et al. Efficient First-Order Algorithms for Large-Scale, Non-Smooth Maximum Entropy Models with Application to Wildfire Science. arXiv:2403.06816, arXiv, 11 Mar. 2024, https://doi.org/10.48550/arXiv.2403.06816 . Pedersen, Christian, et al. Reliable Coarse-Grained Turbulent Simulations through Combined Offline Learning and Neural Emulation. arXiv:2307.13144, arXiv, 24 July 2023, https://doi.org/10.48550/arXiv.2307.13144 . see more ... Perezhogin, Pavel, et al. Implementation of a Data-Driven Equation-Discovery Mesoscale Parameterization into an Ocean Model. arXiv:2311.02517, arXiv, 4 Nov. 2023, https://doi.org/10.48550/arXiv.2311.02517 . Books + Book Chapters Beucler, Tom, et al. ‚ÄúMachine Learning for Clouds and Climate.‚Äù Clouds and Their Climatic Impacts, American Geophysical Union (AGU), 2023, pp. 325‚Äì45, https://doi.org/10.1002/9781119700357.ch16 . Reyes, Nicole Alia Salis, et al. ‚Äú(Re)Wiring Settler Colonial Practices in Higher Education: Creating Indigenous Centered Futures Through Considerations of Power, the Social, Place, and Space.‚Äù Higher Education: Handbook of Theory and Research: Volume 39 , edited by Laura W. Perna, Springer Nature Switzerland, 2024, pp. 187‚Äì263, https://doi.org/10.1007/978-3-031-38077-8_5 . Conferences Acquaviva, Viviana, Barnes, Elizabeth, Gagne, John David II, McKinley, Galen, Thais, Savannah, ‚ÄúEthics and Explainability in Climate AI: from theory to practice), panel discussion, Climate Informatics 2024. https://alan-turing-institute.github.io/climate-informatics-2024/schedule/ . Acquaviva, Viviana, ‚ÄúFrom ML x Astrophysics to ML x Climate: A journey across disciplines‚Äù, PIVOT fellowship symposium, April 2024. Dagon, Katherine, et al. Perturbed Parameter Ensembles (PPEs) for Understanding Processes and Quantifying Uncertainty in Earth System Models I Oral. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Session/202455 . Elsaesser, Gregory, et al. ‚ÄúUsing Machine Learning to Generate a NASA GISS ESM Calibrated Physics Ensemble‚Äù, AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1437342 . Elsaesser, Gregory, and. M. McGraw. AI Advances in Tropical Meteorology: Tropical Cyclones, Sub-Seasonal Phenomena, and More.¬† AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Session/65400 . Gentine, Pierre. Differentiable land-surface model. Cornell University 6th Training Course on New Advances in Land Carbon Cycle Modeling. Cornell University, May 2023 (invited) https://ecolab.cals.cornell.edu/?training_course_2023 . Gentine, Pierre. How can AI help with climate adaptation and resilience? United Nations Geneva, July 2023. (invited) see more... Gentine, Pierre; LEAPing across scales for climate modeling. Multiscale Physics Symposium. Simons Foundation, August 2023. (invited) https://www.simonsfoundation.org/event/multi-scale-physics-2023/ . Gentine, Pierre, and Bhouri, M. Aziz. Multifidelity climate model parameterizations. AGU, 2023. (invited) https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1290385 . Gentine, Pierre, and Nathaniel Juan. Intrinsic dimension of spatio-temporal chaotic systems. AGU, 2023. (invited) https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1378633 . Gentine, Pierre. Next generation climate models with new tools. ETH Zurich, December 2023. Gentine, Pierre. AI for weather and climate forecasting. EESI, Congress AI bill on AI for weather. February 2024. (invited) https://www.eesi.org/briefings/view/021524weather . Gentine, Pierre. AI for climate: from emulation to new discoveries. Simons Foundation Presidential Lecture. March 2024. (invited) https://www.simonsfoundation.org/event/ai-in-climate-science-from-emulation-to-new-discoveries/#:~:text=Simulating%20Earth‚Äôs%20climate%20is%20a,tool%20in%20 overcoming%20 these%20 roadblocks. Gentine, Pierre. Machine learning in climate science, from emulation to discoveries. Berkeley Atmospheric Sciences Center, UC Berkeley. March 2024 (invited) https://atmosphere.berkeley.edu/symposium2024.php . Gentine, Pierre. Next generation climate models. Stanford University, March 2024. (invited) Gentine, Pierre. Interdependencies between migration and climate: a need for a transdisciplinary Earth System approach. Workshop on Climate Change and Human Migration: An Earth Systems Science Perspective. National Academies, March 2024. (invited) https://www.nationalacademies.org/event/41814_03-2024_workshop-on-climate-change-and-human-migration-an-earth-systems-science-perspective . Gentine, Pierre. Keynote: Harnessing AI to Confront Environmental Challenges. Bezos Earth Fund AI For Climate and Nature Spring Convening. Columbia University, April 2024. https://www.climate.columbia.edu/events/ai-climate-and-nature-spring-convening . Gentine, Pierre. Machine learning for climate science: from emulation to new discoveries. Seoul National University, April 2024 (invited) https://calslab.snu.ac.kr/agronomy/board.read?mcode=1910&id=188 Gentine, Pierre, V. Eyring, M. Reichstein,G. Camps-Valls, Gudstau. Pushing Frontier Research in Climate Modelling and Understanding with AI for Urgent Mitigation and Adaptation Needs. United Nations, May 2024. Gentine, Pierre. Climate model refactorization to JAX. Microsoft, May 2024 Jung, Jaeyoung, et al. Canopy Flow Modeling Using Multiscale Homogenization Techniques. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1444637 . Ko, Joseph, et al. Classification of Cloud Particle Imagery Using Variational Autoencoders and Unsupervised Clustering. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/432580 . Ko, Joseph, et al. Informing Depositional Ice Growth Models Through 3-D Reconstruction of Ice Crystal Images Using Machine Learning. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/432551 . Kumar, Vipin. Role of Big Data and Machine Learning for Addressing Global Environmental ¬† ¬† ¬† ¬† ¬† Challenges. AAAI 2023 Fall Symposium Series. https://www.climatechange.ai/events/aaaifss2023#schedule Lamb, Kara D. J. Mikhaeil, J. Harrington, M. van Lier Walqui. Cloud chamber constraints on depositional ice growth models. AGU Fall Meeting 2023. San Francisco, CA, December 2023. Lamb, Kara D. and P. Gentine. Exploring Phase Transitions and Dynamical Processes in Tropical Moist Convection Using Machine Learning. AGU Fall Meeting, San Francisco, CA, December 2023 [invited] Lamb, Kara D., et al. Learning Constraints on Depositional Ice Growth Models from Cloud ¬† Chamber Experiments with Physics Informed Neural Networks. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/437022 . Lamb, Kara D., M. van Lier Walqui, S. Santos, H. Morrison. Reduced Order Modeling to Reduce Structural Uncertainty in Representing Cloud Microphysical Process Rates. AGU Fall Meeting 2023. San Francisco, CA, December 2023. Lamb, Kara D. and P. Gentine. Zero shot learning of aerosol optical properties with graph neural networks. EGU General Assembly, April 2024. [invited] Loftus, Kaitlyn, et al. Parameterizing Cloud Microphysics with Machine Learning-Enabled Bayesian Parameter Inference. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/435624 . Loftus, Kaitlyn, et al. Parameterizing Cloud Microphysics with Machine Learning-Enabled Bayesian Parameter Inference. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1429616 . McKinley, et al. Constraining historical ocean carbon uptake with models, machine learning and data. World Climate Research Program Open Science Conference. Rwanda, Oct. 2023. Mikhaeil, Jonas Magdy, et al. Bayesian Workflow for the Evaluation of Constraints on Depositional Ice Growth Models with Cloud Chamber Observations. AMS, 2024, https://ams.confex.com/ams/104ANNUAL/meetingapp.cgi/Paper/436623 . Pizmony-Levy, Oren, Rivet, Ann, Torres, Chris, and Urbach, Noa. New York City Educators‚Äô Perceptions of Students‚Äô Engagement with Climate Change. Annual Meeting of the Comparative and International Education Society (CIES). Miami, Florida (2024). Presenter: Urbach, Noa Pizmony-Levy, Oren, Rivet, Ann, Torres, Chris, and Urbach, Noa. Tik Tok, Cheese Sticks, and Our Future: How Educators Perceive Students‚Äô Engagement with Climate Change. Annual Meeting of the American Educational Research Association (AERA). Philadelphia, Pennsylvania (2024). Presenters: Torres, Chris, and Urbach, Noa Will, Justus C., Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt. Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds. In Machine Learning and the Physical Sciences Workshop, Neural Information Processing Conference 2023. https://arxiv.org/abs/2310.20168 . Yang, Qingyuan, et al. Flexible Use of Additive Gaussian Processes as a Powerful Tool for More Interpretable Analysis and Emulation of Climate Model PPEs. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1341071 . Yang, Qingyuan, and Susanna Jenkins. Two Sources of Uncertainty in Estimating Tephra Volumes from Isopachs: Perspectives and Quantification. AGU, 2023, https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1343724 . Zanna, Laure. The New Generation of Global Climate Models Enhanced by Machine Learning (Invited). AGU 2023. https://agu.confex.com/agu/fm23/meetingapp.cgi/Paper/1366219 . Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research/seminars-and-panels/",
    "transcript": "Seminars and Panels - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Seminars + Panels Spring 2023 LEAP LECTURES in CLIMATE DATA SCIENCE See the full calendar of Lectures HERE . Fall 2022 LEAP LECTURES in CLIMATE DATA SCIENCE See the full calendar of Lectures HERE . Summer 2022 Seminar: Confronting the Challenge of Modeling Cloud and Precipitation Microphysics Speaker: Dr. Hugh Morrison Date: Thursday, July 21 Time: 11:00 a.m.‚Äì12:30 p.m. EST Format: Hybrid (in-person and virtual options) In-person location: Columbia Innovation Hub, Tang Family Hall (Room 202), 2276 12th Avenue, New York, NY 10027 ( Directions ) Virtual option: Zoom link Abstract: In the atmosphere, microphysics ‚Äì the small-scale processes affecting cloud and precipitation particles such as their growth by condensation, evaporation, and melting ‚Äì is a critical part of Earth‚Äôs climate. Because it is impossible to simulate every cloud particle individually owing to their sheer number within even a small cloud, atmospheric models have to represent particle populations statistically using microphysics parameterization schemes. However, there are critical gaps in knowledge of the microphysical processes that act on particles, especially for atmospheric ice particles because of their wide variety and intricacy of their shapes. The difficulty of representing cloud and precipitation particle populations and fundamental knowledge gaps in microphysical processes both introduce important uncertainties into models that translate into uncertainty in weather forecasts and climate simulations. I will discuss several specific challenges related to these problems. To improve how cloud and precipitation particle populations are represented within microphysics schemes, a ‚Äúparticle-based‚Äù approach is advocated that addresses several limitations of traditional approaches and has recently gained traction as a tool for cloud modeling. Advances in observations, including laboratory studies, are argued to be essential for addressing critical gaps in knowledge of microphysical processes. I will also advocate using statistical modeling tools to improve how these observations are used to constrain microphysics schemes. Finally, a hierarchical approach will be outlined that combines the various pieces discussed in this talk, providing a possible blueprint for accelerating progress in how microphysics is represented in models. Bio: Hugh Morrison studies cloud physics and dynamics, especially using numerical models. He grew up in Minnesota and got his B.S. at the University of Minnesota in 1997. He moved to Boulder, Colorado and received his M.S. in 2000 and Ph.D. in 2003 at the University of Colorado in Atmospheric Science. He then went to the National Center for Atmospheric Research (NCAR) as a post-doc in the Advanced Study Program. Hugh became a staff scientist at NCAR in 2008 and is currently a senior scientist in the Mesoscale and Microscale Meteorology Laboratory at NCAR. Bayesian Modeling for Climate Modeling Speaker: Dr. Marcus van Lier-Walqui Date: Friday, June 24, 2022 Time: 2:00pm ‚Äì 4:00pm EST Format: Hybrid (in-person and virtual options) In-person location: Columbia Innovation Hub, Tang Family Hall (Room 202), 2276 12th Avenue, New York, NY 10027 ( Directions ) Virtual option: Zoom link Abstract: In the atmosphere, microphysics ‚Äì the small-scale processes affecting cloud and precipitation particles such as their growth by condensation, evaporation, and melting ‚Äì is a critical part of Earth‚Äôs climate. Because it is impossible to simulate every cloud particle individually owing to their sheer number within even a small cloud, atmospheric models have to represent particle populations statistically using microphysics parameterization schemes. However, there are critical gaps in knowledge of the microphysical processes that act on particles, especially for atmospheric ice particles because of their wide variety and intricacy of their shapes. The difficulty of representing cloud and precipitation particle populations and fundamental knowledge gaps in microphysical processes both introduce important uncertainties into models that translate into uncertainty in weather forecasts and climate simulations. I will discuss several specific challenges related to these problems. To improve how cloud and precipitation particle populations are represented within microphysics schemes, a ‚Äúparticle-based‚Äù approach is advocated that addresses several limitations of traditional approaches and has recently gained traction as a tool for cloud modeling. Advances in observations, including laboratory studies, are argued to be essential for addressing critical gaps in knowledge of microphysical processes. I will also advocate using statistical modeling tools to improve how these observations are used to constrain microphysics schemes. Finally, a hierarchical approach will be outlined that combines the various pieces discussed in this talk, providing a possible blueprint for accelerating progress in how microphysics is represented in models. Spring 2022 Seminar: Machine Learning Rain to Improve Weather and Climate Prediction Speaker: Dr. Andrew Gettelman, National Center for Atmospheric Research (NCAR) Date: Wednesday, April 27, 2022 Time: 4:30pm ‚Äì 5:30pm EST Format: Hybrid (in-person and virtual options) In-person location: Columbia Innovation Hub, 2276 12th Avenue, Second Floor, Room 206, New York, NY 10027 ( Directions ) Virtual option: Zoom link provided upon registration Abstract: Rain is a critical part of the hydrologic cycle and a key process in understanding the evolution of climate. This talk will provide a background on why and how rain is a critical part of the climate system. The methods for representing rain formation at the scale of micrometer (10-6m) in models which represent the earth‚Äôs general circulation (at the scale of 10+6m) are approximate, and this is a critical problem in weather and climate. To approach this problem, we take detailed numerical approaches to the rain process typically used for small scale models, and put them in a global model. This results in simulations that have higher fidelity with respect to observations and reduce longstanding biases in the global model, but at prohibitive computational cost. We then train neural networks on the detailed treatments, and put those back into the full model. This reproduces all the major features of the detailed simulations from weather to climate, with no impact in computational cost. Diagnostics on the neural networks illustrate the challenges in representing these processes with machine learning such as the potential for overfitting and problems with extrapolation, as well as how physical processes important for climate challenge machine learning methods and provide opportunities for extending them. Bio: Dr. Andrew Gettelman is a senior scientist at the National Center for Atmospheric Research, specializing in global climate modeling. His work concerns development and analysis of global model simulations of the climate system with a focus on development of physical parameterizations of clouds in global climate models, and the physics and chemistry of the Upper Troposphere and Lower Stratosphere. In addition to developing and managing community earth system models at NCAR, Dr. Gettelman is involved in model evaluation data analysis with satellites and field programs, having served on several satellite science teams and on the science teams for field projects. Dr. Gettelman has been a visiting professor in Physics in Oxford, an Erskine visiting fellow at the University of Canterbury in Christchurch, New Zealand, and a visiting professor in the Institute for Atmospheric and Climate Science at the Swiss Federal Institute of Technology (ETH) in Z√ºrich, Switzerland. Dr. Gettelman is a member of the atmospheric model development team for the Community Earth System Model. Dr. Gettelman is a contributing author and reviewer for international scientific assessments of climate change and ozone depletion. He has served on the COMET advisory board, and the NCAR Earth Observing Laboratory advisory board. Dr. Gettelman is author or co-author on over 200 peer reviewed publications, and a textbook (Demystifying Climate Models, Springer). Dr. Gettelman has a PhD in Atmospheric Sciences from the University of Washington and a Bachelors of Science in Civil Engineering from Princeton University. Panel: LEAP, Learning the Earth with Artificial Intelligence and Physics ( video recording ) LEAP Team Presenters: Courtney D. Cogburn, Pierre Gentine, and Tian Zheng, Columbia; Dave Lawrence, National Center for Atmospheric Research (NCAR) Date: Tuesday, April 26, 2022 Time: 2:00pm ‚Äì 3:00pm ET Format: Hybrid (in-person and virtual options) In-person location: Columbia Business School, Kravis (Room 690), 665 W 130th Street, New York, NY 10027 Virtual option: Zoom link provided upon registration Please join the Computational Social Science (CSS) Working Group for a special event with LEAP, Learning the Earth with Artificial Intelligence and Physics, the recently launched NSF STC here at Columbia University. We will discuss the core areas of LEAP‚Äôs research, why education, equity, and bi-directional knowledge transfer are core to LEAP‚Äôs mission, why climate modeling may be important for your scholarship, and the open-access data and computing platform, LEAPangeo, that we hope will transform climate data science. In addition to introducing LEAP to the broader Columbia community, our discussion will also set a foundation for LEAP engagement, collaboration, and data and knowledge sharing generally. For speaker bios and more information, please view the event listing , courtesy of Columbia‚Äôs Data Science Institute . Seminar: From probabilistic forecasting to neural data compression and back: a latent variable perspective. Speaker: Stephan Mandt, UC Irvine Date: Monday, March 28, 2022 Time: 4:10pm ‚Äì 5:10pm ET Format: Hybrid (in-person and virtual options) In-person location: School of Social Work, 1255 Amsterdam Avenue, Room 903 ( Directions ) Virtual option: Zoom Link (passcode 860249) Abstract: The past few years have seen deep generative models mature into promising applications. Two of these applications include neural data compression and forecasting high-dimensional time series, including video. I will begin by reviewing the basic ideas behind neural data compression and show how advances in approximate Bayesian inference and generative modeling can significantly improve the compression performance of existing models. Finally, I show how neural video codecs can inspire probabilistic forecasting, leading to probabilistic sequence prediction methods with high potential for data-driven weather prediction. Bio: Stephan Mandt is an Assistant Professor of Computer Science and Statistics at the University of California, Irvine. From 2016 until 2018, he was a Senior Researcher and Head of the statistical machine learning group at Disney Research, first in Pittsburgh and later in Los Angeles. He held previous postdoctoral positions at Columbia University and Princeton University. Stephan holds a Ph.D. in Theoretical Physics from the University of Cologne, where he received the German National Merit Scholarship. Furthermore, he is a Kavli Fellow of the U.S. National Academy of Sciences, an NSF CAREER Awardee, a member of the ELLIS Society, and a former visiting researcher at Google Brain. Stephan regularly serves as an Area Chair, Action Editor, or Editorial Board member for NeurIPS, ICML, AAAI, ICLR, TMLR, and JMLR. His research is currently supported by NSF, DARPA, DOE, Disney, Intel, and Qualcomm. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research/seminars-and-panels/#content",
    "transcript": "Seminars and Panels - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Seminars + Panels Spring 2023 LEAP LECTURES in CLIMATE DATA SCIENCE See the full calendar of Lectures HERE . Fall 2022 LEAP LECTURES in CLIMATE DATA SCIENCE See the full calendar of Lectures HERE . Summer 2022 Seminar: Confronting the Challenge of Modeling Cloud and Precipitation Microphysics Speaker: Dr. Hugh Morrison Date: Thursday, July 21 Time: 11:00 a.m.‚Äì12:30 p.m. EST Format: Hybrid (in-person and virtual options) In-person location: Columbia Innovation Hub, Tang Family Hall (Room 202), 2276 12th Avenue, New York, NY 10027 ( Directions ) Virtual option: Zoom link Abstract: In the atmosphere, microphysics ‚Äì the small-scale processes affecting cloud and precipitation particles such as their growth by condensation, evaporation, and melting ‚Äì is a critical part of Earth‚Äôs climate. Because it is impossible to simulate every cloud particle individually owing to their sheer number within even a small cloud, atmospheric models have to represent particle populations statistically using microphysics parameterization schemes. However, there are critical gaps in knowledge of the microphysical processes that act on particles, especially for atmospheric ice particles because of their wide variety and intricacy of their shapes. The difficulty of representing cloud and precipitation particle populations and fundamental knowledge gaps in microphysical processes both introduce important uncertainties into models that translate into uncertainty in weather forecasts and climate simulations. I will discuss several specific challenges related to these problems. To improve how cloud and precipitation particle populations are represented within microphysics schemes, a ‚Äúparticle-based‚Äù approach is advocated that addresses several limitations of traditional approaches and has recently gained traction as a tool for cloud modeling. Advances in observations, including laboratory studies, are argued to be essential for addressing critical gaps in knowledge of microphysical processes. I will also advocate using statistical modeling tools to improve how these observations are used to constrain microphysics schemes. Finally, a hierarchical approach will be outlined that combines the various pieces discussed in this talk, providing a possible blueprint for accelerating progress in how microphysics is represented in models. Bio: Hugh Morrison studies cloud physics and dynamics, especially using numerical models. He grew up in Minnesota and got his B.S. at the University of Minnesota in 1997. He moved to Boulder, Colorado and received his M.S. in 2000 and Ph.D. in 2003 at the University of Colorado in Atmospheric Science. He then went to the National Center for Atmospheric Research (NCAR) as a post-doc in the Advanced Study Program. Hugh became a staff scientist at NCAR in 2008 and is currently a senior scientist in the Mesoscale and Microscale Meteorology Laboratory at NCAR. Bayesian Modeling for Climate Modeling Speaker: Dr. Marcus van Lier-Walqui Date: Friday, June 24, 2022 Time: 2:00pm ‚Äì 4:00pm EST Format: Hybrid (in-person and virtual options) In-person location: Columbia Innovation Hub, Tang Family Hall (Room 202), 2276 12th Avenue, New York, NY 10027 ( Directions ) Virtual option: Zoom link Abstract: In the atmosphere, microphysics ‚Äì the small-scale processes affecting cloud and precipitation particles such as their growth by condensation, evaporation, and melting ‚Äì is a critical part of Earth‚Äôs climate. Because it is impossible to simulate every cloud particle individually owing to their sheer number within even a small cloud, atmospheric models have to represent particle populations statistically using microphysics parameterization schemes. However, there are critical gaps in knowledge of the microphysical processes that act on particles, especially for atmospheric ice particles because of their wide variety and intricacy of their shapes. The difficulty of representing cloud and precipitation particle populations and fundamental knowledge gaps in microphysical processes both introduce important uncertainties into models that translate into uncertainty in weather forecasts and climate simulations. I will discuss several specific challenges related to these problems. To improve how cloud and precipitation particle populations are represented within microphysics schemes, a ‚Äúparticle-based‚Äù approach is advocated that addresses several limitations of traditional approaches and has recently gained traction as a tool for cloud modeling. Advances in observations, including laboratory studies, are argued to be essential for addressing critical gaps in knowledge of microphysical processes. I will also advocate using statistical modeling tools to improve how these observations are used to constrain microphysics schemes. Finally, a hierarchical approach will be outlined that combines the various pieces discussed in this talk, providing a possible blueprint for accelerating progress in how microphysics is represented in models. Spring 2022 Seminar: Machine Learning Rain to Improve Weather and Climate Prediction Speaker: Dr. Andrew Gettelman, National Center for Atmospheric Research (NCAR) Date: Wednesday, April 27, 2022 Time: 4:30pm ‚Äì 5:30pm EST Format: Hybrid (in-person and virtual options) In-person location: Columbia Innovation Hub, 2276 12th Avenue, Second Floor, Room 206, New York, NY 10027 ( Directions ) Virtual option: Zoom link provided upon registration Abstract: Rain is a critical part of the hydrologic cycle and a key process in understanding the evolution of climate. This talk will provide a background on why and how rain is a critical part of the climate system. The methods for representing rain formation at the scale of micrometer (10-6m) in models which represent the earth‚Äôs general circulation (at the scale of 10+6m) are approximate, and this is a critical problem in weather and climate. To approach this problem, we take detailed numerical approaches to the rain process typically used for small scale models, and put them in a global model. This results in simulations that have higher fidelity with respect to observations and reduce longstanding biases in the global model, but at prohibitive computational cost. We then train neural networks on the detailed treatments, and put those back into the full model. This reproduces all the major features of the detailed simulations from weather to climate, with no impact in computational cost. Diagnostics on the neural networks illustrate the challenges in representing these processes with machine learning such as the potential for overfitting and problems with extrapolation, as well as how physical processes important for climate challenge machine learning methods and provide opportunities for extending them. Bio: Dr. Andrew Gettelman is a senior scientist at the National Center for Atmospheric Research, specializing in global climate modeling. His work concerns development and analysis of global model simulations of the climate system with a focus on development of physical parameterizations of clouds in global climate models, and the physics and chemistry of the Upper Troposphere and Lower Stratosphere. In addition to developing and managing community earth system models at NCAR, Dr. Gettelman is involved in model evaluation data analysis with satellites and field programs, having served on several satellite science teams and on the science teams for field projects. Dr. Gettelman has been a visiting professor in Physics in Oxford, an Erskine visiting fellow at the University of Canterbury in Christchurch, New Zealand, and a visiting professor in the Institute for Atmospheric and Climate Science at the Swiss Federal Institute of Technology (ETH) in Z√ºrich, Switzerland. Dr. Gettelman is a member of the atmospheric model development team for the Community Earth System Model. Dr. Gettelman is a contributing author and reviewer for international scientific assessments of climate change and ozone depletion. He has served on the COMET advisory board, and the NCAR Earth Observing Laboratory advisory board. Dr. Gettelman is author or co-author on over 200 peer reviewed publications, and a textbook (Demystifying Climate Models, Springer). Dr. Gettelman has a PhD in Atmospheric Sciences from the University of Washington and a Bachelors of Science in Civil Engineering from Princeton University. Panel: LEAP, Learning the Earth with Artificial Intelligence and Physics ( video recording ) LEAP Team Presenters: Courtney D. Cogburn, Pierre Gentine, and Tian Zheng, Columbia; Dave Lawrence, National Center for Atmospheric Research (NCAR) Date: Tuesday, April 26, 2022 Time: 2:00pm ‚Äì 3:00pm ET Format: Hybrid (in-person and virtual options) In-person location: Columbia Business School, Kravis (Room 690), 665 W 130th Street, New York, NY 10027 Virtual option: Zoom link provided upon registration Please join the Computational Social Science (CSS) Working Group for a special event with LEAP, Learning the Earth with Artificial Intelligence and Physics, the recently launched NSF STC here at Columbia University. We will discuss the core areas of LEAP‚Äôs research, why education, equity, and bi-directional knowledge transfer are core to LEAP‚Äôs mission, why climate modeling may be important for your scholarship, and the open-access data and computing platform, LEAPangeo, that we hope will transform climate data science. In addition to introducing LEAP to the broader Columbia community, our discussion will also set a foundation for LEAP engagement, collaboration, and data and knowledge sharing generally. For speaker bios and more information, please view the event listing , courtesy of Columbia‚Äôs Data Science Institute . Seminar: From probabilistic forecasting to neural data compression and back: a latent variable perspective. Speaker: Stephan Mandt, UC Irvine Date: Monday, March 28, 2022 Time: 4:10pm ‚Äì 5:10pm ET Format: Hybrid (in-person and virtual options) In-person location: School of Social Work, 1255 Amsterdam Avenue, Room 903 ( Directions ) Virtual option: Zoom Link (passcode 860249) Abstract: The past few years have seen deep generative models mature into promising applications. Two of these applications include neural data compression and forecasting high-dimensional time series, including video. I will begin by reviewing the basic ideas behind neural data compression and show how advances in approximate Bayesian inference and generative modeling can significantly improve the compression performance of existing models. Finally, I show how neural video codecs can inspire probabilistic forecasting, leading to probabilistic sequence prediction methods with high potential for data-driven weather prediction. Bio: Stephan Mandt is an Assistant Professor of Computer Science and Statistics at the University of California, Irvine. From 2016 until 2018, he was a Senior Researcher and Head of the statistical machine learning group at Disney Research, first in Pittsburgh and later in Los Angeles. He held previous postdoctoral positions at Columbia University and Princeton University. Stephan holds a Ph.D. in Theoretical Physics from the University of Cologne, where he received the German National Merit Scholarship. Furthermore, he is a Kavli Fellow of the U.S. National Academy of Sciences, an NSF CAREER Awardee, a member of the ELLIS Society, and a former visiting researcher at Google Brain. Stephan regularly serves as an Area Chair, Action Editor, or Editorial Board member for NeurIPS, ICML, AAAI, ICLR, TMLR, and JMLR. His research is currently supported by NSF, DARPA, DOE, Disney, Intel, and Qualcomm. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research/policies/",
    "transcript": "Policies - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Policies Acknowledgements Acknowledging LEAP Support in Publications All papers supported in full or in part by LEAP should acknowledge LEAP funding in the Acknowledgements section as follows: ‚ÄúWe acknowledge funding from NSF through the Learning the Earth with Artificial intelligence and Physics (LEAP) Science and Technology Center (STC) (Award #2019625).‚Äù Types of Membership LEAP‚Äôs Executive Committee invites you to APPLY TO the following types of LEAP Membership: PUBLIC MEMBERSHIP. (Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP. Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP. Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHER MEMBERSHIP. Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. Innovation Hub Space Use Policy LEAP‚Äôs Executive Committee is pleased to share the Innovation Hub Space Use Policy portion of our Operations Manual. As no written policy can cover all situations, we expect that questions will arise. Please contact LEAP‚Äôs Managing Director, Molly Lopez , with any questions or concerns. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/research/policies/#content",
    "transcript": "Policies - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Policies Acknowledgements Acknowledging LEAP Support in Publications All papers supported in full or in part by LEAP should acknowledge LEAP funding in the Acknowledgements section as follows: ‚ÄúWe acknowledge funding from NSF through the Learning the Earth with Artificial intelligence and Physics (LEAP) Science and Technology Center (STC) (Award #2019625).‚Äù Types of Membership LEAP‚Äôs Executive Committee invites you to APPLY TO the following types of LEAP Membership: PUBLIC MEMBERSHIP. (Please note: Public Membership will be live soon.) Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP. Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP. Open to researchers who have been referred by a LEAP scientist. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHER MEMBERSHIP. Open to researchers who receive LEAP funding. Receive weekly LEAP newsletter and invitations to LEAP events. Access to LEAP Pangeo compute and storage for duration of time with LEAP. Innovation Hub Space Use Policy LEAP‚Äôs Executive Committee is pleased to share the Innovation Hub Space Use Policy portion of our Operations Manual. As no written policy can cover all situations, we expect that questions will arise. Please contact LEAP‚Äôs Managing Director, Molly Lopez , with any questions or concerns. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/",
    "transcript": "LEAP Pangeo - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo What is LEAP Pangeo? LEAP Pangeo is our modern data cloud computing infrastructure for data climate science research. This infrastructure: Empowers LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives; Facilitates seamless collaboration between project members around data-intensive science, accelerating research progress; Enables rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research; and Places actionable data in the hands of LEAP partners to support knowledge transfer. Read more here and browse the LEAP Pangeo Technical Documentation . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/#content",
    "transcript": "LEAP Pangeo - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo What is LEAP Pangeo? LEAP Pangeo is our modern data cloud computing infrastructure for data climate science research. This infrastructure: Empowers LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives; Facilitates seamless collaboration between project members around data-intensive science, accelerating research progress; Enables rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research; and Places actionable data in the hands of LEAP partners to support knowledge transfer. Read more here and browse the LEAP Pangeo Technical Documentation . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-sub/",
    "transcript": "LEAP Pangeo - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo What is LEAP Pangeo? LEAP Pangeo is our modern data cloud computing infrastructure for data climate science research. This infrastructure: Empowers LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives; Facilitates seamless collaboration between project members around data-intensive science, accelerating research progress; Enables rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research; and Places actionable data in the hands of LEAP partners to support knowledge transfer. Read more here and browse the LEAP Pangeo Technical Documentation . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-sub/#content",
    "transcript": "LEAP Pangeo - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo What is LEAP Pangeo? LEAP Pangeo is our modern data cloud computing infrastructure for data climate science research. This infrastructure: Empowers LEAP participants with instant access to high-performance computing and analysis-ready data in order to support ambitious research objectives; Facilitates seamless collaboration between project members around data-intensive science, accelerating research progress; Enables rich data-driven classroom experiences for learners, helping them transition successfully from coursework to research; and Places actionable data in the hands of LEAP partners to support knowledge transfer. Read more here and browse the LEAP Pangeo Technical Documentation . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-benefits/",
    "transcript": "Benefits - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo: Benefits Benefits of LEAP Pangeo FOR RESEARCH Easier collaboration Reproducible results Free resources (storage + compute) for the user Better support and faster scientific iterations FOR EDUCATION Zero set-up during classes Scalable to hundreds of students Ability to work with ‚Äòreal world‚Äô datasets Students and researchers are on the same platform FOR KNOWLEDGE TRANSFER Flexible data access options: LEAP partners can bring their own tools + data or use ours Fully public datasets: LEAP partners can build their own stories + products Broader impact of climate data science! Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-benefits/#content",
    "transcript": "Benefits - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo: Benefits Benefits of LEAP Pangeo FOR RESEARCH Easier collaboration Reproducible results Free resources (storage + compute) for the user Better support and faster scientific iterations FOR EDUCATION Zero set-up during classes Scalable to hundreds of students Ability to work with ‚Äòreal world‚Äô datasets Students and researchers are on the same platform FOR KNOWLEDGE TRANSFER Flexible data access options: LEAP partners can bring their own tools + data or use ours Fully public datasets: LEAP partners can build their own stories + products Broader impact of climate data science! Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-access/",
    "transcript": "Request Access - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Accessing LEAP Pangeo REQUEST ACCESS Request Access to LEAP Pangeo LEAP‚Äôs Executive Committee invites you to APPLY TO the following types of LEAP Membership to gain access to LEAP Pangeo: PUBLIC MEMBERSHIP. (Please note: Public Membership will be live soon.) ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP. Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP. Open to researchers who have been referred by a LEAP scientist. ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHER MEMBERSHIP. Open to researchers who receive LEAP funding. ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo compute and storage for duration of time with LEAP. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-access/#content",
    "transcript": "Request Access - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Accessing LEAP Pangeo REQUEST ACCESS Request Access to LEAP Pangeo LEAP‚Äôs Executive Committee invites you to APPLY TO the following types of LEAP Membership to gain access to LEAP Pangeo: PUBLIC MEMBERSHIP. (Please note: Public Membership will be live soon.) ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo that includes limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. EDUCATION MEMBERSHIP. Open to participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo compute and storage for duration of education program. RESEARCH MEMBERSHIP. Open to researchers who have been referred by a LEAP scientist. ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo compute and storage for six (6) months (renewal applications may be submitted at the conclusion of six months). LEAP-FUNDED RESEARCHER MEMBERSHIP. Open to researchers who receive LEAP funding. ‚Äì Receive weekly LEAP newsletter and invitations to LEAP events. ‚Äì Access to LEAP Pangeo compute and storage for duration of time with LEAP. Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-cite/",
    "transcript": "Citing LEAP Pangeo - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo: Citation Citing LEAP Pangeo If you use the LEAP Pangeo platform to perform analysis, please add a statement similar to the below in your acknowledgement section of the paper: ‚ÄúWe acknowledge the computing and storage resources¬† provided by the ‚ÄòNSF Science and Technology Center (STC) Learning the Earth With Artificial Intelligence and Physics (LEAP)‚Äô (Award #2019625).‚Äù Please read about how to cite LEAP Pangeo HERE . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-pangeo-3/leap-pangeo-cite/#content",
    "transcript": "Citing LEAP Pangeo - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Pangeo: Citation Citing LEAP Pangeo If you use the LEAP Pangeo platform to perform analysis, please add a statement similar to the below in your acknowledgement section of the paper: ‚ÄúWe acknowledge the computing and storage resources¬† provided by the ‚ÄòNSF Science and Technology Center (STC) Learning the Earth With Artificial Intelligence and Physics (LEAP)‚Äô (Award #2019625).‚Äù Please read about how to cite LEAP Pangeo HERE . Support LEAP Help support our grand challenge to develop the next generation of climate model and climate projection for tailored adaptation. Please contact Molly Lopez , Managing Director of LEAP, to start the conversation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/",
    "transcript": "Education - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Education Overview + Broadening Participation LEAP‚Äôs education programs foster convergence of climate science and data science, and vertical integration of research and education, forge a learning community including K-12 teachers, undergraduate, graduate, postdoc, faculty and other stakeholders, and promote a shared mission of societal impact, a collaborative culture, peer learning and mentoring, and the responsible / ethical use of data. Our curriculum is tightly connected with our research and provides multiple entryways into Climate Data Science to both undergraduate and graduates students from climate and data science backgrounds. LEAP offers immersive research experience programs to undergraduates and K-12 teachers. Our education programs strive to broaden participation in Climate Science and Data Science. Our education and outreach programs allow LEAP researchers to amplify their research‚Äôs societal impact by engaging in activities such as mentoring, communication, translation, human-centered design, curriculum development, and evaluation and assessment. LEAP also seeks to broaden participation via partnerships with Columbia‚Äôs Bridge-to-PhD in STEM program and the University Corporation for Atmospheric Research (UCAR)‚Äôs Significant Opportunities in Atmospheric Research and Science (SOARS) program. We also organize Hackathons for climate data science, and partner with the American Museum of Natural History (AMNH) and STEM programs to reach a wide educational audience. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/#content",
    "transcript": "Education - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Education Overview + Broadening Participation LEAP‚Äôs education programs foster convergence of climate science and data science, and vertical integration of research and education, forge a learning community including K-12 teachers, undergraduate, graduate, postdoc, faculty and other stakeholders, and promote a shared mission of societal impact, a collaborative culture, peer learning and mentoring, and the responsible / ethical use of data. Our curriculum is tightly connected with our research and provides multiple entryways into Climate Data Science to both undergraduate and graduates students from climate and data science backgrounds. LEAP offers immersive research experience programs to undergraduates and K-12 teachers. Our education programs strive to broaden participation in Climate Science and Data Science. Our education and outreach programs allow LEAP researchers to amplify their research‚Äôs societal impact by engaging in activities such as mentoring, communication, translation, human-centered design, curriculum development, and evaluation and assessment. LEAP also seeks to broaden participation via partnerships with Columbia‚Äôs Bridge-to-PhD in STEM program and the University Corporation for Atmospheric Research (UCAR)‚Äôs Significant Opportunities in Atmospheric Research and Science (SOARS) program. We also organize Hackathons for climate data science, and partner with the American Museum of Natural History (AMNH) and STEM programs to reach a wide educational audience. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/leap-education/",
    "transcript": "LEAP Education - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Education Overview + Broadening Participation LEAP‚Äôs education programs foster convergence of climate science and data science, and vertical integration of research and education, forge a learning community including K-12 teachers, undergraduate, graduate, postdoc, faculty and other stakeholders, and promote a shared mission of societal impact, a collaborative culture, peer learning and mentoring, and the responsible / ethical use of data. Our curriculum is tightly connected with our research and provides multiple entryways into Climate Data Science to both undergraduate and graduates students from climate and data science backgrounds. LEAP offers immersive research experience programs to undergraduates and K-12 teachers. Our education programs strive to broaden participation in Climate Science and Data Science. Our education and outreach programs allow LEAP researchers to amplify their research‚Äôs societal impact by engaging in activities such as mentoring, communication, translation, human-centered design, curriculum development, and evaluation and assessment. LEAP also seeks to broaden participation via partnerships with Columbia‚Äôs Bridge-to-PhD in STEM program and the University Corporation for Atmospheric Research (UCAR)‚Äôs Significant Opportunities in Atmospheric Research and Science (SOARS) program. We also organize Hackathons for climate data science, and partner with the American Museum of Natural History (AMNH) and STEM programs to reach a wide educational audience. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/leap-education/#content",
    "transcript": "LEAP Education - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Education Overview + Broadening Participation LEAP‚Äôs education programs foster convergence of climate science and data science, and vertical integration of research and education, forge a learning community including K-12 teachers, undergraduate, graduate, postdoc, faculty and other stakeholders, and promote a shared mission of societal impact, a collaborative culture, peer learning and mentoring, and the responsible / ethical use of data. Our curriculum is tightly connected with our research and provides multiple entryways into Climate Data Science to both undergraduate and graduates students from climate and data science backgrounds. LEAP offers immersive research experience programs to undergraduates and K-12 teachers. Our education programs strive to broaden participation in Climate Science and Data Science. Our education and outreach programs allow LEAP researchers to amplify their research‚Äôs societal impact by engaging in activities such as mentoring, communication, translation, human-centered design, curriculum development, and evaluation and assessment. LEAP also seeks to broaden participation via partnerships with Columbia‚Äôs Bridge-to-PhD in STEM program and the University Corporation for Atmospheric Research (UCAR)‚Äôs Significant Opportunities in Atmospheric Research and Science (SOARS) program. We also organize Hackathons for climate data science, and partner with the American Museum of Natural History (AMNH) and STEM programs to reach a wide educational audience. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/education-reach/",
    "transcript": "Education Reach - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP's Education Reach LEAP‚Äôs research and areas of focus are reflected in a broad range of courses and programs with which LEAP researchers are affiliated, both at Columbia University and at LEAP‚Äôs partner institutions. Relevant Courses + Educational Resources Undergraduate Programs Graduate Programs Undergraduate-Level Courses Graduate-Level Courses Research Experience Program Informal Education Undergraduate Programs Columbia University has multiple undergraduate majors and minors related to LEAP‚Äôs mission. Many LEAP senior personnel are affiliated with these programs and LEAP has established contacts with program directors of these programs. BA in Climate System Science BA in Earth Science BA in Environmental Science BA in Climate and Sustainability Minor in Climate System Science Minor in Earth and Environmental Science BS in Earth and Environmental Engineering Minor in Earth and Environmental Engineering BS in Computer Science BS in Informational Science BA in Statistics Minor in Statistics Earth System Sciences ‚Äì University of California at Irvine Program details Applied Computational Physics ‚Äì City University of New York Program details Graduate Programs Columbia University has graduate programs related to LEAP‚Äôs mission. Multiple LEAP senior personnel are affiliated with these programs and LEAP has established contacts with program directors of these programs. Doctoral programs in Earth and Environmental Science PhD in Earth and Environmental Engineering MS in Earth and Environmental Engineering PhD in Computer Science MS in Computer Science PhD in Statistics MA in Statistics MS in Climate MA in Climate and Society University of California at Irvine PhD in Earth System Science MS program in AI for Science Teachers College, Columbia University MA program in Sustainability & Education Undergraduate-Level Courses LEAP Courses at Columbia University Machine Learning (ML) for Environmental Engineering & Science Idealized Models of Climate Processes Humans and the Carbon Cycle LEAP collaborates with introductory courses in Computer Science and Machine Learning at Columbia for research and education integration Computing in Context (Computer Science) AI in Context (interdepartmental) Applied Machine Learning (Statistics) Applied Data Science (Statistic) University of California at Irvine Data Analysis for Earth Sciences University of California at San Diego Data Science Programming Deep Learning The Practice and Application of Data Science City University of New York An Introduction to the Physics of Natural Disasters Fluid Mechanics Environmental Economics Graduate-Level Courses LEAP‚Äôs courses at Columbia University: Climate Prediction Challenges with Machine Learning Readings in Climate Data Science Research Computing in Earth & Environmental Sciences LEAP collaborates with Columbia Business School‚Äôs Tamer Institute for Social Enterprise and Climate Change on their climate related electives. University of California at Irvine: ‚ÄúDeep Generative Models‚Äù featuring a module on ‚ÄúGenAI for Climate‚Äù Teachers College, Columbia University: AI & The Environment Climate Change, Society and Education University of California at San Diego: Generative AI AI Seminar Deep Learning for Environmental Science New York University: Data-driven Methods for Geophysical Fluids Climate Dynamics Machine Learning for Climate Change City University of New York: Machine Learning in Physics and Astronomy Introduction to Geographic Information Systems Research Experience Program LEAP: Momentum Fellows Program Research Experience for Undergraduates ( REU) Program City University of New York: Undergraduate Research Program Honors Scholars Program Informal Education LEAP: Momentum Bootcamp LEAP: Summer Institute for K-12 teachers Open Climate Curriculum Initiative led by Tamer Institute for Social Enterprise and Climate Change at Columbia Business School Climatematch Academy : with whom LEAP shares open-source education and free access to LEAP Pangeo Generative Modeling Summer School (Nice, France, 2025) Rising Stars in Data Science YouTube collections of fundamental courses in Data Science and Statistics (Fernandez-Granda) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/education-reach/#content",
    "transcript": "Education Reach - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP's Education Reach LEAP‚Äôs research and areas of focus are reflected in a broad range of courses and programs with which LEAP researchers are affiliated, both at Columbia University and at LEAP‚Äôs partner institutions. Relevant Courses + Educational Resources Undergraduate Programs Graduate Programs Undergraduate-Level Courses Graduate-Level Courses Research Experience Program Informal Education Undergraduate Programs Columbia University has multiple undergraduate majors and minors related to LEAP‚Äôs mission. Many LEAP senior personnel are affiliated with these programs and LEAP has established contacts with program directors of these programs. BA in Climate System Science BA in Earth Science BA in Environmental Science BA in Climate and Sustainability Minor in Climate System Science Minor in Earth and Environmental Science BS in Earth and Environmental Engineering Minor in Earth and Environmental Engineering BS in Computer Science BS in Informational Science BA in Statistics Minor in Statistics Earth System Sciences ‚Äì University of California at Irvine Program details Applied Computational Physics ‚Äì City University of New York Program details Graduate Programs Columbia University has graduate programs related to LEAP‚Äôs mission. Multiple LEAP senior personnel are affiliated with these programs and LEAP has established contacts with program directors of these programs. Doctoral programs in Earth and Environmental Science PhD in Earth and Environmental Engineering MS in Earth and Environmental Engineering PhD in Computer Science MS in Computer Science PhD in Statistics MA in Statistics MS in Climate MA in Climate and Society University of California at Irvine PhD in Earth System Science MS program in AI for Science Teachers College, Columbia University MA program in Sustainability & Education Undergraduate-Level Courses LEAP Courses at Columbia University Machine Learning (ML) for Environmental Engineering & Science Idealized Models of Climate Processes Humans and the Carbon Cycle LEAP collaborates with introductory courses in Computer Science and Machine Learning at Columbia for research and education integration Computing in Context (Computer Science) AI in Context (interdepartmental) Applied Machine Learning (Statistics) Applied Data Science (Statistic) University of California at Irvine Data Analysis for Earth Sciences University of California at San Diego Data Science Programming Deep Learning The Practice and Application of Data Science City University of New York An Introduction to the Physics of Natural Disasters Fluid Mechanics Environmental Economics Graduate-Level Courses LEAP‚Äôs courses at Columbia University: Climate Prediction Challenges with Machine Learning Readings in Climate Data Science Research Computing in Earth & Environmental Sciences LEAP collaborates with Columbia Business School‚Äôs Tamer Institute for Social Enterprise and Climate Change on their climate related electives. University of California at Irvine: ‚ÄúDeep Generative Models‚Äù featuring a module on ‚ÄúGenAI for Climate‚Äù Teachers College, Columbia University: AI & The Environment Climate Change, Society and Education University of California at San Diego: Generative AI AI Seminar Deep Learning for Environmental Science New York University: Data-driven Methods for Geophysical Fluids Climate Dynamics Machine Learning for Climate Change City University of New York: Machine Learning in Physics and Astronomy Introduction to Geographic Information Systems Research Experience Program LEAP: Momentum Fellows Program Research Experience for Undergraduates ( REU) Program City University of New York: Undergraduate Research Program Honors Scholars Program Informal Education LEAP: Momentum Bootcamp LEAP: Summer Institute for K-12 teachers Open Climate Curriculum Initiative led by Tamer Institute for Social Enterprise and Climate Change at Columbia Business School Climatematch Academy : with whom LEAP shares open-source education and free access to LEAP Pangeo Generative Modeling Summer School (Nice, France, 2025) Rising Stars in Data Science YouTube collections of fundamental courses in Data Science and Statistics (Fernandez-Granda) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/education-curriculum/",
    "transcript": "Curriculum - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Curriculum Overview Through the Design Studio , LEAP will integrate course-based research experiences into existing curriculum in climate science and data science across LEAP‚Äôs institutions. In addition, LEAP will develop three new transdisciplinary courses that will train a next generation of Climate Data Scientists. Courses Humans + the Carbon Cycle (Columbia University, GU4020) Instructor : Galen McKinley Fall 2024 : Tuesdays + Thursdays, 11:40am ‚Äì 12:55pm (EDT) The accelerating climate change of the current day is driven by humanity‚Äôs modifications to the global carbon cycle. This course offers an introduction basic science of the carbon cycle, with a focus on large-scale processes occurring on annual to centennial timescales. Students will leave this course with an understanding of the degree to which the global carbon cycle is understood and quantified, as well as the key uncertainties that are the focus of current research. We will build understanding of the potential pathways, and the significant challenges, to limiting global warming to 2o C as intended by the 2015 Paris Climate Agreement. The course will begin with a brief review of climate science basics and the role of CO2 in climate and climate change (weeks 1-2). In weeks 3-4, the natural reservoirs and fluxes that make up the global carbon cycle will be introduced. In week 5-6, anthropogenic emissions and the observed changes in climate associated with increasing atmospheric CO2 will be discussed. In weeks 7-11, we will learn about how the land biosphere and ocean are mitigating the increase in atmospheric CO2 and the feedbacks that may substantially modify these natural sinks. In weeks 12-13, the international policy process and the potential for carbon cycle management will be the focus. In weeks 14, students will present their final projects. Research Computing in Earth and Environmental Sciences (Columbia University, GR6901) Instructor : Yutian Wu Fall 2024 : Tuesdays + Thursdays, 2:40 ‚Äì 3:55pm (EDT) Computing has become an indispensable tool for Earth Scientists. This course will introduce incoming DEES PhD students to modern computing software, programming tools and best practices that are broadly applicable to carrying out research in the Earth Sciences. This includes an introduction to Unix, programming in three commonly used languages (Python, MATLAB and Fortran), version control and data backup, tools for visualizing geoscience data and making maps. Students will learn the basics of high performance computing and big data analysis tools available on cluster computers. Student learning will be facilitated through a combination of lectures, in-class exercises, homework assignments and class projects. All topics will be taught through example datasets or problems from Earth Sciences. The course is designed to be accessible for Earth Science graduate students in any discipline. Machine Learning for Environmental Engineering and Science (Columbia University, EAEE4000E) Instructors : Conrad Albrecht Fall 2024 : Wednesdays, 4:10 ‚Äì 6:40pm (EDT) Aimed at understanding and testing state-of-the-art methods in machine learning applied to environmental sciences and engineering problems. Potential applications include but are not limited to remote sensing, and environmental and geophysical fluid dynamics. Includes testing ‚Äúvanilla‚Äù ML algorithms, feedforward neural networks, random forests, shallow vs deep networks, and the details of machine learning techniques. Readings in Climate Data Science (Columbia University, EAEE6000E) Instructor : Greg Elsaesser Fall 2024 : Thursdays, 12:00 ‚Äì 1:30pm (EDT) In conjunction with LEAP-STC‚Äôs Fall 2024 Lectures in Climate Data Science , this ‚Äújournal-club‚Äù style course will expose geoscience students to cutting-edge data science methods, and data science students to currently-existing climate science problems and Earth system model development needs. Recommended Electives Would you like to recommend a related course to share with the LEAP community? Please submit your recommendation HERE . Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/education-curriculum/#content",
    "transcript": "Curriculum - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Curriculum Overview Through the Design Studio , LEAP will integrate course-based research experiences into existing curriculum in climate science and data science across LEAP‚Äôs institutions. In addition, LEAP will develop three new transdisciplinary courses that will train a next generation of Climate Data Scientists. Courses Humans + the Carbon Cycle (Columbia University, GU4020) Instructor : Galen McKinley Fall 2024 : Tuesdays + Thursdays, 11:40am ‚Äì 12:55pm (EDT) The accelerating climate change of the current day is driven by humanity‚Äôs modifications to the global carbon cycle. This course offers an introduction basic science of the carbon cycle, with a focus on large-scale processes occurring on annual to centennial timescales. Students will leave this course with an understanding of the degree to which the global carbon cycle is understood and quantified, as well as the key uncertainties that are the focus of current research. We will build understanding of the potential pathways, and the significant challenges, to limiting global warming to 2o C as intended by the 2015 Paris Climate Agreement. The course will begin with a brief review of climate science basics and the role of CO2 in climate and climate change (weeks 1-2). In weeks 3-4, the natural reservoirs and fluxes that make up the global carbon cycle will be introduced. In week 5-6, anthropogenic emissions and the observed changes in climate associated with increasing atmospheric CO2 will be discussed. In weeks 7-11, we will learn about how the land biosphere and ocean are mitigating the increase in atmospheric CO2 and the feedbacks that may substantially modify these natural sinks. In weeks 12-13, the international policy process and the potential for carbon cycle management will be the focus. In weeks 14, students will present their final projects. Research Computing in Earth and Environmental Sciences (Columbia University, GR6901) Instructor : Yutian Wu Fall 2024 : Tuesdays + Thursdays, 2:40 ‚Äì 3:55pm (EDT) Computing has become an indispensable tool for Earth Scientists. This course will introduce incoming DEES PhD students to modern computing software, programming tools and best practices that are broadly applicable to carrying out research in the Earth Sciences. This includes an introduction to Unix, programming in three commonly used languages (Python, MATLAB and Fortran), version control and data backup, tools for visualizing geoscience data and making maps. Students will learn the basics of high performance computing and big data analysis tools available on cluster computers. Student learning will be facilitated through a combination of lectures, in-class exercises, homework assignments and class projects. All topics will be taught through example datasets or problems from Earth Sciences. The course is designed to be accessible for Earth Science graduate students in any discipline. Machine Learning for Environmental Engineering and Science (Columbia University, EAEE4000E) Instructors : Conrad Albrecht Fall 2024 : Wednesdays, 4:10 ‚Äì 6:40pm (EDT) Aimed at understanding and testing state-of-the-art methods in machine learning applied to environmental sciences and engineering problems. Potential applications include but are not limited to remote sensing, and environmental and geophysical fluid dynamics. Includes testing ‚Äúvanilla‚Äù ML algorithms, feedforward neural networks, random forests, shallow vs deep networks, and the details of machine learning techniques. Readings in Climate Data Science (Columbia University, EAEE6000E) Instructor : Greg Elsaesser Fall 2024 : Thursdays, 12:00 ‚Äì 1:30pm (EDT) In conjunction with LEAP-STC‚Äôs Fall 2024 Lectures in Climate Data Science , this ‚Äújournal-club‚Äù style course will expose geoscience students to cutting-edge data science methods, and data science students to currently-existing climate science problems and Earth system model development needs. Recommended Electives Would you like to recommend a related course to share with the LEAP community? Please submit your recommendation HERE . Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/bootcamp/",
    "transcript": "Momentum Bootcamps - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Momentum Bootcamps Overview LEAP‚Äôs Momentum Bootcamps introduce participants to Climate Data Science through a variety of activities including lectures, labs, and team-hacking, with the goal of preparing colleagues for machine learning-enabled climate research and more advanced and intensive research-oriented workshops. Read about the Winter 2024 Momentum Bootcamp , and check out the Winter 2024 Momentum Bootcamp Call for Applications . Read about the Winter 2023 Momentum Bootcamp , and check out the Winter 2023 Momentum Bootcamp Call for Applications . Check out the Summer 2022 Momentum Bootcamp Call for Applications . SUBSCRIBE to hear about future Momentum Bootcamp opportunities. Bootcamp Goals LEAP Momentum Bootcamps are designed by LEAP researchers as part of LEAP‚Äôs education programming to: foster convergence of climate science and data science vertically integrate research and education, and forge a LEAP research and learning community including K-12 teachers, undergraduates, graduate students, postdocs, faculty, and other stakeholders Participation is open to all! Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/bootcamp/#content",
    "transcript": "Momentum Bootcamps - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Momentum Bootcamps Overview LEAP‚Äôs Momentum Bootcamps introduce participants to Climate Data Science through a variety of activities including lectures, labs, and team-hacking, with the goal of preparing colleagues for machine learning-enabled climate research and more advanced and intensive research-oriented workshops. Read about the Winter 2024 Momentum Bootcamp , and check out the Winter 2024 Momentum Bootcamp Call for Applications . Read about the Winter 2023 Momentum Bootcamp , and check out the Winter 2023 Momentum Bootcamp Call for Applications . Check out the Summer 2022 Momentum Bootcamp Call for Applications . SUBSCRIBE to hear about future Momentum Bootcamp opportunities. Bootcamp Goals LEAP Momentum Bootcamps are designed by LEAP researchers as part of LEAP‚Äôs education programming to: foster convergence of climate science and data science vertically integrate research and education, and forge a LEAP research and learning community including K-12 teachers, undergraduates, graduate students, postdocs, faculty, and other stakeholders Participation is open to all! Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/bootcamp/2025-winter-bootcamp/",
    "transcript": "Winter 2025 LEAP Momentum Bootcamp - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Winter 2025 Momentum Bootcamp AGENDA Overview LEAP‚Äôs Winter 2025 Momentum Bootcamp is a hands-on event designed to foster convergence of climate science and data science; vertically integrate research and education; and forge a LEAP research and learning community including K-12 teachers, PhDs, postdocs, faculty, and other stakeholders. The two-day Bootcamp aims to teach Climate Data Science in the cloud using python tools with an emphasis on reproducibility and collaboration. Participants will learn through a variety of activities including lectures, labs, and team hacking, which will prepare participants for machine learning-enabled climate research and more advanced and intensive research-oriented workshops. Bootcamp participants will: Discover, access, and explore open-access climate datasets, including satellite observations and climate simulations, using the Xarray python package. Calculate common climate statistics and diagnostics of variability and change using Xarray. Perform interactive visualization of climate data using the Holoviews package. Perform machine learning on spatio-temporal climate data Compare machine learning models‚Äô performance and prediction skill Perform open science in the cloud using the LEAP-Pangeo Jupyter Hub BOOTCAMP AGENDA Important Details Logistics: Monday ‚Äì Tuesday, January 13 ‚Äì 14, 2025 9:00 am ‚Äì 5:00 pm, both days Smith Learning Theater (Teachers College, Columbia University) Cost: Registration Fee: $250 Partial / Full financial aid may be available Important Dates December 16, 2024 : Application deadline December 20, 2024 : Applicants will be contacted to complete registration and secure Bootcamp participation slots January 13 ‚Äì 14, 2025 : Winter Momentum Bootcamp takes place in-person Eligibility Applications are welcome from: Faculty members and research scientists from LEAP institutions (Columbia, NYU, University of Minnesota, University of California at Irvine, NASA/GISS, NCAR) Postdocs and PhD students Research scientists NYC Public School teachers, Summer Institute LEAP Partners Members of the general public Pre-requisites: Basic knowledge of scientific computing with Python: numpy, matplotlib, pandas Access to LEAP-Pangeo Hub (participants will be granted access upon acceptance) Bring your own laptop and charger! Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/bootcamp/2025-winter-bootcamp/#content",
    "transcript": "Winter 2025 LEAP Momentum Bootcamp - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Winter 2025 Momentum Bootcamp AGENDA Overview LEAP‚Äôs Winter 2025 Momentum Bootcamp is a hands-on event designed to foster convergence of climate science and data science; vertically integrate research and education; and forge a LEAP research and learning community including K-12 teachers, PhDs, postdocs, faculty, and other stakeholders. The two-day Bootcamp aims to teach Climate Data Science in the cloud using python tools with an emphasis on reproducibility and collaboration. Participants will learn through a variety of activities including lectures, labs, and team hacking, which will prepare participants for machine learning-enabled climate research and more advanced and intensive research-oriented workshops. Bootcamp participants will: Discover, access, and explore open-access climate datasets, including satellite observations and climate simulations, using the Xarray python package. Calculate common climate statistics and diagnostics of variability and change using Xarray. Perform interactive visualization of climate data using the Holoviews package. Perform machine learning on spatio-temporal climate data Compare machine learning models‚Äô performance and prediction skill Perform open science in the cloud using the LEAP-Pangeo Jupyter Hub BOOTCAMP AGENDA Important Details Logistics: Monday ‚Äì Tuesday, January 13 ‚Äì 14, 2025 9:00 am ‚Äì 5:00 pm, both days Smith Learning Theater (Teachers College, Columbia University) Cost: Registration Fee: $250 Partial / Full financial aid may be available Important Dates December 16, 2024 : Application deadline December 20, 2024 : Applicants will be contacted to complete registration and secure Bootcamp participation slots January 13 ‚Äì 14, 2025 : Winter Momentum Bootcamp takes place in-person Eligibility Applications are welcome from: Faculty members and research scientists from LEAP institutions (Columbia, NYU, University of Minnesota, University of California at Irvine, NASA/GISS, NCAR) Postdocs and PhD students Research scientists NYC Public School teachers, Summer Institute LEAP Partners Members of the general public Pre-requisites: Basic knowledge of scientific computing with Python: numpy, matplotlib, pandas Access to LEAP-Pangeo Hub (participants will be granted access upon acceptance) Bring your own laptop and charger! Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/summer-reu-program/",
    "transcript": "Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Summer Research Experience for Undergraduates (REU) Overview LEAP‚Äôs summer Research Experience for Undergraduate (REU) program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. Each year, LEAP invites applications from students who will be a rising sophomore, junior, or senior at a college/university in the United States. We invite all students to apply . Participants receive a stipend for the 10-week program. On-campus housing and travel stipends are also available. 2023 Summer REU Cohort, with instructors Tian Zheng, Candace Agonafir, and Yu Huang The REU Program Each summer, LEAP‚Äôs REU Program kicks off with a Momentum Bootcamp , designed to familiarize participants with fundamentals of data science, machine learning, climate modeling, and developing research proposals. ** The Bootcamp is followed by the Summer Research Experience , during which participants apply the skills learned or enhanced in Bootcamp to ongoing LEAP research projects. Students are supervised and mentored by LEAP Senior Personnel and PIs, and postdoctoral researchers. Read more about 2022 REU , 2023 REU , and 2024 REU programs. ** In addition to undergraduate coursework and the REU Momentum Bootcamp, REU applicants + participants may also take advantage of self-guided resources to supplement their skills and prepare for the REU Program. Such resources include and are not limited to : An Introduction to Earth and Environmental Data Science (Ryan Abernathey, et al.) Intro to Computer Science ‚Äì Python (Khan Academy) Learn Python (CodeAcademy) Python Data Science Handbook (Jake VanderPlas) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/summer-reu-program/#content",
    "transcript": "Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Summer Research Experience for Undergraduates (REU) Overview LEAP‚Äôs summer Research Experience for Undergraduate (REU) program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. Each year, LEAP invites applications from students who will be a rising sophomore, junior, or senior at a college/university in the United States. We invite all students to apply . Participants receive a stipend for the 10-week program. On-campus housing and travel stipends are also available. 2023 Summer REU Cohort, with instructors Tian Zheng, Candace Agonafir, and Yu Huang The REU Program Each summer, LEAP‚Äôs REU Program kicks off with a Momentum Bootcamp , designed to familiarize participants with fundamentals of data science, machine learning, climate modeling, and developing research proposals. ** The Bootcamp is followed by the Summer Research Experience , during which participants apply the skills learned or enhanced in Bootcamp to ongoing LEAP research projects. Students are supervised and mentored by LEAP Senior Personnel and PIs, and postdoctoral researchers. Read more about 2022 REU , 2023 REU , and 2024 REU programs. ** In addition to undergraduate coursework and the REU Momentum Bootcamp, REU applicants + participants may also take advantage of self-guided resources to supplement their skills and prepare for the REU Program. Such resources include and are not limited to : An Introduction to Earth and Environmental Data Science (Ryan Abernathey, et al.) Intro to Computer Science ‚Äì Python (Khan Academy) Learn Python (CodeAcademy) Python Data Science Handbook (Jake VanderPlas) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2025-leap-summer-reu-program-info/",
    "transcript": "2025 LEAP Summer REU Program Info - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2025 LEAP Summer REU Program Overview LEAP‚Äôs 2025 Research Experiences for Undergraduates (REU) Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. This 10-week Program is in partnership with the NSF Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The REU Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. LEAP is committed to building a vibrant research community at the intersection of climate science, data science, and real-world impact. All students will be supported by a team of researchers and graduate student mentors. We want our students to succeed and learn what it takes to make a larger impact on the world via STEM research. Upon completion of the Program, students may have the opportunity to continue their research or present their findings at academic conferences. Interested applicants must review all the details on this page to complete the application (see more application details below). VIEW THE OPEN HOUSE INFO SESSION APPLY BY MARCH 15, 2025 2025 Important Dates February 28 at 1:00pm EST : Summer Programs Open House Info Session March 15 : APPLICATION deadline March 31 : Application decision notification May 26 : Move in to campus housing May 27 ‚Äì August 1 : On-campus Research Experience August 2 : Move out of campus housing Eligibility I have no programming or coding experience. Can I still apply ? YES! All admitted students will receive guidance regarding how to best prepare for the research experience through self-guided learning materials. Keep in mind : the REU experience begins with the Momentum Bootcamp, which will equip students with the skills necessary to complete the research experience. To get the most out of the REU Program, you should have some prior experience with or knowledge of coding, but all students are encouraged to APPLY , regardless of coding and/or programming background. And please note : this is a learning experience and you are not expected to produce a research paper or ‚Äúsolve‚Äù a problem. Research takes time, and this is only one part of the journey! The students who will benefit most from the REU Program are interested in learning about how data science skills can be applied toward climate science research. Are there resources that may help me evaluate my experience and understanding of some basic concepts before I apply? YES! In addition to undergraduate coursework and the Momentum Bootcamp, you may also take advantage of supplemental resources to gain or hone your skills and prepare for the REU Program, such as: An Introduction to Earth and Environmental Data Science (Ryan Abernathey, et al.) Intro to Computer Science ‚Äì Python (Khan Academy) Learn Python (CodeAcademy) Python Data Science Handbook (Jake VanderPlas) Application Who can apply to the 2025 REU Program? We invite applications from students who will be a rising sophomore, junior, or senior in Fall 2025 at a college/university in the United States. (We are only able to offer admission to U.S. citizens and permanent residents.) We invite all students to apply. Former LEAP REU participants are also welcome to apply. What are the application requirements? ONLINE APPLICATION R√©sum√©/CV Academic transcript (unofficial transcripts are acceptable) Statement of interest (500 words about your research goals this summer and your future plans) Letter of recommendation from an academic professor with whom you have studied at your home institution Please be sure to read each project description carefully before submitting the application. How do I apply? And what are the deadlines? The Application is available online. Applications are due by Sunday, March 15, 2025, 11:59 p.m. (EDT). Letters of recommendation should be submitted via email to leap@columbia.edu by Sunday, March 15, 2025, 11:59 p.m. (EDT). We anticipate notifying applicants with a decision at the end of March 2025. Program The 2-week Momentum Bootcamp (May 27 ‚Äì June 6) will be led by: TBD The 8-week Research Experience (June 9 ‚Äì August 1) will be led by the following Project Leads/Mentors: Katie Dagon Da Fan David John Gagne II Pierre Gentine Linnia Hawkins Daniel Kennedy Joseph Ko Kara Lamb Shuolin ‚ÄúShawn‚Äù Li Mike Pritchard Yongquan Qu Ensheng Weng Projects Click the image to learn more about the Summer 2025 REU Research Projects and Project Mentors . Logistics Where will the 2025 REU Program be located? The 2025 REU Program ‚Äî including enrichment and social activities ‚Äî will be held at Columbia University‚Äôs Morningside Heights campus and Manhattanville Campus; some projects may also be based in Boulder, CO. LEAP‚Äôs office is located in the Columbia Innovation Hub ( 2276 12th Avenue, New York, NY 10027 ). Are there activities, workshops, and other events planned for the summer? YES! LEAP will support summer researchers in their professional development goals. The 2025 REU Program will include various workshops on graduate school planning and professional development, research seminars, social events, and more. LEAP will also provide additional learning and enrichment opportunities throughout the summer. I have another summer commitment (e.g., research position or employment); can I still participate in LEAP‚Äôs 2025 REU Program? The 2025 REU Program is a full-time commitment that requires students to participate in research, seminars, and workshops. We strongly recommend that participants plan to keep other weekly commitments to a minimum (less than 15 hours per week). Funding How much is the stipend? The 2025 REU Rrogram offers a competitive stipend of $700 per week for the duration of the program. Is my 2025 REU stipend taxable? 1099 forms will be issued to students. Please visit the 1099 tax withholding page for more information. You should save documentation of the direct deposit of your stipend. Will travel funds be provided? A travel stipend will be available for eligible students. Will housing be provided? Are housing costs covered? Students who require summer housing will be housed on Columbia University‚Äôs Morningside Heights campus for the duration of the program. This will allow students to take advantage of Columbia‚Äôs resources, facilities, and social events throughout the summer. Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2025-leap-summer-reu-program-info/#content",
    "transcript": "2025 LEAP Summer REU Program Info - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2025 LEAP Summer REU Program Overview LEAP‚Äôs 2025 Research Experiences for Undergraduates (REU) Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. This 10-week Program is in partnership with the NSF Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The REU Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. LEAP is committed to building a vibrant research community at the intersection of climate science, data science, and real-world impact. All students will be supported by a team of researchers and graduate student mentors. We want our students to succeed and learn what it takes to make a larger impact on the world via STEM research. Upon completion of the Program, students may have the opportunity to continue their research or present their findings at academic conferences. Interested applicants must review all the details on this page to complete the application (see more application details below). VIEW THE OPEN HOUSE INFO SESSION APPLY BY MARCH 15, 2025 2025 Important Dates February 28 at 1:00pm EST : Summer Programs Open House Info Session March 15 : APPLICATION deadline March 31 : Application decision notification May 26 : Move in to campus housing May 27 ‚Äì August 1 : On-campus Research Experience August 2 : Move out of campus housing Eligibility I have no programming or coding experience. Can I still apply ? YES! All admitted students will receive guidance regarding how to best prepare for the research experience through self-guided learning materials. Keep in mind : the REU experience begins with the Momentum Bootcamp, which will equip students with the skills necessary to complete the research experience. To get the most out of the REU Program, you should have some prior experience with or knowledge of coding, but all students are encouraged to APPLY , regardless of coding and/or programming background. And please note : this is a learning experience and you are not expected to produce a research paper or ‚Äúsolve‚Äù a problem. Research takes time, and this is only one part of the journey! The students who will benefit most from the REU Program are interested in learning about how data science skills can be applied toward climate science research. Are there resources that may help me evaluate my experience and understanding of some basic concepts before I apply? YES! In addition to undergraduate coursework and the Momentum Bootcamp, you may also take advantage of supplemental resources to gain or hone your skills and prepare for the REU Program, such as: An Introduction to Earth and Environmental Data Science (Ryan Abernathey, et al.) Intro to Computer Science ‚Äì Python (Khan Academy) Learn Python (CodeAcademy) Python Data Science Handbook (Jake VanderPlas) Application Who can apply to the 2025 REU Program? We invite applications from students who will be a rising sophomore, junior, or senior in Fall 2025 at a college/university in the United States. (We are only able to offer admission to U.S. citizens and permanent residents.) We invite all students to apply. Former LEAP REU participants are also welcome to apply. What are the application requirements? ONLINE APPLICATION R√©sum√©/CV Academic transcript (unofficial transcripts are acceptable) Statement of interest (500 words about your research goals this summer and your future plans) Letter of recommendation from an academic professor with whom you have studied at your home institution Please be sure to read each project description carefully before submitting the application. How do I apply? And what are the deadlines? The Application is available online. Applications are due by Sunday, March 15, 2025, 11:59 p.m. (EDT). Letters of recommendation should be submitted via email to leap@columbia.edu by Sunday, March 15, 2025, 11:59 p.m. (EDT). We anticipate notifying applicants with a decision at the end of March 2025. Program The 2-week Momentum Bootcamp (May 27 ‚Äì June 6) will be led by: TBD The 8-week Research Experience (June 9 ‚Äì August 1) will be led by the following Project Leads/Mentors: Katie Dagon Da Fan David John Gagne II Pierre Gentine Linnia Hawkins Daniel Kennedy Joseph Ko Kara Lamb Shuolin ‚ÄúShawn‚Äù Li Mike Pritchard Yongquan Qu Ensheng Weng Projects Click the image to learn more about the Summer 2025 REU Research Projects and Project Mentors . Logistics Where will the 2025 REU Program be located? The 2025 REU Program ‚Äî including enrichment and social activities ‚Äî will be held at Columbia University‚Äôs Morningside Heights campus and Manhattanville Campus; some projects may also be based in Boulder, CO. LEAP‚Äôs office is located in the Columbia Innovation Hub ( 2276 12th Avenue, New York, NY 10027 ). Are there activities, workshops, and other events planned for the summer? YES! LEAP will support summer researchers in their professional development goals. The 2025 REU Program will include various workshops on graduate school planning and professional development, research seminars, social events, and more. LEAP will also provide additional learning and enrichment opportunities throughout the summer. I have another summer commitment (e.g., research position or employment); can I still participate in LEAP‚Äôs 2025 REU Program? The 2025 REU Program is a full-time commitment that requires students to participate in research, seminars, and workshops. We strongly recommend that participants plan to keep other weekly commitments to a minimum (less than 15 hours per week). Funding How much is the stipend? The 2025 REU Rrogram offers a competitive stipend of $700 per week for the duration of the program. Is my 2025 REU stipend taxable? 1099 forms will be issued to students. Please visit the 1099 tax withholding page for more information. You should save documentation of the direct deposit of your stipend. Will travel funds be provided? A travel stipend will be available for eligible students. Will housing be provided? Are housing costs covered? Students who require summer housing will be housed on Columbia University‚Äôs Morningside Heights campus for the duration of the program. This will allow students to take advantage of Columbia‚Äôs resources, facilities, and social events throughout the summer. Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2024-leap-summer-reu-program-2/",
    "transcript": "2024 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2024 LEAP Summer REU Program Overview LEAP‚Äôs 2024 Research Experiences for Undergraduates (REU) Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. This 10-week Program is in partnership with the NSF Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The REU Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. LEAP is committed to building a broad research community at the intersection of climate science and data science. We want to apply our research to help stakeholders make informed decisions about climate adaptation. All students are supported by a team of researchers and graduate student mentors. We want our students to succeed and learn what it takes to make a larger impact on the world via STEM research. Upon completion of the Program, students may have the opportunity to continue their research or present their findings at academic conferences. Program The 2-week Momentum Bootcamp (May 28 ‚Äì June 7) will be led by: Juan Nathaniel (PhD student, Dept. of Earth + Environmental Engineering, Columbia University; Gentine Lab, Columbia Engineering) Candace Agonafir (Postdoc, Depts. of Data Science + Civil Engineering, Columbia University) The 8-week Research Experience (June 10 ‚Äì August 2) will be led by the following Project Leads/Mentors: Dhruv Balwada Greg Elsaesser Joseph Ko Kara Lamb Jerry Lin Stephan Mandt Brian Medeiros Mike Pritchard Yongquan Qu Sara Shamekh Eliot Wong-Toi Qingyuan Yang 2024 REU Participants OMAR ABDEL AZIM Project 1: Quantifying Epistemic and Aleatoric Uncertainty in Climate Data (Mandt) Omar Abdel Azim is a computer science student at the Clemson University Honors College. He is deeply interested in studying how to increase reliability of machine learning technologies for application in large-scale, data intensive, societal challenges. One of Omar‚Äôs proudest achievements in publishing a first author paper in the IEEE journal in addition to an oral conference in the IEEE International Automated Vehicle Validation Conference 2023. He is currently conducting research within the fields of Materials Science, specifically using Machine Learning for prediction of properties in high entropy alloys and developing a database for materials science researchers to use. Additionally, he is a member of Clemson University‚Äôs Design, Build, Fly Club and Rocket Engineering Club. He plans to attain a Ph.D. in Computer Science with an emphasis on optimizing machine learning algorithms. JACK ALFANDRE Project 5: Understanding and Modeling the Impact of Air-Sea Heterogeneity on Surface Fluxes (Balwada) My research interests include quantum physics, quantum computing, artificial intelligence, and data science. One of my professional goals is to be a tech entrepreneur specializing in quantum computing and using quantum computers for data analysis and high complexity problem solving. Another professional goal of mine is to become a neurosurgeon (I know, they‚Äôre pretty unrelated and I‚Äôll have to pick one which is sad because I would love to do both). I unfortunately cannot think of my proudest personal achievement, but if I had to pick something it would be winning an award for outstanding physics student at my high school graduation. PATRICK DICUS Project 4: Understanding Ice Crystal Growth and Evolution in the Atmosphere (Lamb) Patrick Dicus is very interested in the intersection between computer science and climate science and hopes to be able to gain skills relating to both which could be used to further his career in these subjects. Patrick is aiming to attend grad school within some field of geosciences, but he is unsure which one to choose just yet. Beyond academics, Patrick enjoys training for bike races he competes in during the summer months, and attending live DJ sets. Patrick is most proud of his qualification for the Collegiate Road Cycling National Championships for two years in a row and being able to represent Hamilton College Cycling at the national level. ANTHONY GUZM√ÅN Project 3: Analysis of Climate Model Perturbed Physics Ensembles (Elsaesser, Medeiros) Anthony is a rising senior double-majoring in Earth Sciences and Data Sciences at the University of Southern California. He is known within his local community as one who encourages to question pre-existing beliefs through eloquent writing. His hobbies include playing the mellophone in the marching band, baton twirling, and reading novels. One of his favorite memories is performing with Gwen Stefani at the Hollywood Bowl. NICOLINE JOENSON Project 3: Analysis of Climate Model Perturbed Physics Ensembles (Elsaesser, Medeiros) Nicoline is a rising Junior in Columbia College, majoring in physics. She is currently the Outreach Head of Columbia‚Äôs Physics Club SPECTRA. This year together with the SPECTRA board and volunteers, she will be presenting different experiments to local middle schoolers on Columbia‚Äôs campus. Nicoline‚Äôs primary research interest is climate modeling and geophysics. She hopes to be able to apply this knowledge in future climate science research or in environmental law. Before coming to Columbia, Nicoline had two gap years where she volunteered, travelled, worked and studied. In her personal time, Nicoline loves to travel and discover new cultures. Her proudest personal achievement is having hiked to the Tunari peak in Bolivia at 16.519 ft. TONY MANRIQUEZ Project 5: Understanding and Modeling the Impact of Air-Sea Heterogeneity on Surface Fluxes (Balwada) Tony Manriquez is an undergraduate student in the Meteorology program at MSU Denver. He is interested in bridging the gap between weather and climate, particularly in subseasonal-to-seasonal (S2S) research. While working on a machine learning project with his NSF Significant Opportunities for Atmospheric Research and Science (SOARS) research mentors on S2S at the National Center for Atmospheric Research (NCAR), he found machine learning applications extremely interesting, with many research applications. After finishing his undergraduate degree from MSU Denver, he plans on attending graduate school in Atmospheric Science. In his spare time, he enjoys storm chasing and aurora chasing! He previously worked as a firefighter for his hometown after earning his certification. He plans to earn certifications in fire, water, earth, and air in the future. MIA MONTROSE Project 1: Quantifying Epistemic and Aleatoric Uncertainty in Climate Data (Mandt) Mia is a rising Junior at Harvard College studying Environmental Science and Engineering. Her research interests include climate-focused data science, sustainable technology, and sustainable architecture. Last summer, she traveled to S√£o Paulo, Brasil with Harvard‚Äôs David Rockefeller Center for Latin American Studies and interned with Eureciclo, a company that stimulates Brasil‚Äôs recycling chain by providing reverse logistics solutions for packaging waste. In addition to gaining exposure and experience in ML and data analysis, she immersed herself in Brasil‚Äôs culture and language which proved to be the most rewarding aspect of the experience! ASHLEY NGUYEN Project 4: Understanding Ice Crystal Growth and Evolution in the Atmosphere (Lamb) Ashley Nguyen, an undergraduate student at UC Berkeley majoring in Molecular Environmental Biology and Data Science, possesses a deep-seated passion for exploring environmental issues through a range of STEM disciplines including data science, synthetic biology, green chemistry, engineering, and sustainable entrepreneurship. As an aspiring climate scientist, she aims to integrate these broad interests to develop innovative solutions that address urgent long-term environmental concerns. Ashley plans to further her education through graduate school, where she intends to continue her research contributions to the climate science community. Notable among her achievements is leading her team to nationals in a research challenge, where she advocated for a pioneering bee probiotic aimed at addressing agricultural toxicity and bee mortality. LAURA PONG Project 2: Understanding and Modeling Turbulent Flow in the Atmosphere (Shamekh) Laura Pong is an undergraduate studying Atmospheric Science and Data Science at the University of Washington. She is currently studying the impact of extreme heat on plants in the Northwest of the United States and Canada in the Ecoclimate Lab and Turner Lab, and is broadly interested in land-atmospheric interactions. Laura is hoping to take the skills she learns from research and channel them into answering climate questions relevant to her communities in her home of New York City. GRETA VANZETTEN Project 2: Understanding and Modeling Turbulent Flow in the Atmosphere (Shamekh) Greta VanZetten is a rising junior at Columbia College studying both Data Science and Women & Gender Studies. From growing up along the shores of Lake Michigan to running through the parks of New York City, she frequently marvels at our dynamic world. At Columbia, she synthesizes statistics with narrative as she leads campus tours and works as a Statistics tutor. Through her interests in the carbon cycle/sequestration, atmospheric modeling, and machine learning, Greta has goals of influencing policy decisions as a scientist, utilizing climate models to analyze and interpret data, and providing insight into how we can alter our intersecting trajectories. In high school, she was a Michigan All-State swimmer who received the 2022 MHSAA Scholar-Athlete award given to the top 16 female varsity athletes in the state. Since then, Greta has continued her athletic endeavors as an Olympic-distance triathlete and water polo player at Columbia. Projects Click the image below to learn more about the Summer 2024 REU Research Projects and Project Mentors. Important Dates March 1 : Virtual info session ( View Here ) March 10 : APPLICATION deadline March 31 : Application decision notification May 26 : Move in to campus housing May 28 ‚Äì August 2 : On-campus Research Experience August 3 : Move out of campus housing Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2024-leap-summer-reu-program-2/#content",
    "transcript": "2024 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2024 LEAP Summer REU Program Overview LEAP‚Äôs 2024 Research Experiences for Undergraduates (REU) Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. This 10-week Program is in partnership with the NSF Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The REU Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. LEAP is committed to building a broad research community at the intersection of climate science and data science. We want to apply our research to help stakeholders make informed decisions about climate adaptation. All students are supported by a team of researchers and graduate student mentors. We want our students to succeed and learn what it takes to make a larger impact on the world via STEM research. Upon completion of the Program, students may have the opportunity to continue their research or present their findings at academic conferences. Program The 2-week Momentum Bootcamp (May 28 ‚Äì June 7) will be led by: Juan Nathaniel (PhD student, Dept. of Earth + Environmental Engineering, Columbia University; Gentine Lab, Columbia Engineering) Candace Agonafir (Postdoc, Depts. of Data Science + Civil Engineering, Columbia University) The 8-week Research Experience (June 10 ‚Äì August 2) will be led by the following Project Leads/Mentors: Dhruv Balwada Greg Elsaesser Joseph Ko Kara Lamb Jerry Lin Stephan Mandt Brian Medeiros Mike Pritchard Yongquan Qu Sara Shamekh Eliot Wong-Toi Qingyuan Yang 2024 REU Participants OMAR ABDEL AZIM Project 1: Quantifying Epistemic and Aleatoric Uncertainty in Climate Data (Mandt) Omar Abdel Azim is a computer science student at the Clemson University Honors College. He is deeply interested in studying how to increase reliability of machine learning technologies for application in large-scale, data intensive, societal challenges. One of Omar‚Äôs proudest achievements in publishing a first author paper in the IEEE journal in addition to an oral conference in the IEEE International Automated Vehicle Validation Conference 2023. He is currently conducting research within the fields of Materials Science, specifically using Machine Learning for prediction of properties in high entropy alloys and developing a database for materials science researchers to use. Additionally, he is a member of Clemson University‚Äôs Design, Build, Fly Club and Rocket Engineering Club. He plans to attain a Ph.D. in Computer Science with an emphasis on optimizing machine learning algorithms. JACK ALFANDRE Project 5: Understanding and Modeling the Impact of Air-Sea Heterogeneity on Surface Fluxes (Balwada) My research interests include quantum physics, quantum computing, artificial intelligence, and data science. One of my professional goals is to be a tech entrepreneur specializing in quantum computing and using quantum computers for data analysis and high complexity problem solving. Another professional goal of mine is to become a neurosurgeon (I know, they‚Äôre pretty unrelated and I‚Äôll have to pick one which is sad because I would love to do both). I unfortunately cannot think of my proudest personal achievement, but if I had to pick something it would be winning an award for outstanding physics student at my high school graduation. PATRICK DICUS Project 4: Understanding Ice Crystal Growth and Evolution in the Atmosphere (Lamb) Patrick Dicus is very interested in the intersection between computer science and climate science and hopes to be able to gain skills relating to both which could be used to further his career in these subjects. Patrick is aiming to attend grad school within some field of geosciences, but he is unsure which one to choose just yet. Beyond academics, Patrick enjoys training for bike races he competes in during the summer months, and attending live DJ sets. Patrick is most proud of his qualification for the Collegiate Road Cycling National Championships for two years in a row and being able to represent Hamilton College Cycling at the national level. ANTHONY GUZM√ÅN Project 3: Analysis of Climate Model Perturbed Physics Ensembles (Elsaesser, Medeiros) Anthony is a rising senior double-majoring in Earth Sciences and Data Sciences at the University of Southern California. He is known within his local community as one who encourages to question pre-existing beliefs through eloquent writing. His hobbies include playing the mellophone in the marching band, baton twirling, and reading novels. One of his favorite memories is performing with Gwen Stefani at the Hollywood Bowl. NICOLINE JOENSON Project 3: Analysis of Climate Model Perturbed Physics Ensembles (Elsaesser, Medeiros) Nicoline is a rising Junior in Columbia College, majoring in physics. She is currently the Outreach Head of Columbia‚Äôs Physics Club SPECTRA. This year together with the SPECTRA board and volunteers, she will be presenting different experiments to local middle schoolers on Columbia‚Äôs campus. Nicoline‚Äôs primary research interest is climate modeling and geophysics. She hopes to be able to apply this knowledge in future climate science research or in environmental law. Before coming to Columbia, Nicoline had two gap years where she volunteered, travelled, worked and studied. In her personal time, Nicoline loves to travel and discover new cultures. Her proudest personal achievement is having hiked to the Tunari peak in Bolivia at 16.519 ft. TONY MANRIQUEZ Project 5: Understanding and Modeling the Impact of Air-Sea Heterogeneity on Surface Fluxes (Balwada) Tony Manriquez is an undergraduate student in the Meteorology program at MSU Denver. He is interested in bridging the gap between weather and climate, particularly in subseasonal-to-seasonal (S2S) research. While working on a machine learning project with his NSF Significant Opportunities for Atmospheric Research and Science (SOARS) research mentors on S2S at the National Center for Atmospheric Research (NCAR), he found machine learning applications extremely interesting, with many research applications. After finishing his undergraduate degree from MSU Denver, he plans on attending graduate school in Atmospheric Science. In his spare time, he enjoys storm chasing and aurora chasing! He previously worked as a firefighter for his hometown after earning his certification. He plans to earn certifications in fire, water, earth, and air in the future. MIA MONTROSE Project 1: Quantifying Epistemic and Aleatoric Uncertainty in Climate Data (Mandt) Mia is a rising Junior at Harvard College studying Environmental Science and Engineering. Her research interests include climate-focused data science, sustainable technology, and sustainable architecture. Last summer, she traveled to S√£o Paulo, Brasil with Harvard‚Äôs David Rockefeller Center for Latin American Studies and interned with Eureciclo, a company that stimulates Brasil‚Äôs recycling chain by providing reverse logistics solutions for packaging waste. In addition to gaining exposure and experience in ML and data analysis, she immersed herself in Brasil‚Äôs culture and language which proved to be the most rewarding aspect of the experience! ASHLEY NGUYEN Project 4: Understanding Ice Crystal Growth and Evolution in the Atmosphere (Lamb) Ashley Nguyen, an undergraduate student at UC Berkeley majoring in Molecular Environmental Biology and Data Science, possesses a deep-seated passion for exploring environmental issues through a range of STEM disciplines including data science, synthetic biology, green chemistry, engineering, and sustainable entrepreneurship. As an aspiring climate scientist, she aims to integrate these broad interests to develop innovative solutions that address urgent long-term environmental concerns. Ashley plans to further her education through graduate school, where she intends to continue her research contributions to the climate science community. Notable among her achievements is leading her team to nationals in a research challenge, where she advocated for a pioneering bee probiotic aimed at addressing agricultural toxicity and bee mortality. LAURA PONG Project 2: Understanding and Modeling Turbulent Flow in the Atmosphere (Shamekh) Laura Pong is an undergraduate studying Atmospheric Science and Data Science at the University of Washington. She is currently studying the impact of extreme heat on plants in the Northwest of the United States and Canada in the Ecoclimate Lab and Turner Lab, and is broadly interested in land-atmospheric interactions. Laura is hoping to take the skills she learns from research and channel them into answering climate questions relevant to her communities in her home of New York City. GRETA VANZETTEN Project 2: Understanding and Modeling Turbulent Flow in the Atmosphere (Shamekh) Greta VanZetten is a rising junior at Columbia College studying both Data Science and Women & Gender Studies. From growing up along the shores of Lake Michigan to running through the parks of New York City, she frequently marvels at our dynamic world. At Columbia, she synthesizes statistics with narrative as she leads campus tours and works as a Statistics tutor. Through her interests in the carbon cycle/sequestration, atmospheric modeling, and machine learning, Greta has goals of influencing policy decisions as a scientist, utilizing climate models to analyze and interpret data, and providing insight into how we can alter our intersecting trajectories. In high school, she was a Michigan All-State swimmer who received the 2022 MHSAA Scholar-Athlete award given to the top 16 female varsity athletes in the state. Since then, Greta has continued her athletic endeavors as an Olympic-distance triathlete and water polo player at Columbia. Projects Click the image below to learn more about the Summer 2024 REU Research Projects and Project Mentors. Important Dates March 1 : Virtual info session ( View Here ) March 10 : APPLICATION deadline March 31 : Application decision notification May 26 : Move in to campus housing May 28 ‚Äì August 2 : On-campus Research Experience August 3 : Move out of campus housing Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2023-leap-summer-reu-program-2/",
    "transcript": "2023 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 LEAP Summer REU Program Overview LEAP‚Äôs 2023 REU program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with¬†the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. We invite all students to apply. 2023 Summer REU Cohort, with instructors Tian Zheng, Candace Agonafir, and Yu Huang Program The 3-week Momentum Bootcamp (online, June 5‚Äì23, 2023) will be led by: Tian Zheng (Chief Convergence Officer & Education Director, LEAP; Professor & Chair of the Department of Statistics, Columbia University; Affiliate Member of the Data Science Institute) Candace Agonafir (Postdoc, Depts. of Data Science + Civil Engineering, Columbia University) Yu Huang (PhD Student, Dept. of Earth + Environmental Engineering, Columbia University) The Summer 2023 Research Experience (in-person, June 25 ‚Äì July 29, 2023) will be led by: Pierre Gentine (Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University) Mike Pritchard (Institutional Integration Director, LEAP; Associate Professor of Earth Systems Science, University of California at Irvine) Stephan Mandt (Associate Professor of Computer Science and Statistics, University of California at Irvine). 2023 REU Participants Sammy Agrawal (Massachusetts) Computer Science, Columbia University Thomas Chen (New Jersey) Computer Science, Columbia University Mark Irby-Gill (New York) Geological Engineering, Red Rocks Community College Rebecca Porter (Kansas) Interdisciplinary Studies, University of Saint Mary Amanda Sun (California) Computer Science + Environmental Studies, Dartmouth College Subashree Venkatasubramanian (Washington) Computer Science, Columbia University Project Scope Topic : Machine learning (ML) to break deadlock in achieving climate simulations with high-resolution physics. Short Description: Climate model predictions are full of uncertainty. Some of it is because processes like cloud formation are crudely represented simply because they are too computationally intensive to model explicitly. Multi-scale climate models that embed small realizations of these explicit physics provide an opportunity to sidestep Moore‚Äôs law, by learning these physics with machine learning models. Once trained, the machine learning models can be coupled to the climate predictions, with fast inference allowing high-resolution physics in climate simulations ahead of schedule. An open research challenge is finding reproducible, reliable ways to achieve performant ML workflows in situations of real-world operational complexity. In this context, LEAP has innovated a ML training data set harvested from a state-of-the-art climate simulator, which exposes REU students to the relevant pipeline issues in a real-world research setting at the frontier of climate science and data science. Learning Outcomes: Familiarity with datasets used in modern climate simulations, baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Potential Continuation of Research After REU: Successful or standout fits could be considered for follow-on work that goes to the next step of coupling well-performing architectures to global climate simulators. The resulting ‚Äúhybrid‚Äù (ML + physics) climate simulators may exhibit interesting pathologies or physically desirable behaviors, either of which provide¬† opportunities for collaborative analysis of climate prediction output. Important Dates March 10, 2023: REU Application deadline March 31, 2023: REU Application decision notification June 5 ‚Äì June 23, 2023: Momentum Bootcamp (online) June 25, 2023: Move in to Columbia University campus June 26 ‚Äì July 28, 2023: In-Person Research Experience July 29, 2023: Move out Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2023-leap-summer-reu-program-2/#content",
    "transcript": "2023 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 LEAP Summer REU Program Overview LEAP‚Äôs 2023 REU program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with¬†the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. We invite all students to apply. 2023 Summer REU Cohort, with instructors Tian Zheng, Candace Agonafir, and Yu Huang Program The 3-week Momentum Bootcamp (online, June 5‚Äì23, 2023) will be led by: Tian Zheng (Chief Convergence Officer & Education Director, LEAP; Professor & Chair of the Department of Statistics, Columbia University; Affiliate Member of the Data Science Institute) Candace Agonafir (Postdoc, Depts. of Data Science + Civil Engineering, Columbia University) Yu Huang (PhD Student, Dept. of Earth + Environmental Engineering, Columbia University) The Summer 2023 Research Experience (in-person, June 25 ‚Äì July 29, 2023) will be led by: Pierre Gentine (Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University) Mike Pritchard (Institutional Integration Director, LEAP; Associate Professor of Earth Systems Science, University of California at Irvine) Stephan Mandt (Associate Professor of Computer Science and Statistics, University of California at Irvine). 2023 REU Participants Sammy Agrawal (Massachusetts) Computer Science, Columbia University Thomas Chen (New Jersey) Computer Science, Columbia University Mark Irby-Gill (New York) Geological Engineering, Red Rocks Community College Rebecca Porter (Kansas) Interdisciplinary Studies, University of Saint Mary Amanda Sun (California) Computer Science + Environmental Studies, Dartmouth College Subashree Venkatasubramanian (Washington) Computer Science, Columbia University Project Scope Topic : Machine learning (ML) to break deadlock in achieving climate simulations with high-resolution physics. Short Description: Climate model predictions are full of uncertainty. Some of it is because processes like cloud formation are crudely represented simply because they are too computationally intensive to model explicitly. Multi-scale climate models that embed small realizations of these explicit physics provide an opportunity to sidestep Moore‚Äôs law, by learning these physics with machine learning models. Once trained, the machine learning models can be coupled to the climate predictions, with fast inference allowing high-resolution physics in climate simulations ahead of schedule. An open research challenge is finding reproducible, reliable ways to achieve performant ML workflows in situations of real-world operational complexity. In this context, LEAP has innovated a ML training data set harvested from a state-of-the-art climate simulator, which exposes REU students to the relevant pipeline issues in a real-world research setting at the frontier of climate science and data science. Learning Outcomes: Familiarity with datasets used in modern climate simulations, baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Potential Continuation of Research After REU: Successful or standout fits could be considered for follow-on work that goes to the next step of coupling well-performing architectures to global climate simulators. The resulting ‚Äúhybrid‚Äù (ML + physics) climate simulators may exhibit interesting pathologies or physically desirable behaviors, either of which provide¬† opportunities for collaborative analysis of climate prediction output. Important Dates March 10, 2023: REU Application deadline March 31, 2023: REU Application decision notification June 5 ‚Äì June 23, 2023: Momentum Bootcamp (online) June 25, 2023: Move in to Columbia University campus June 26 ‚Äì July 28, 2023: In-Person Research Experience July 29, 2023: Move out Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2022-leap-summer-reu-program-2/",
    "transcript": "2022 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2022 LEAP Summer REU Program Overview LEAP‚Äôs 2022 REU program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. We invite all students to apply. 2022 Summer REU Cohort, with instructor Tian Zheng 2022 REU Participants Kayla Ansari (North Carolina) Computer Science, Barnard College Fatoumata Camara (New York) Computer Science, Barnard College Camden Fessler (New Jersey) Computer Science, Univ. of Minnesota - Twin Cities Charles Jan (New York) Operations Research, Columbia University Ryan Anselm (Texas) Computer Science, Columbia University Claire Zhang (Washington) Computer Science, Columbia University REU Research Projects Click the image below to learn more about the Summer 2022 REU Research Projects. Important Dates February 17 : Application opens February 24 : Info Session March 3 : Application deadline March 24 : Application decision notification March 24 ‚Äì March 31 : Admitted students matched with a project June 6, 2022 ‚Äì July 29 : Summer REU program (dates may vary for different projects) Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2022-leap-summer-reu-program-2/#content",
    "transcript": "2022 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2022 LEAP Summer REU Program Overview LEAP‚Äôs 2022 REU program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. We invite all students to apply. 2022 Summer REU Cohort, with instructor Tian Zheng 2022 REU Participants Kayla Ansari (North Carolina) Computer Science, Barnard College Fatoumata Camara (New York) Computer Science, Barnard College Camden Fessler (New Jersey) Computer Science, Univ. of Minnesota - Twin Cities Charles Jan (New York) Operations Research, Columbia University Ryan Anselm (Texas) Computer Science, Columbia University Claire Zhang (Washington) Computer Science, Columbia University REU Research Projects Click the image below to learn more about the Summer 2022 REU Research Projects. Important Dates February 17 : Application opens February 24 : Info Session March 3 : Application deadline March 24 : Application decision notification March 24 ‚Äì March 31 : Admitted students matched with a project June 6, 2022 ‚Äì July 29 : Summer REU program (dates may vary for different projects) Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/momentum-fellowship/",
    "transcript": "Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Summer Momentum Fellowship Overview Every summer, LEAP invites applications from doctoral students in data science (Computer Science, Statistics, Operations Research, and related fields) to participate in the LEAP Summer Momentum Fellowship . LEAP Summer Momentum Fellows are U.S. doctoral students (enrolled in a U.S.-based data science Ph.D. program) in data science interested in a summer research immersion in climate data science, applying their data science/machine learning skills in climate modeling and developing research interests in climate data science. We invite all students to apply. Participants receive a summer stipend, travel support, and access to LEAP resources + workspace. The Momentum Fellowship Program Momentum Fellows work closely with Project Leads throughout the summer on a well-defined, yet open-ended, machine learning research problem in climate data science. Project Leads guide their Fellows to present their summer research at future LEAP events and other workshops/conferences. Fellows are also responsible for mentoring up to two (2) undergraduate students in LEAP‚Äôs Research Experiences for Undergraduates (REU) program . Read more about the 2023 Summer Momentum Fellowship and 2024 Summer Momentum Fellowship programs. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/momentum-fellowship/#content",
    "transcript": "Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Summer Momentum Fellowship Overview Every summer, LEAP invites applications from doctoral students in data science (Computer Science, Statistics, Operations Research, and related fields) to participate in the LEAP Summer Momentum Fellowship . LEAP Summer Momentum Fellows are U.S. doctoral students (enrolled in a U.S.-based data science Ph.D. program) in data science interested in a summer research immersion in climate data science, applying their data science/machine learning skills in climate modeling and developing research interests in climate data science. We invite all students to apply. Participants receive a summer stipend, travel support, and access to LEAP resources + workspace. The Momentum Fellowship Program Momentum Fellows work closely with Project Leads throughout the summer on a well-defined, yet open-ended, machine learning research problem in climate data science. Project Leads guide their Fellows to present their summer research at future LEAP events and other workshops/conferences. Fellows are also responsible for mentoring up to two (2) undergraduate students in LEAP‚Äôs Research Experiences for Undergraduates (REU) program . Read more about the 2023 Summer Momentum Fellowship and 2024 Summer Momentum Fellowship programs. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2025-leap-summer-momentum-fellowship-info/",
    "transcript": "2025 LEAP Summer Momentum Fellowship Info - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2025 LEAP Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling and develop research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship project leads work closely with Fellows throughout the summer on a well-defined, yet open-ended, machine learning research problem in climate data science. Project leads will also guide their Fellows to present their summer research at future LEAP events and other workshops/conferences. Momentum Fellows are responsible for mentoring up to two (2) undergraduate students in the LEAP Research Experiences for Undergraduates (REU) program. Each project will host up to two (2) undergraduate students paired with one (1) Fellow. 2025 Important Dates February 28 at 1:00pm EST : Summer Programs Open House Info Session March 15 : APPLICATION deadline March 31 : Application decision notification May 27 ‚Äì August 1 : On-Campus Research VIEW THE OPEN HOUSE INFO SESSION APPLY BY MARCH 15, 2025 Eligibility LEAP invites applications from doctoral students in data science* (Computer Science, Statistics, Operations Research, and related fields) interested in a summer research immersion in climate data science, applying their data science/machine learning skills in climate modeling, and developing research interests in climate data science. We invite all candidates to apply. Fellows are expected to work for 10 weeks (May 27 ‚Äì August 1, 2025) and 25 hours on average each week. Each Fellow will be awarded an $8,000 summer stipend to support their research and professional growth, receive travel support, and have access to LEAP resources. Additional housing support may be available. To apply, please complete the below APPLICATION by 11:59 p.m. (EDT) on Sunday, March 15, 2025. *Applicants must be enrolled in a data science Ph.D. program in the United States to be eligible. If you are enrolled at an institution outside of the U.S., and are interested in applying, please reach out to LEAP . Application How do I apply? And what are the deadlines? The APPLICATION is available online. Applications are due by March 15, 2025, 11:59 p.m. (EDT). Letters of recommendation should be submitted via email to leap@columbia.edu by March 15, 2025, 11:59 p.m. (EDT). If selected, when will I be notified? Select candidates will be notified to schedule an interview with relevant project leads before an official offer is extended by the end of March 2025. Projects Click the image to learn more about the Summer 2025 Research Projects and Project Mentors . Program The 10-week Research Experience (June 9 ‚Äì August 1) will be led by the following Project Leads/Mentors: Katie Dagon Da Fan David John Gagne II Pierre Gentine Linnia Hawkins Daniel Kennedy Joseph Ko Kara Lamb Shuolin ‚ÄúShawn‚Äù Li Mike Pritchard Yongquan Qu Ensheng Weng Logistics Where will the 2025 Momentum Fellowship be located? The 2025 Momentum Fellowship will be held at Columbia University‚Äôs Morningside Heights campus and Manhattanville Campus; some projects may be based in Boulder, CO. LEAP‚Äôs office is located in the Columbia Innovation Hub ( 2276 12th Avenue, New York, NY 10027 ). Are there activities, workshops, and other events planned for the summer? YES! LEAP will support summer researchers in their professional development goals. The 2025 summer program will connect students with professional development skills, research seminars, social events, and more. LEAP will also provide additional learning and enrichment opportunities throughout the summer. Funding How much is the stipend? The program offers a competitive stipend of $8,000 for the duration of the program. Those enrolled in a Columbia University program will receive a salary equivalent to $8,000. Is my Momentum Fellowship stipend taxable? 1099 forms will be issued to students. Please visit the 1099 tax withholding page for more information. You should save documentation of the direct deposit of your stipend. Will travel be provided? A travel stipend will be available for travel related to the project. Will housing be provided? Are housing costs covered? Housing support may be available. Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2025-leap-summer-momentum-fellowship-info/#content",
    "transcript": "2025 LEAP Summer Momentum Fellowship Info - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2025 LEAP Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling and develop research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship project leads work closely with Fellows throughout the summer on a well-defined, yet open-ended, machine learning research problem in climate data science. Project leads will also guide their Fellows to present their summer research at future LEAP events and other workshops/conferences. Momentum Fellows are responsible for mentoring up to two (2) undergraduate students in the LEAP Research Experiences for Undergraduates (REU) program. Each project will host up to two (2) undergraduate students paired with one (1) Fellow. 2025 Important Dates February 28 at 1:00pm EST : Summer Programs Open House Info Session March 15 : APPLICATION deadline March 31 : Application decision notification May 27 ‚Äì August 1 : On-Campus Research VIEW THE OPEN HOUSE INFO SESSION APPLY BY MARCH 15, 2025 Eligibility LEAP invites applications from doctoral students in data science* (Computer Science, Statistics, Operations Research, and related fields) interested in a summer research immersion in climate data science, applying their data science/machine learning skills in climate modeling, and developing research interests in climate data science. We invite all candidates to apply. Fellows are expected to work for 10 weeks (May 27 ‚Äì August 1, 2025) and 25 hours on average each week. Each Fellow will be awarded an $8,000 summer stipend to support their research and professional growth, receive travel support, and have access to LEAP resources. Additional housing support may be available. To apply, please complete the below APPLICATION by 11:59 p.m. (EDT) on Sunday, March 15, 2025. *Applicants must be enrolled in a data science Ph.D. program in the United States to be eligible. If you are enrolled at an institution outside of the U.S., and are interested in applying, please reach out to LEAP . Application How do I apply? And what are the deadlines? The APPLICATION is available online. Applications are due by March 15, 2025, 11:59 p.m. (EDT). Letters of recommendation should be submitted via email to leap@columbia.edu by March 15, 2025, 11:59 p.m. (EDT). If selected, when will I be notified? Select candidates will be notified to schedule an interview with relevant project leads before an official offer is extended by the end of March 2025. Projects Click the image to learn more about the Summer 2025 Research Projects and Project Mentors . Program The 10-week Research Experience (June 9 ‚Äì August 1) will be led by the following Project Leads/Mentors: Katie Dagon Da Fan David John Gagne II Pierre Gentine Linnia Hawkins Daniel Kennedy Joseph Ko Kara Lamb Shuolin ‚ÄúShawn‚Äù Li Mike Pritchard Yongquan Qu Ensheng Weng Logistics Where will the 2025 Momentum Fellowship be located? The 2025 Momentum Fellowship will be held at Columbia University‚Äôs Morningside Heights campus and Manhattanville Campus; some projects may be based in Boulder, CO. LEAP‚Äôs office is located in the Columbia Innovation Hub ( 2276 12th Avenue, New York, NY 10027 ). Are there activities, workshops, and other events planned for the summer? YES! LEAP will support summer researchers in their professional development goals. The 2025 summer program will connect students with professional development skills, research seminars, social events, and more. LEAP will also provide additional learning and enrichment opportunities throughout the summer. Funding How much is the stipend? The program offers a competitive stipend of $8,000 for the duration of the program. Those enrolled in a Columbia University program will receive a salary equivalent to $8,000. Is my Momentum Fellowship stipend taxable? 1099 forms will be issued to students. Please visit the 1099 tax withholding page for more information. You should save documentation of the direct deposit of your stipend. Will travel be provided? A travel stipend will be available for travel related to the project. Will housing be provided? Are housing costs covered? Housing support may be available. Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2024-leap-summer-momentum-fellowship/",
    "transcript": "2024 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2024 LEAP Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling and develop research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship project leads work closely with Fellows throughout the summer on a well-defined, yet open-ended, machine learning research problem in climate data science. Project leads will also guide their Fellows to present their summer research at future LEAP events and other workshops/conferences. Momentum Fellows are responsible for mentoring up to two (2) undergraduate students in the LEAP Research Experiences for Undergraduates (REU) program. Each project will host up to two (2) undergraduate students paired with one (1) Fellow. 2024 Summer Momentum Fellows KYLE McEVOY Project 3: Analysis of Climate Model Perturbed Physics Ensembles (Elsaesser, Medeiros) Kyle is a third year PhD student in Statistics in the Department of Statistics & Data Science at UCLA, advised by Karen McKinnon. His research interests include multiple hypothesis testing with spatially correlated data, covariance matrix estimation, distributional shifts in precipitation under climate change, and incorporating distributional shifts into simulated precipitation. After his PhD, Kyle hopes to work as a research scientist, investigating the climate system and exploring the impacts of climate change. In his free time, Kyle loves hiking, camping, and exploring the outdoors, especially in the many beautiful places located across California (though the redwood forests and the Sierra Nevada mountains are his favorites). PRANI NALLURI Project 5: Understanding and Modeling the Impact of Air-Sea Heterogeneity on Surface Fluxes (Balwada) Prani Nalluri (they/them)is a PhD student on the Geophysical Fluid Dynamics track in the Applied Physics and Applied Mathematics department at Columbia University. They are interested in improving parameterizations ofturbulent processes at submeso- and meso-scales in the ocean. This summer, Prani will be working with Dhruv Balwada to develop ML-based parameterizations for subgrid-scale air-sea fluxes in coupled atmosphere-ocean models. In the future, Prani hopes to continue their research in a national lab or as a part of a climate-focused start-up. Outside of science, they spend their time distance running and creating art through sewing and sculpture. JIMENG SHI Project 1: Quantifying Epistemic and Aleatoric Uncertainty in Climate Data (Mandt) Jimeng is pursuing his Ph.D. in computer science at the Knight Foundation School of Computing and Information Sciences, Florida International University. His research interests primarily focus on employing AI methods to model large-scale complex systems in real-world scenarios. He has proposed a groundbreaking ML framework named FIDLAr, designed for flood prediction and mitigation. FIDLAr has demonstrated notable efficacy, outperforming traditional physics-based models in accuracy and efficiency. Expanding its utility, Jimeng is now customizing FIDLAr for more optimization and data generation tasks across diverse domains. Furthermore, he is actively engaged in building AI foundation models within the weather and climate science. ANTONY SIKORSKI Project 2: Understanding and Modeling Turbulent Flow in the Atmosphere (Shamekh) Antony is a Statistics PhD candidate at the Colorado School of Mines. He also completed his MS in Data Science at Mines, and a BS in Applied Mathematics at the University of California, San Diego. His research interests primarily lie in merging machine learning and computational statistics to develop predictive methods for large spatial data volumes. Following the completion of his PhD, Antony aims to perform research as a data scientistin industry. A proud personal achievement of his is recently being awarded the NSF GRFP. Outside of work, Antony enjoys skiing, live music, and traveling! OBIN STURM Project 4: Understanding Ice Crystal Growth and Evolution in the Atmosphere (Lamb) Obin Sturm is a PhD student in the Atmospheric Composition and Earth Data Science group at the University of Southern California. His research focuses on improving our understanding and predictive power of atmospheric chemistry and physics through computational models. These models encompass realistic process-based representations of fundamental phenomena, 3D Earth system models exploring the interactions¬†of these processes at multiple scales, and modern AI and data-driven techniques bridging the first two approaches. Obin¬†believes the use of all three can lead to new scientific discoveries, and better inform environmental policies and decision making important for the well-being of both specific communities and our broader, global society. Obin‚Äôs recent contributions include data-driven, scientifically consistent aerosol superspecies to transport realistic volatility distributions in air quality forecasts, and creating an interface to extract detailed chemical states from NASA GEOS atmospheric composition forecasts for comprehensive process-based analysis. Projects Click the image below to learn more about the Summer 2024 Momentum Fellowship Research Projects. Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2024-leap-summer-momentum-fellowship/#content",
    "transcript": "2024 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2024 LEAP Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling and develop research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship project leads work closely with Fellows throughout the summer on a well-defined, yet open-ended, machine learning research problem in climate data science. Project leads will also guide their Fellows to present their summer research at future LEAP events and other workshops/conferences. Momentum Fellows are responsible for mentoring up to two (2) undergraduate students in the LEAP Research Experiences for Undergraduates (REU) program. Each project will host up to two (2) undergraduate students paired with one (1) Fellow. 2024 Summer Momentum Fellows KYLE McEVOY Project 3: Analysis of Climate Model Perturbed Physics Ensembles (Elsaesser, Medeiros) Kyle is a third year PhD student in Statistics in the Department of Statistics & Data Science at UCLA, advised by Karen McKinnon. His research interests include multiple hypothesis testing with spatially correlated data, covariance matrix estimation, distributional shifts in precipitation under climate change, and incorporating distributional shifts into simulated precipitation. After his PhD, Kyle hopes to work as a research scientist, investigating the climate system and exploring the impacts of climate change. In his free time, Kyle loves hiking, camping, and exploring the outdoors, especially in the many beautiful places located across California (though the redwood forests and the Sierra Nevada mountains are his favorites). PRANI NALLURI Project 5: Understanding and Modeling the Impact of Air-Sea Heterogeneity on Surface Fluxes (Balwada) Prani Nalluri (they/them)is a PhD student on the Geophysical Fluid Dynamics track in the Applied Physics and Applied Mathematics department at Columbia University. They are interested in improving parameterizations ofturbulent processes at submeso- and meso-scales in the ocean. This summer, Prani will be working with Dhruv Balwada to develop ML-based parameterizations for subgrid-scale air-sea fluxes in coupled atmosphere-ocean models. In the future, Prani hopes to continue their research in a national lab or as a part of a climate-focused start-up. Outside of science, they spend their time distance running and creating art through sewing and sculpture. JIMENG SHI Project 1: Quantifying Epistemic and Aleatoric Uncertainty in Climate Data (Mandt) Jimeng is pursuing his Ph.D. in computer science at the Knight Foundation School of Computing and Information Sciences, Florida International University. His research interests primarily focus on employing AI methods to model large-scale complex systems in real-world scenarios. He has proposed a groundbreaking ML framework named FIDLAr, designed for flood prediction and mitigation. FIDLAr has demonstrated notable efficacy, outperforming traditional physics-based models in accuracy and efficiency. Expanding its utility, Jimeng is now customizing FIDLAr for more optimization and data generation tasks across diverse domains. Furthermore, he is actively engaged in building AI foundation models within the weather and climate science. ANTONY SIKORSKI Project 2: Understanding and Modeling Turbulent Flow in the Atmosphere (Shamekh) Antony is a Statistics PhD candidate at the Colorado School of Mines. He also completed his MS in Data Science at Mines, and a BS in Applied Mathematics at the University of California, San Diego. His research interests primarily lie in merging machine learning and computational statistics to develop predictive methods for large spatial data volumes. Following the completion of his PhD, Antony aims to perform research as a data scientistin industry. A proud personal achievement of his is recently being awarded the NSF GRFP. Outside of work, Antony enjoys skiing, live music, and traveling! OBIN STURM Project 4: Understanding Ice Crystal Growth and Evolution in the Atmosphere (Lamb) Obin Sturm is a PhD student in the Atmospheric Composition and Earth Data Science group at the University of Southern California. His research focuses on improving our understanding and predictive power of atmospheric chemistry and physics through computational models. These models encompass realistic process-based representations of fundamental phenomena, 3D Earth system models exploring the interactions¬†of these processes at multiple scales, and modern AI and data-driven techniques bridging the first two approaches. Obin¬†believes the use of all three can lead to new scientific discoveries, and better inform environmental policies and decision making important for the well-being of both specific communities and our broader, global society. Obin‚Äôs recent contributions include data-driven, scientifically consistent aerosol superspecies to transport realistic volatility distributions in air quality forecasts, and creating an interface to extract detailed chemical states from NASA GEOS atmospheric composition forecasts for comprehensive process-based analysis. Projects Click the image below to learn more about the Summer 2024 Momentum Fellowship Research Projects. Contact Us E-mail: leap@columbia.edu Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2023-momentum-fellowship/",
    "transcript": "2023 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science who are interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling, and developing research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship Faculty work closely with fellows throughout the summer (June ‚Äì August 2023) on a well-defined, yet open-ended, machine learning research problem in climate data science. Faculty also guide their fellows to present their summer research at future LEAP events and other workshops / conferences. Tian Zheng, LEAP‚Äôs Education Director/Chief Convergence Officer, and Carl Vondrick, LEAP‚Äôs Data Science Director 2023 Summer Momentum Fellows Mohammad Erfani Mohammad is graduate research assistant currently pursuing his PhD in the Department of Civil and Environmental Engineering at the University of South Carolina. He graduated with a MSc in Civil and Environmental Engineering at Ferdowsi University of Mashhad, Iran. Since joining iWERS in August 2019, Mohammad has been applying Computer Vision and Deep Learning techniques to develop real-time flood mapping and early warning systems. His work has the potential to mitigate the devastating impacts of flooding by providing a framework that captures flood information with a higher spatiotemporal resolution and renders visual outcomes easier to interpret for decision-makers, emergency responders + the public. Yu Huang Yu is a PhD student in the Department of Earth and Environmental Engineering at Columbia University, and a member of the Gentine Lab. She received a B.Sc. degree in Atmospheric Sciences from Nanjing University, and is  now studying land-atmospheric interactions. She is especially interested in how water cycle and carbon cycle are coupled through land-atmosphere processes. Using climate models, machine learning techniques, and causal inference in her research, her professional goal is to be a professor or research scientist in either academia or industry. Matthew Beveridge Matt is an incoming doctoral student at Columbia University in the Columbia Imaging and Vision Lab (CAVE) with Prof. Shree Nayar and a Visiting Researcher at the University of Colorado Boulder with Prof. Morteza Karimzadeh. He completed his Master of Engineering (M.Eng.), advised by Prof. Daniela Rus, and Bachelor of Science (B.S.) in Electrical Engineering and Computer Science (EECS) at MIT, with a double major in Mathematics and minor in Theater Arts. Matt's research focuses on computer vision and machine learning for robust perception and its application to the science of the physical environment. Matt has also been involved with startups in the field of autonomy, organized community events around energy and climate, and worked on spaceflight at NASA. 2023 Summer Momentum Fellowship Faculty Dhruv Balwada Assistant Research Professor, Columbia Climate School / Lamont-Doherty Earth Observatory Kara Lamb Associate Research Scientist, Department of Earth + Environmental Engineering at Columbia University Carl Vondrick Associate Professor of Computer Science, Columbia University Pierre Gentine Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University 2023 Summer Momentum Fellowship Research Projects PROJECT I: ESTIMATING OCEAN CURRENTS FROM MEASUREMENTS OF SEA SURFACE HEIGHT (SSH) Project Lead : Dhruv Balwada Project Description : One of the best ways to observe the Earth‚Äôs ocean is from space. Space-based observations allow for spatial and temporal coverage that is hard to achieve from in-situ observations, which are usually point measurements made by ships or other in-water platforms. While we routinely observe the sea level (also called sea surface height ‚Äì SSH), temperature and color from space, other variables of interest, such as the ocean currents, are not directly observed. Estimating these currents is important since they transport materials, like heat, carbon, phytoplankton, and better knowledge of these currents can help to improve climate predictions, fisheries management, and oil spill monitoring. The LEAP momentum fellow will work on the task of estimating ocean currents from measurements of SSH. This will be in service of a new SSH measuring satellite, SWOT, which was recently launched to measure SSH at unprecedented spatial resolution. However, these new measurements come with a challenge ‚Äì the conventional physics based approaches for estimating surface currents from SSH do not work at the finer scales that will be observed. The reason being that the SSH at these scales is usually dominated by internal waves, and the currents and SSH corresponding to these waves are not linked by simple mathematical relationships. Recent work has shown that ML based techniques can potentially help address this challenge (eg. Xiao, Balwada et al 2023 , Wang et al 2022 ). However, important questions remain about the response of the ML models to wave amplitude, the generalization of the ML models trained on simulation data to real flows, the potential to apply transfer learning to improve predictions using in-situ observations, and the ability to estimate the errors in the predicted fields. The fellow will tackle some of these questions. Learning Outcomes + Deliverables : A well organized github repository showcasing the jupyter notebooks and codes used during the internship. A final research paper written with the goal of submission to an academic journal. *** PROJECT II: EQUIVARIANT GRAPH NEURAL NETWORKS FOR EFFICIENT EMULATION OF AEROSOL OPTICAL PROPERTIES Project Leads : Kara Lamb and Carl Vondrick Project Description : Forest fires and fossil fuel combustion are an important source of aerosols (small particulate matter) to the atmosphere. Aerosols from these sources strongly absorb sunlight, which can influence the atmospheric temperature profile, impact cloud formation, and cause snow to melt more quickly‚Äì all of which have important effects on the climate. Understanding the role aerosols play in the climate remains a major challenge due to limitations in our modeling and observational capacities. Aerosols are a variety of sizes, shapes, and chemical compositions, all of which impact their optical properties. Atmospheric models and observational retrievals typically approximate these particles as spheres or ellipsoids, which leads to biases in determining how much sunlight aerosols absorb and scatter. Accurate methods (such as the Multiple Sphere T-Matrix Method, MSTM) to calculate aerosol optical properties for arbitrarily shaped particles are very computationally expensive, often requiring hours or days to compute the optical properties of single particles with complex shapes. Our recent work demonstrated that graph neural networks (GNN) are a promising approach for emulating expensive aerosol optical properties codes such as MSTM that may be capable of generalizing to new (and previously unseen) particle shapes. The goal of this summer project would be to extend and improve this GNN modeling approach by including physical constraints in the machine learning model architecture (such as equivariant neural network layers or spherical harmonic basis functions). Novel training methodologies for random orientation calculations of aerosol optical properties (calculating optical properties over all possible orientations and polarization states) will be explored. The Fellow will explore methods to improve the zero-shot performance (i.e. without re-training) of this GNN model. Learning Outcomes + Deliverables : This research will lead to research papers submitted in scientific journals and/or in papers submitted to machine learning conferences. It would also contribute to the development of a code base to train and test machine learning methods for aerosol optical properties codes. *** PROJECT III: INSTRUCTION + MENTORING FOR 2023 SUMMER REU (RESEARCH EXPERIENCE FOR UNDERGRADUATES) PROGRAM Project Lead : Tian Zheng and Pierre Gentine Project Description : Presented¬†in partnership with the Summer at SEAS program, the DSI Scholars program and the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR , LEAP‚Äôs 2023 REU Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. This summer, six (6) undergraduate students from around the United States will participate in the REU Program, starting with a 3-week-long Momentum Bootcamp during which they will gain familiarity and skills with the ClimSim dataset , baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit, and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Momentum Bootcamp will be followed by 5 weeks of on-campus research at Columbia University, culminating in Final Research Presentations on July 27, 2023. The Momentum Fellow will join Tian Zheng and Candace Agonafir on the Bootcamp‚Äôs team of instructors, and mentor the REU students through their on-campus research experiences. Learning Outcomes + Deliverables : Measurable skill set in teaching and academic mentoring in the fields of climate science and machine learning. Maturation as a project manager and independent researcher with an eye toward establishment as a Principal Investigator capable of guiding the next generation of climate data science scholars. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2023-momentum-fellowship/#content",
    "transcript": "2023 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science who are interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling, and developing research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship Faculty work closely with fellows throughout the summer (June ‚Äì August 2023) on a well-defined, yet open-ended, machine learning research problem in climate data science. Faculty also guide their fellows to present their summer research at future LEAP events and other workshops / conferences. Tian Zheng, LEAP‚Äôs Education Director/Chief Convergence Officer, and Carl Vondrick, LEAP‚Äôs Data Science Director 2023 Summer Momentum Fellows Mohammad Erfani Mohammad is graduate research assistant currently pursuing his PhD in the Department of Civil and Environmental Engineering at the University of South Carolina. He graduated with a MSc in Civil and Environmental Engineering at Ferdowsi University of Mashhad, Iran. Since joining iWERS in August 2019, Mohammad has been applying Computer Vision and Deep Learning techniques to develop real-time flood mapping and early warning systems. His work has the potential to mitigate the devastating impacts of flooding by providing a framework that captures flood information with a higher spatiotemporal resolution and renders visual outcomes easier to interpret for decision-makers, emergency responders + the public. Yu Huang Yu is a PhD student in the Department of Earth and Environmental Engineering at Columbia University, and a member of the Gentine Lab. She received a B.Sc. degree in Atmospheric Sciences from Nanjing University, and is  now studying land-atmospheric interactions. She is especially interested in how water cycle and carbon cycle are coupled through land-atmosphere processes. Using climate models, machine learning techniques, and causal inference in her research, her professional goal is to be a professor or research scientist in either academia or industry. Matthew Beveridge Matt is an incoming doctoral student at Columbia University in the Columbia Imaging and Vision Lab (CAVE) with Prof. Shree Nayar and a Visiting Researcher at the University of Colorado Boulder with Prof. Morteza Karimzadeh. He completed his Master of Engineering (M.Eng.), advised by Prof. Daniela Rus, and Bachelor of Science (B.S.) in Electrical Engineering and Computer Science (EECS) at MIT, with a double major in Mathematics and minor in Theater Arts. Matt's research focuses on computer vision and machine learning for robust perception and its application to the science of the physical environment. Matt has also been involved with startups in the field of autonomy, organized community events around energy and climate, and worked on spaceflight at NASA. 2023 Summer Momentum Fellowship Faculty Dhruv Balwada Assistant Research Professor, Columbia Climate School / Lamont-Doherty Earth Observatory Kara Lamb Associate Research Scientist, Department of Earth + Environmental Engineering at Columbia University Carl Vondrick Associate Professor of Computer Science, Columbia University Pierre Gentine Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University 2023 Summer Momentum Fellowship Research Projects PROJECT I: ESTIMATING OCEAN CURRENTS FROM MEASUREMENTS OF SEA SURFACE HEIGHT (SSH) Project Lead : Dhruv Balwada Project Description : One of the best ways to observe the Earth‚Äôs ocean is from space. Space-based observations allow for spatial and temporal coverage that is hard to achieve from in-situ observations, which are usually point measurements made by ships or other in-water platforms. While we routinely observe the sea level (also called sea surface height ‚Äì SSH), temperature and color from space, other variables of interest, such as the ocean currents, are not directly observed. Estimating these currents is important since they transport materials, like heat, carbon, phytoplankton, and better knowledge of these currents can help to improve climate predictions, fisheries management, and oil spill monitoring. The LEAP momentum fellow will work on the task of estimating ocean currents from measurements of SSH. This will be in service of a new SSH measuring satellite, SWOT, which was recently launched to measure SSH at unprecedented spatial resolution. However, these new measurements come with a challenge ‚Äì the conventional physics based approaches for estimating surface currents from SSH do not work at the finer scales that will be observed. The reason being that the SSH at these scales is usually dominated by internal waves, and the currents and SSH corresponding to these waves are not linked by simple mathematical relationships. Recent work has shown that ML based techniques can potentially help address this challenge (eg. Xiao, Balwada et al 2023 , Wang et al 2022 ). However, important questions remain about the response of the ML models to wave amplitude, the generalization of the ML models trained on simulation data to real flows, the potential to apply transfer learning to improve predictions using in-situ observations, and the ability to estimate the errors in the predicted fields. The fellow will tackle some of these questions. Learning Outcomes + Deliverables : A well organized github repository showcasing the jupyter notebooks and codes used during the internship. A final research paper written with the goal of submission to an academic journal. *** PROJECT II: EQUIVARIANT GRAPH NEURAL NETWORKS FOR EFFICIENT EMULATION OF AEROSOL OPTICAL PROPERTIES Project Leads : Kara Lamb and Carl Vondrick Project Description : Forest fires and fossil fuel combustion are an important source of aerosols (small particulate matter) to the atmosphere. Aerosols from these sources strongly absorb sunlight, which can influence the atmospheric temperature profile, impact cloud formation, and cause snow to melt more quickly‚Äì all of which have important effects on the climate. Understanding the role aerosols play in the climate remains a major challenge due to limitations in our modeling and observational capacities. Aerosols are a variety of sizes, shapes, and chemical compositions, all of which impact their optical properties. Atmospheric models and observational retrievals typically approximate these particles as spheres or ellipsoids, which leads to biases in determining how much sunlight aerosols absorb and scatter. Accurate methods (such as the Multiple Sphere T-Matrix Method, MSTM) to calculate aerosol optical properties for arbitrarily shaped particles are very computationally expensive, often requiring hours or days to compute the optical properties of single particles with complex shapes. Our recent work demonstrated that graph neural networks (GNN) are a promising approach for emulating expensive aerosol optical properties codes such as MSTM that may be capable of generalizing to new (and previously unseen) particle shapes. The goal of this summer project would be to extend and improve this GNN modeling approach by including physical constraints in the machine learning model architecture (such as equivariant neural network layers or spherical harmonic basis functions). Novel training methodologies for random orientation calculations of aerosol optical properties (calculating optical properties over all possible orientations and polarization states) will be explored. The Fellow will explore methods to improve the zero-shot performance (i.e. without re-training) of this GNN model. Learning Outcomes + Deliverables : This research will lead to research papers submitted in scientific journals and/or in papers submitted to machine learning conferences. It would also contribute to the development of a code base to train and test machine learning methods for aerosol optical properties codes. *** PROJECT III: INSTRUCTION + MENTORING FOR 2023 SUMMER REU (RESEARCH EXPERIENCE FOR UNDERGRADUATES) PROGRAM Project Lead : Tian Zheng and Pierre Gentine Project Description : Presented¬†in partnership with the Summer at SEAS program, the DSI Scholars program and the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR , LEAP‚Äôs 2023 REU Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. This summer, six (6) undergraduate students from around the United States will participate in the REU Program, starting with a 3-week-long Momentum Bootcamp during which they will gain familiarity and skills with the ClimSim dataset , baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit, and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Momentum Bootcamp will be followed by 5 weeks of on-campus research at Columbia University, culminating in Final Research Presentations on July 27, 2023. The Momentum Fellow will join Tian Zheng and Candace Agonafir on the Bootcamp‚Äôs team of instructors, and mentor the REU students through their on-campus research experiences. Learning Outcomes + Deliverables : Measurable skill set in teaching and academic mentoring in the fields of climate science and machine learning. Maturation as a project manager and independent researcher with an eye toward establishment as a Principal Investigator capable of guiding the next generation of climate data science scholars. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2022-leap-summer-momentum-fellowship/",
    "transcript": "2022 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2022 Summer Momentum Fellowship Overview LEAP¬†invites applications from doctoral students who are or will be working at the interface of Climate Science and Data Science to its inaugural 2022 Summer Momentum Fellowship. A cohort of five LEAP Momentum Summer Fellows will learn about research projects at LEAP, develop research ideas in climate data science, and participate in LEAP‚Äôs summer activities, which include: Assisting learning activities during the LEAP Momentum Bootcamp (May 25-27, 2022) Serving as mentors to LEAP‚Äôs REU students Helping with LEAP‚Äôs summer workshops and socials for the REU students and SOARS Reviewing CESM datasets and machine learning research to create LEAP community data challenges Becoming early users of LEAP Pangeo and developing tutorials for the LEAP community Each fellow will be awarded a summer stipend to support their research and professional growth. Tian Zheng, LEAP‚Äôs Education Director/Chief Convergence Officer, and Carl Vondrick, LEAP‚Äôs Data Science Director 2022 Summer Momentum Fellows Shanice Bailey PhD Student, Ocean + Climate Physics (Lamont-Doherty Earth Observatory, Columbia University) Zhewen Hou PhD Student, Statistics (Columbia University) Jaesung Son PhD Student, Statistics (Columbia University) Harry Xi PhD Student, Statistics (Columbia University) Zhewen Hou PhD Student, Statistics (Columbia University) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2022-leap-summer-momentum-fellowship/#content",
    "transcript": "2022 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2022 Summer Momentum Fellowship Overview LEAP¬†invites applications from doctoral students who are or will be working at the interface of Climate Science and Data Science to its inaugural 2022 Summer Momentum Fellowship. A cohort of five LEAP Momentum Summer Fellows will learn about research projects at LEAP, develop research ideas in climate data science, and participate in LEAP‚Äôs summer activities, which include: Assisting learning activities during the LEAP Momentum Bootcamp (May 25-27, 2022) Serving as mentors to LEAP‚Äôs REU students Helping with LEAP‚Äôs summer workshops and socials for the REU students and SOARS Reviewing CESM datasets and machine learning research to create LEAP community data challenges Becoming early users of LEAP Pangeo and developing tutorials for the LEAP community Each fellow will be awarded a summer stipend to support their research and professional growth. Tian Zheng, LEAP‚Äôs Education Director/Chief Convergence Officer, and Carl Vondrick, LEAP‚Äôs Data Science Director 2022 Summer Momentum Fellows Shanice Bailey PhD Student, Ocean + Climate Physics (Lamont-Doherty Earth Observatory, Columbia University) Zhewen Hou PhD Student, Statistics (Columbia University) Jaesung Son PhD Student, Statistics (Columbia University) Harry Xi PhD Student, Statistics (Columbia University) Zhewen Hou PhD Student, Statistics (Columbia University) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/summer-institute/",
    "transcript": "Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP NYC Summer Institute Overview Each summer, The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability partner to present the NYC Summer Institute: Integrating Climate Education in NYC Public Schools. A vital part of LEAP‚Äôs education programming , the NYC Summer Institute constitutes 4-5 full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. Each Summer Institute cohort then presents the results of their climate lesson plan pilots in a final showcase event at Teachers College . Cohorts represents all five boroughs, and presentations will provide insights into bringing climate change education to the diversity of students in New York City. The NYC Summer Institute is free. Participants receive a stipend, and may earn up to 28 CTLE credits. Read about the 2024 Summer Institute and 2023 Summer Institute . Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of public school teachers to develop and lead lessons aligned with standards- and place-based learning. The goals of the NYC Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/summer-institute/#content",
    "transcript": "Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP NYC Summer Institute Overview Each summer, The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability partner to present the NYC Summer Institute: Integrating Climate Education in NYC Public Schools. A vital part of LEAP‚Äôs education programming , the NYC Summer Institute constitutes 4-5 full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. Each Summer Institute cohort then presents the results of their climate lesson plan pilots in a final showcase event at Teachers College . Cohorts represents all five boroughs, and presentations will provide insights into bringing climate change education to the diversity of students in New York City. The NYC Summer Institute is free. Participants receive a stipend, and may earn up to 28 CTLE credits. Read about the 2024 Summer Institute and 2023 Summer Institute . Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of public school teachers to develop and lead lessons aligned with standards- and place-based learning. The goals of the NYC Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2025-nyc-summer-institute/",
    "transcript": "2025 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2025 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2025 Summer Institute: Integrating Climate Education in NYC Public Schools from July 21 ‚Äì 25, 2025, at Teachers College, Columbia University . A vital part of LEAP‚Äôs education programming, the 2025 Summer Institute constitutes five full days of professional learning, in a collaborative space where high school educators, experts, and NYC Public Schools leaders come together to explore innovative ways of bringing climate education into classrooms. The Summer Institute will equip participants with the tools, resources, and ideas to: Adapt climate education to their unique classroom contexts. Meet educational standards while emphasizing place-based learning. Inspire their students to engage with climate change as it relates to their lives and communities. The 2025 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. APPLY BY FRIDAY, MARCH 7, 2025 Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of high school teachers** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2025 Summer Institute are: Deepen teachers‚Äô understanding of climate change. Empower teachers to engage students across diverse subjects. Co-create innovative teaching materials tailored to NYC. Build a supportive learning community of educators from across the city. ** Subsequent Summer Institutes will focus on preparing high school teachers. Read about the 2023 Summer Institute for K-5 teachers HERE , and the 2024 Summer Institute for middle school teachers HERE . Application, Dates, and Other Details High school teachers throughout NYC are eligible to apply to the 2025 Summer Institute. Application¬†deadline : March 7, 2027 Decisions released by : April 1, 2024 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2025-nyc-summer-institute/#content",
    "transcript": "2025 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2025 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2025 Summer Institute: Integrating Climate Education in NYC Public Schools from July 21 ‚Äì 25, 2025, at Teachers College, Columbia University . A vital part of LEAP‚Äôs education programming, the 2025 Summer Institute constitutes five full days of professional learning, in a collaborative space where high school educators, experts, and NYC Public Schools leaders come together to explore innovative ways of bringing climate education into classrooms. The Summer Institute will equip participants with the tools, resources, and ideas to: Adapt climate education to their unique classroom contexts. Meet educational standards while emphasizing place-based learning. Inspire their students to engage with climate change as it relates to their lives and communities. The 2025 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. APPLY BY FRIDAY, MARCH 7, 2025 Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of high school teachers** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2025 Summer Institute are: Deepen teachers‚Äô understanding of climate change. Empower teachers to engage students across diverse subjects. Co-create innovative teaching materials tailored to NYC. Build a supportive learning community of educators from across the city. ** Subsequent Summer Institutes will focus on preparing high school teachers. Read about the 2023 Summer Institute for K-5 teachers HERE , and the 2024 Summer Institute for middle school teachers HERE . Application, Dates, and Other Details High school teachers throughout NYC are eligible to apply to the 2025 Summer Institute. Application¬†deadline : March 7, 2027 Decisions released by : April 1, 2024 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2024-nyc-summer-institute/",
    "transcript": "2024 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2024 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2024 Summer Institute: Integrating Climate Education in NYC Public Schools from July 15 ‚Äì 19, 2024. A vital part of LEAP‚Äôs education programming, the 2024 Summer Institute constitutes five full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. The 2024 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. Applications are now closed Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of middle school teachers** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2024 Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. ** Subsequent Summer Institutes will focus on preparing high school teachers. Read about the 2023 Summer Institute for K-5 teachers HERE . Application, Dates, and Other Details Middle school teachers throughout NYC are eligible to apply to the 2024 Summer Institute. Application¬†deadline : March 1, 2024 Decisions released by : April 8, 2024 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/2024-nyc-summer-institute/#content",
    "transcript": "2024 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2024 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2024 Summer Institute: Integrating Climate Education in NYC Public Schools from July 15 ‚Äì 19, 2024. A vital part of LEAP‚Äôs education programming, the 2024 Summer Institute constitutes five full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. The 2024 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. Applications are now closed Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of middle school teachers** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2024 Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. ** Subsequent Summer Institutes will focus on preparing high school teachers. Read about the 2023 Summer Institute for K-5 teachers HERE . Application, Dates, and Other Details Middle school teachers throughout NYC are eligible to apply to the 2024 Summer Institute. Application¬†deadline : March 1, 2024 Decisions released by : April 8, 2024 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/nyc-summer-institute/",
    "transcript": "2023 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2023 Summer Institute: Integrating Climate Education in NYC Public Schools from July 17 ‚Äì 20, 2023 . A vital part of LEAP‚Äôs education programming, the 2023 Summer Institute constitutes four full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. The 2023 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. APPLICATIONS CLOSED Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of K-5 teachers ** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2023 Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. ** Subsequent Summer Institutes will focus on preparing middle and high school teachers. Application, Dates, and Other Details K-5 teachers throughout NYC are eligible to apply to the 2023 Summer Institute. Application deadline : May 1, 2023 Decisions released by : May 15, 2023 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education-2/nyc-summer-institute/#content",
    "transcript": "2023 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2023 Summer Institute: Integrating Climate Education in NYC Public Schools from July 17 ‚Äì 20, 2023 . A vital part of LEAP‚Äôs education programming, the 2023 Summer Institute constitutes four full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. The 2023 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. APPLICATIONS CLOSED Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of K-5 teachers ** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2023 Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. ** Subsequent Summer Institutes will focus on preparing middle and high school teachers. Application, Dates, and Other Details K-5 teachers throughout NYC are eligible to apply to the 2023 Summer Institute. Application deadline : May 1, 2023 Decisions released by : May 15, 2023 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/design-studio/",
    "transcript": "Design Studio - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Design Studio Research-Education Integration The Design Studio develops case studies and formal curricula for all LEAP education programs, while training graduate interns in curriculum development. Receiving course credit, interns will learn human-centered design, acquire experience with raw geoscience datasets, and develop metacognitive skills that advance their climate data science skills. The Design Studio will also organize an annual ‚Äútrain the trainer‚Äù two-day workshop. Summer 2022: Call for LEAP Design Studio Interns Summer 2022: TAI4ES Summer School Summer 2022: LEAP Momentum Bootcamp Spring 2022: LEAP Course Climate Prediction Challenges Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/design-studio/#content",
    "transcript": "Design Studio - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Design Studio Research-Education Integration The Design Studio develops case studies and formal curricula for all LEAP education programs, while training graduate interns in curriculum development. Receiving course credit, interns will learn human-centered design, acquire experience with raw geoscience datasets, and develop metacognitive skills that advance their climate data science skills. The Design Studio will also organize an annual ‚Äútrain the trainer‚Äù two-day workshop. Summer 2022: Call for LEAP Design Studio Interns Summer 2022: TAI4ES Summer School Summer 2022: LEAP Momentum Bootcamp Spring 2022: LEAP Course Climate Prediction Challenges Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/broadening-participation-2/",
    "transcript": "Broadening Participation - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP BROADENING PARTICIPATION Overview Resources LEAP prioritizes broad participation in the development and expansion of the field of Climate Data Science so that talented and inspired colleagues may work together to address current scientific challenges. Core = Broadening Participation is central to LEAP‚Äôs mission and is a part of every aspect of the Center‚Äôs work. Commitment =¬† LEAP is committed to leveraging perspectives, experiences and ideas from all participants to produce rigorous climate data science and impactful community engagement initiatives. To access or inquire about LEAP resources to support broad participation in climate data science, please reach out to Catherine Cha , Senior Manager of Communications + Knowledge Transfer. Key Tenets of Broadening Participation at LEAP Broad participation is critical to LEAPs scientific mission. Innovation = Richer questions, problems, and solutions emerge from skilled participants. LEAP‚Äôs team is equipped to address these complexities. Legacy = LEAP is committed to bidirectional knowledge transfer . Not only will we share our science out, we also acknowledge that we have much to learn from other stakeholders who possess knowledge critical to LEAP‚Äôs mission. Impact = LEAP‚Äôs work aims to address real needs. Our science must reach beyond the university . Broadening Participation Objectives Integrate LEAP‚Äôs values into recruitment, research, education, and knowledge transfer efforts. Increase engagement of all interested parties in the discipline of climate data science. Assess , review and revise all LEAP activities to evaluate the scope of participation. Knowledge Transfer Objectives Bidirectional Dialogue = between academia, private industry, government, non-profits and the public. Innovation = in storytelling, visualization and social media for broad stakeholder intake. Access = so that all LEAP research is open-source and broadly accessible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/knowledge-transfer/",
    "transcript": "Knowledge Transfer - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Knowledge Transfer Objectives LEAP‚Äôs knowledge transfer initiative grows out of and bolsters its commitments to broadening participation. LEAP‚Äôs knowledge transfer program sets forth three objectives: (1) Establish a bidirectional dialogue between academia and stakeholders; (2) Innovate storytelling, visualization, and social media for broad stakeholder engagement ; and (3) Make all LEAP outputs (e.g., data and code) open-source and broadly accessible . Bidirectional Knowledge Transfer Communications Programming and Partnerships : LEAP will develop a two-way engagement strategy between academia and stakeholders to provide public, corporate, and policy audiences access to ML-informed climate projections, and to train external audiences in navigating climate data science. LEAP will provide a variety in content and format of knowledge outputs to ensure relevance for a broad range of public and corporate audiences. Applied Management Research : LEAP will integrate not just knowledge transfer, but also knowledge generation, with respect to the implications of climate change and climate projections for public and private audiences. LEAP will conduct applied management research whose findings will inform causally-supported best practices for firms and organizations to communicate climate information to stakeholders. Storytelling + Social Media Innovation To innovate storytelling, visualization, and social media for broad stakeholder intake, LEAP will launch several initiatives including a ‚ÄúStorytellers in Residence‚Äù program, ‚ÄúTranslate-a-Thons,‚Äù public lectures, and a monthly newsletter. LEAP will also create, organize, translate, and disseminate research findings to aid public servants, community organizations, and corporations in adapting and innovating in response to climate change. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/",
    "transcript": "Events - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Events Focus Button Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/events-sub/",
    "transcript": "LEAP Events Calendar - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Events Focus Button Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/wallerstein-panel/",
    "transcript": "Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Wallerstein Panel on AI + Extreme Weather Preparedness LEAP‚Äôs new Panel Series on AI and Extreme Weather Preparedness will foster a continuous exchange of knowledge, providing a space for interdisciplinary dialogue on improving disaster preparedness globally through the use of AI in climate forecasts and modeling, monitoring, and improved communication and dissemination of advisories. The goal of this dialogue is to provide an opportunity for the broader climate and disaster community to come together with climate and data scientists to identify the emerging scientific advances in early warning systems and to learn best practices to improve decision-making and facilitate effective anticipatory action and response. Key themes will include: Effective Use of Climate Forecasts : How can forecasts ‚Äì from weather to seasonal time scales, as well as the emerging role of AI in climate projections ‚Äì be best utilized for predicting and responding to extreme events? Decision-Making Tools : How can existing tools and data, including AI-backed real-time monitoring, be employed more effectively to protect people and enhance disaster preparedness? Action : What are some strategies for improving communication and engagement to ensure that forecasts are understood and acted upon for real-world disaster mitigation? Long-term Planning : What role does AI play in improving climate prediction, and how can it be harnessed for long-term resilience building and infrastructure planning? Format: Virtual Moderated by: Geneva List , Senior Staff Associate This Panel Series is made possible by generous support from David Wallerstein. photo credit: Eva Manez, Reuters FRIDAY || FEBRUARY 28, 2025 || 10:00 a.m. EST Register // Add to Calendar FLOODING IN VALENCIA, SPAIN On October 29th, 2024, Spain‚Äôs Valencia region received one year‚Äôs worth of rain in eight hours. The resulting flash floods killed more than 220 people and highlighted critical gaps in disaster preparedness‚Äîdespite having an advanced national weather agency. Key challenges stemmed from deficiencies in local warning communications and urban development on floodplains. The inaugural event of the new LEAP Panel Series: AI + Extreme Weather will examine this flood event, assessing what disaster preparedness and response mechanisms proved effective and where improvements could have been made‚Äîparticularly through AI-driven solutions. The discussion will explore the role of AI in enhancing early warning systems, flood mapping, and forecasting to mitigate the impact of future extreme weather events. Panelists Gustau Camps-Valls ( University of Valencia) F√©lix Franc√©s (Polytechnic University of Valencia) Luis G√≥mez-Chova ( University of Valencia) Ana Ruescas ( University of Valencia) Introductory remarks from Pierre Gentine (Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Depts of Earth + Environmental Engineering and Earth + Environmental Sciences, Columbia University) and moderated by Geneva List (Senior Staff Associate, Columbia Climate School). photo credit: Erwan Hesry FRIDAY || MARCH 28, 2025 || 10:00 a.m. EST Registration link forthcoming EARLY WARNING SYSTEMS Abstract forthcoming photo credit: Scott Graham FRIDAY || April 18, 2025 || 10:00 a.m. EST Registration link forthcoming DECISION-SUPPORT AND POLICY Abstract forthcoming photo credit: Tyler Rutherford FRIDAY || May 16, 2025 || 10:00 a.m. EST Registration link forthcoming FIRE SEASON, HURRICANE SEASON Abstract forthcoming Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/2025-spring-lectures/",
    "transcript": "2025 Spring Lectures - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2025 Spring Lectures in Climate Data Science January 14, 2025 - May 22, 2025 12:00 ‚Äì 1:30 pm (EDT) IN-PERSON at the Tang Family Hall (Rm 202) at the Columbia Engineering Innovation Hub ( 2276 12th Ave, New York, NY ) VIRTUAL ATTENDANCE available Add to Calendar Click to see past Lectures in Climate Data Science from Fall 2024 , Summer 2024 , Spring 2024 , Fall 2023 , Spring 2023 , and Fall 2022 . TUESDAY || JANUARY 14, 2025 Watch on YouTube BERNARD MINOUNGOU Center R√©gional AGRHYMET ‚Äú NextGen Approach to Hydrological Forecasting: Adapting PyCPT Tool for Hydrological Forecasting‚Äù The NextGen forecasting system helps forecasters to evaluate the performance of different global climate models, which helps determine how best to correct and combine them. AGRHYMET Regional Climate Centre has been capacitating National Meteorological and Hydrological Services (NMHSs) in West Africa and the Sahel on NextGen seasonal forecasting systems. The capacity development efforts focus mainly on Python interface to the Climate Predictability Tool (CPT) or PyCPT, a tool developed by the International Research Institute for Climate and Society (IRI) to implement the NextGen approach to climate forecasting. While hydrological forecasts of water availability from watersheds in major river basins are essential to support operational planning and management, the latest version of PyCPT developed by IRI does not take into account the seasonal forecast of hydrological variables. To address this challenge and respond to the needs of NMHSs in the region in charge of hydrological monitoring, AGRHYMET has adapted and improving the PyCPT tool for seasonal hydrological forecasts. The primary hydrological data used to produce seasonal forecasts are the daily flows recorded at the main river stations in the region. These data are distributed across the main river basins. Forecasts are based on correlations between ocean surface temperature, precipitation, wind speed, and hydrological variables such as river flows for the high-water period. Principal component regression (PCR) is the most widely considered method. A variety of forecast performance scores consisting of scores based on continuous measurements, those on observed measurements, and, in some cases, forecasts are used as well. The work carried out by the AGRHYMET Regional Center, in response to the needs expressed by players in the sub-region, has resulted in the enhancement of PyCPT for hydrological applications. The current development has been tested through case studies in West Africa, and the results obtained are promising. THURSDAY || JANUARY 23, 2025 SARA SHAMEKH NYU Watch on YouTube LEAP Research Update: ‚ÄúDeveloping an Interpretable Model for Tropical Precipitation Using Machine Learning‚Äù Precipitation remains one of the most challenging variables to model in both traditional climate models and modern data-driven approaches. Climate models often fail to accurately reproduce rainfall distributions, typically underestimating heavy precipitation events. Many uncertainties persist, including the precise dependence of precipitation on its environmental conditions‚Äîboth thermodynamic and dynamic‚Äîand the role of small-scale structures like convective organization. In this study, we use observational data and machine learning to develop a transparent and interpretable model for tropical precipitation and precipitation area. Our findings indicate that precipitation can be accurately predicted when small-scale aspects of the domain are known, however, prediction accuracy significantly decreases when only large-scale conditions are considered, indicating the stochastic nature of precipitation. We then develop a stochastic model for rain area and connect large scale variables to to this model. Our model not only elucidates the dependency of precipitation on environmental conditions but can also be used to evaluate predicted precipitation from AI or convection schemes. VIVIANA ACQUAVIVA CUNY / Columbia University Watch on YouTube LEAP Research Update: ‚ÄúTiny Ocean: Physically meaningful dimensionality reduction for pCO2 reconstruction‚Äù Accurate reconstruction of the full pCO2 field is a crucial element in estimating air-sea carbon flux and ultimately understanding the magnitude and trend of the ocean carbon sink. Ocean data used to build machine learning models to reconstruct the full field of pCO2 rely on variables of different nature (spatial, temporal, and physical features), and are distributed inhomogeneously in space and time. Additionally, the dimensionality of data is significant, with over a dozen features and hundreds of thousands of highly correlated data points. As a result, it is beneficial to seek physically meaningful ways to reduce the dimensionality of data for easier manipulation. Clustering methods are promising, but they are often based on the Euclidean distance in the space of features, which is not a good tracer of similarity in the space of the target variable, in this case pCO2. To alleviate this problem, we build a new distance that follows the pCO2 more closely. We introduce the concept of information imbalance, which quantifies the asymmetry of information between different metrics, and use it to evaluate the information content of various candidate distances with respect to the target, the distance in pCO2 space, looking for the ‚Äúsweet spot‚Äù between dimensionality reduction and preservation of information. This new distance can be used to perform physically meaningful clustering and reduce dimensionality along both axes (features and number of data points), to build more agile machine learning models, and to aid visualization and physical interpretation of data. THURSDAY || JANUARY 30, 2025 HIKARU HAYAKAWA + JACKIE VANDERMEL Climate Cardinals Leveraging AI for Climate Justice: Partnerships Between Researchers and Civil Society This presentation explores the critical role of partnerships between researchers and civil society in harnessing the power of artificial intelligence to advance climate justice. Drawing from real-world experience in science communication with the United Nations and collaborations with leading tech companies, we will highlight how AI can break down barriers and create equitable climate solutions.¬†We will delve into the growing support for climate-justice-focused AI solutions and the challenges and barriers civil society organizations face in accessing and implementing these technologies. Using Climate Cardinals as a case study, we will showcase impactful partnerships with Google Cloud and Monday.com, illustrating how civil society can integrate AI to amplify its reach and efficiency in addressing climate issues.¬†The session will conclude by exploring actionable opportunities for AI research institutions to collaborate with civil society organizations, paving the way for innovative, inclusive, and scalable climate justice solutions. This talk aims to inspire researchers and civil society leaders to co-create impactful strategies, driving meaningful change through technology and collaboration. THURSDAY || FEBRUARY 6, 2025 Watch on YouTube TIAN ZHENG Columbia University Improving Distributional Learning for Dynamical Systems Accurately modeling dynamic systems requires understanding the full distribution of outcomes, especially when accounting for uncertainties and biases. This talk explores the importance of distributional learning in dynamic systems, focusing on a recent project that uses Kernelized Stein Discrepancy (KSD) for post hoc calibration. Our approach leverages prior knowledge of the marginal distribution to correct prediction biases resulting from distribution shifts and sampling errors, enhancing model robustness. Through numerical experiments with climate models, we demonstrate the effectiveness of KSD-based calibration in refining predictions and improving adaptability to unseen data in complex, chaotic systems. THURSDAY || FEBRUARY 13, 2025 Watch on YouTube AMY BRAVERMAN NASA JPL / California Institute of Technology Uncertainty Quantification for Remote Sensing Remote sensing data provide a vast trove of information for studying the Earth system and climate. These data are often treated as if they are ‚Äútruth‚Äù, but they are the results of complex inferential algorithms and are subject to uncertainty. In fact, the process of inferring the behavior of geophysical variables from noisy spectra observed from space, is itself an inverse problem. There is at present no universally agreed upon method for quantifying uncertainties in remote sensing data, and a wide variety of mostly ad hoc techniques are employed. Here I describe in general how these data sets are produced, the nature of the inverse problem that is embedded, and a probabilistic framework we have developed with an eye towards operational use. The latter demands that we address computational challenges we expect to face with NASA‚Äôs next generation of Earth observing satellites: the Earth System Observatory. Finally, I will describe an approach to ‚Äúspatial‚Äù UQ that not only provides uncertainties on a footprint-by-footprint basis, but also preserves between-footprint uncertainty relationships. THURSDAY || FEBRUARY 20, 2025 DION HO Columbia University Watch on YouTube LEAP Research Update: ‚ÄúData-Enhanced Two-Stream Methods for Computing Atmospheric Flux Transfer‚Äù The two‚Äêstream equations are the most parsimonious general models for radiative flux transfer with one equation to model each of upward and downward fluxes. This framework undergirds almost every weather and climate model worldwide for it enables blisteringly fast computations, and it is applicable to both longwave and shortwave radiation though our focus is on the latter. Current methods under the two-stream framework are, however, known to be inaccurate in many circumstances, for example, when modeling shortwave flux transfer through arctic aerosols. Consequently, we seek to use data to enhance two-stream methods while preserving their computational efficiency. We will present our new methods which involve spline interpolation, symbolic regression and neural networks as well as present both empirical and theoretical evidence for our improvements. MATTHIEU BLANKE NYU Watch on YouTube LEAP Research Update: ‚ÄúInterpretable Meta-Learning for Physical Systems‚Äù Machine learning methods can be a valuable aid in the scientific process, but they need to face challenging settings where data come from inhomogeneous experimental conditions. Recent meta-learning methods have made significant progress in multi-task learning, but they rely on black-box neural networks, resulting in high computational costs and limited interpretability. We introduce a new meta-learning model capable of learning efficiently from multiple environments, with an affine structure with respect to the learning task. We prove that this architecture can identify the physical parameters of the system, enabling interpretable learning. We demonstrate the competitive generalization performance and the low computational cost of our method by comparing it to state-of-the-art algorithms on physical systems, ranging from toy models to complex, non-analytical systems. The interpretability of our method is illustrated with applications to parameter identification and to adaptive control and system identification. THURSDAY || FEBRUARY 27, 2025 CHRISTINA TORRES Teachers College ‚Äì Columbia University Watch on YouTube LEAP Research Update: ‚ÄúBridging educational divides: A research-practice partnership to empower teachers to integrate climate change across subjects‚Äù No singular institution can address as large and complex a challenge as climate change education. Through an on-going research-practice partnership (RPP) between the Center for Sustainable Futures at Teachers College, Columbia University and New York City Public¬†Schools (NYCPS) Office of Energy and Sustainability, we identified main barriers for teachers‚Äô engagement with climate change education. These barriers include: misperception of climate as only a science topic, teacher knowledge gaps, curriculum resource gaps, and overall teacher preparedness. We sought to address these barriers by developing professional learning opportunities for NYCPS teachers aiming to develop teachers‚Äô knowledge of climate change, situate climate change in a local NYC context, and build connections between the impacts of climate change to K-12 education. Climate scientists from the Learning the Earth with Artificial Intelligence and Physics (LEAP) center at Columbia University, curriculum development experts, and NGOs/CBOs were active in these programs. We offered these programs in two modalities. The summer institute was created as a week-long, intensive workshop with 40 teachers from the same school level (e.g., elementary) to develop transdisciplinary lesson plans on climate change over the school year. The mid-winter institute provided 500 teachers access to NYCPS curricular experts and local partners around climate change education during a three-day program. This paper will present findings from both cases and future directions for this RPP. We hope this will serve as a model for future partnerships around climate change education, inspiring strategic collaborations between K-12 education systems and higher education. JOSEPH KO Columbia University Watch on YouTube LEAP Research Update: ‚ÄúML for Ice Microphysics: Challenges, Progress, and Opportunities‚Äù Clouds are a leading order source of uncertainty in weather and climate models. Ice clouds are particularly hard to model due to diverse crystal habits and poorly understood microphysics. Machine learning (ML) has the potential to improve parameterizations, by incorporating diverse observations and reducing both parametric and structural errors. Here, we present an overview of recent work to improve representations of both ice microphysical properties and processes, using different ML approaches. First, we review recent work by Lamb et al. 2024, where neural ordinary differential equations (NODE) and symbolic regression (SR) were applied to laboratory measurements of single-crystal growth to develop a new functional parameterization for vapor depositional growth. Second, we revisit existing m-D relationships in ice microphysics schemes and present a bottom up framework to infer mass from in situ optical imagery, using ML models trained on synthetic data. Finally, we present a roadmap for implementation into next-generation ice microphysical schemes, as well as opportunities for novel applications of ML in ice microphysics. THURSDAY || MARCH 6, 2025 Register to attend GABRIELE ACCARINO Columbia University LEAP Research Update: ‚ÄúNew Similarity Metrics for Earth System Model Evaluation: A Multi-Scale Wavelet-based Approach‚Äù Earth System Models (ESMs) simulate complex interactions within the Earth system, including the atmosphere, oceans, land, and cryosphere, to predict future climate conditions. These simulations generate vast, multi-dimensional data, that capture diverse physical fields such as temperature, precipitation, and radiative fluxes, across multiple spatial and temporal scales. Comparing ESM projections poses significant challenges particularly in the context of the Coupled Model Intercomparison Project Phase 6 (CMIP6), where projections are related to a wide range of scenarios and model realizations. To address this, the assessment of differences among ESM outputs (or between ESM outputs and data) can be reframed as the task of evaluating and quantifying perceptual similarities. Drawing on well-established concepts from image processing, we focus on similarities in relevant physical patterns and structural properties at the scales of interest. In this work, we introduce: (i) a set of candidate metrics that extend traditional pixel-based error measures, adapted to handle floating-point data typical of ESM data; (ii) a benchmark dataset and an evaluation pipeline to systematically assess the sensitivity of these metrics to controlled perturbations generated from a single reference map, and (iii) a prototype wavelet-based similarity metric tailored for climate data. This metric is highly flexible and tunable, designed to capture structural variations across multiple scales while remaining sensitive to both spatial patterns and magnitude differences in field values. MAX KAGAN Columbia Business School LEAP Research Update: ‚ÄúCorporate Sociopolitical Alignment and Policymaker Relations‚Äù A growing body of scholarship considers how firms‚Äô sociopolitical activism or alignment may help or harm their performance. Much of this scholarship considers how firms‚Äô association with controversial positions can polarize stakeholders‚Äô attitudes. These prior studies have demonstrated the potential for sociopolitical alignment to attract and/or alienate consumers, employees, and investors, but relatively few studies consider the effect of stance-taking on government policymakers. This is despite a large literature suggesting that government relations are a key element of firms‚Äô non-market strategy and that positive firm-government relations can yield myriad strategic benefits for companies. In this study, we take up this gap and ask whether firms are rewarded and/or punished by government policymakers for when they communicate sociopolitical stances. THURSDAY || MARCH 13, 2025 Register to attend MATTHIAS IHME Stanford Title TBA Abstract forthcoming ‚Ä¶ THURSDAY || MARCH 27, 2025 Register to attend HUGH MORRISON NSF NCAR Toward an Improved Representation of Warm Cloud Microphysics in Earth System Models The formation of drizzle and rain via drop collision-coalescence (‚Äúwarm rain‚Äù initiation) is a key component of Earth system models. This is typically represented using microphysics schemes that only predict a few variables to describe the drop population. When developing these relatively simple ‚Äúbulk‚Äù schemes, choices must be made regarding which cloud and rain variables to predict and how to calculate the micro-scale physical processes affecting drops. These structural choices need to be simple enough so that schemes can run efficiently in three-dimensional models. This talk will describe an investigation of the accuracy of bulk schemes using different sets of predicted variables and different methods, including machine learning, for representing drop collision-coalescence. The goal is to assess how accurately these schemes simulate the formation of rain drops starting from an initial population of (small) cloud droplets, relative to a detailed reference microphysical model. Using predicted variables that represent the properties of drops of all sizes ‚Äì the single category approach ‚Äî leads to much greater accuracy than the traditional two-category approach with separate variables for cloud (small drops) and rain (large drops), despite both approaches utilizing the same number of predicted variables (4). A sub-optimal choice of the predicted variables, as in the two-category approach, limits overall accuracy even when the coalescence rate calculation itself is highly accurate. This places an upper limit on accuracy using machine learned rates even when the rate calculation is perfectly emulated. On the other hand, using a neural network to obtain coalescence rates with the single category approach gives accurate results for a reasonable computational cost. Furthermore, only 3 predicted variables are needed for relatively accurate solutions (at least as accurate as the best-performing two-category schemes with 4 predicted variables). This result is consistent with Lamb et al. (2024, JAMES), who used a variational auto-encoder to show that the number of intrinsic dimensions for the problem of warm rain initiation is roughly 3. Prospects for utilizing the single category approach in Earth system models will be discussed, including challenges owing to the relatively coarse spatial resolution and long time step (~5 min) of these models. Finally, I will highlight recent developments of the Lagrangian ‚Äúsuper-droplet‚Äù method for modeling microphysics, which tracks representative cloud and rain drops in the modeled flow, and how this method can be used to further improve bulk single category schemes. This includes accounting for turbulent enhancement of drop collision-coalescence, which may be critical for rain initiation in some cloud regimes. THURSDAY || APRIL 3, 2025 Register to attend OREOLUWA BADAKI Teachers College ‚Äì Columbia University Title TBA Abstract forthcoming ‚Ä¶ THURSDAY || APRIL 17, 2025 Register to attend LINNIA HAWKINS Columbia University / NCAR JIARONG WU NYU LEAP Research Update: titles TBA Abstract forthcoming ‚Ä¶ THURSDAY || APRIL 24, 2025 Register to attend AXEL SEIFERT Deutscher Wetterdienst Title TBA Abstract forthcoming ‚Ä¶ THURSDAY || MAY 1, 2025 Register to attend KAITLYN LOFTUS Columbia University AYA LAHLOU Columbia University LEAP Research Update: titles TBA Abstract forthcoming ‚Ä¶ THURSDAY || MAY 8, 2025 Register to attend JAEYOUNG JUNG Columbia University YONGQUAN QU Columbia University LEAP Research Update: titles TBA Abstract forthcoming ‚Ä¶ THURSDAY || MAY 15, 2025 Register to attend ABIGAIL BODNER Climatematch Academy Title TBA Abstract forthcoming ‚Ä¶ THURSDAY || MAY 22, 2025 Register to attend DHRUV BALWADA Columbia University LEAP Research Update: Title TBA Abstract forthcoming ‚Ä¶ Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/2024-fall-lectures/",
    "transcript": "2024 Fall Lectures - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2024 Fall Lectures in Climate Data Science September 5, 2024 - December 19, 2024 12:00 ‚Äì 1:30 pm (EDT) IN-PERSON at the Tang Family Hall (Rm 202) at the Innovation Hub ( 2276 12th Ave, New York, NY ) VIRTUAL ATTENDANCE available Add to Calendar Click to see past Lectures in Climate Data Science from Summer 2024 , Spring 2024 , Fall 2023 , Spring 2023 , and Fall 2022 . THURSDAY || SEPTEMBER 5, 2024 Watch on YouTube JULIA SIMPSON Columbia University LEAP Research Update : ‚ÄúImproving parameterizations of heat & momentum air-sea fluxes‚Äù The ocean and atmosphere influence each other in elaborate exchanges at the air-sea nexus. These fluxes are transfers of properties such as heat, moisture, mass, and radiation. Atmospheric winds also exert stress‚Äìmomentum flux¬≠‚Äìon the ocean surface. Momentum and heat fluxes from the atmosphere drive ocean circulation; on longer timescales, the ocean acts as a reservoir of heat and greenhouse gases, holding tremendous influence over weather and climate patterns. However, the limited scope of observations leads to inaccuracies in global estimates of air-sea fluxes, which then affect the prediction of ocean circulation and the subsequent storage of heat and mass in the deeper ocean. Anthropogenic emissions have caused imbalances in this natural exchange and storage of properties, highlighting the need for accurate representation of air-sea fluxes in climate models. While advances in remote sensing and further deployment of in-situ measurements are enhancing observational capabilities, the closure problem persists, limiting the ability to calculate fluxes purely from physics-based equations. Therefore, empirical parameterizations are conventionally used to calculate air-sea fluxes. These ‚Äúbulk‚Äù parameterizations are based on extremely limited in-situ measurements from relatively homogeneous conditions; thus, though they are applied virtually universally, conventional bulk parameterizations are less accurate under varying conditions. Machine learning (ML)-based solutions to this problem are at an exciting but early stage, with current ML-based parameterizations trained solely on bulk parameterizations of fluxes. This work aims to utilize existing parameterizations and global satellite¬†data for pre-training to achieve robust representation, with transfer learning on in-situ observations to further refine estimates. This approach thereby leverages existing empirical knowledge, growing remote sensing capabilities, and novel ML techniques to expand the applicability and accuracy of air-sea flux models. THURSDAY || SEPTEMBER 12, 2024 Watch on YouTube ANDRE PERKINS Allen Institute for AI ACEv2 and the new frontier of AI-based emulators for climate simulation Climate simulation has been an invaluable tool for projecting future climate changes due to human activity, while also serving as our best laboratory for studying the interconnected Earth System.¬† These large and complex models are primarily limited to direct use by organizations with access to large computing facilities due to their demanding computational requirements.¬† However, as in many disciplines, recent advances in AI-based predictive technology are rapidly changing this landscape.¬† The weather forecasting community is one early leader in this transition for atmospheric science, where multiple recent AI-based models now offer competitive or even improved 1-10 day atmospheric forecasts compared to their physics-based predecessors, while requiring only a fraction of the compute power (e.g., a desktop workstation).¬† A similar shift is already on the horizon for climate simulation, with analogous techniques showing promise for large speedups and improved performance at coarse horizontal grid resolutions. In this talk, I will discuss the latest research from the Ai2 Climate Modeling team in partnership with GFDL, LLNL, and NVIDIA on the AI2 Climate Emulator (ACE), a machine-learning-based emulator of a physics-based global atmosphere model. As an emulator, ACE maintains realistic weather variability throughout decades-long rollouts with prescribed sea surface temperatures and sea ice, provides an accurate representation of the source model‚Äôs climate and the atmospheric response to forcing in AMIP scenarios, and is over 60x faster than the traditional model it emulates.¬† We‚Äôve also made progress toward coupling ACE to a more realistic ocean, with initial results from a slab ocean model.¬† We believe this technology provides a viable pathway toward full model emulation of an Earth System, serving as a valuable part of the toolkit for climate science.¬† However, since we are in the early stages, I will also touch upon some of the many remaining technical and ideological challenges to overcome. THURSDAY || SEPTEMBER 19, 2024 Watch on YouTube BRIAN MEDEIROS NCAR Hops, Skips, and Jumps: development activities around CAM7 The development cycle of the Community Earth System Model (CESM) and its atmospheric component, the Community Atmosphere Model (CAM) is long and tortuous. This lecture presents changes being introduced for CAM version 7 and describes the processes involved in developing CAM, including some of the challenges and compromises. Supporting model development, as well as users and their applications, creates a small ecosystem of tools, documentation, and communication channels. Some of these will be highlighted, including development of updated diagnostics software and tools that facilitate novel numerical experiments. Ongoing projects that push the boundaries of CESM and CAM, including those from LEAP, will be described to start sketching what future versions might look like. THURSDAY || SEPTEMBER 26, 2024 Watch on YouTube ** This event is part of Climate Week at Columbia Engineering . ** CLIMATE CHANGE AND FLOOD RISK LEAP presents this interactive discussion to explore the flood risks incurred by climate change, how modeling can support communities‚Äô efforts to adapt to climate change, and how flood risk models are being improved. During the workshop, participants will: 1. Learn about climate data and modeling, with a focus on flood risks; 2. Have the opportunity to ask questions to Columbia University flood risk experts; 3. Contribute ideas for how climate models can address their needs. Panelists : George Deodatis (Vice Dean of Research for Columbia Engineering, Santiago and Robertina Calatrava Family Professor of Civil Engineering and Professor of Earth and Environmental Engineering, Columbia University) Candace Agonafir (Postdoctoral Research Scientist, Columbia Water Center) Fatoumata Camara (SP^2 Science Pathways Scholar, Barnard College) Adam Nayak (PhD Student, Earth and Environmental Engineering, Columbia University) Facilitator : Savannah Thais (Associate Research Scientist, Data Science Institute, Columbia University) THURSDAY || OCTOBER 3, 2024 Watch on YouTube MAGGIE POWELL Columbia University LEAP Research Update: ‚ÄúLeveraging Subgrid-Scale Spatial Organization and Variability for Improved Cloud Fraction Parametrization Using PINACLES Simulations‚Äù Given the coarse resolution of Earth System Models (on the order of 100 km), cloud processes must be parametrized. As a consequence, cloud processes are a major source of uncertainty in climate projections, with particular intermodel disagreement in the representation of low marine boundary layer clouds. Cloud fraction parametrizations have long relied upon subgrid-scale information of the total water distribution using various probability density functional forms. However, the importance of subgrid-scale spatial organization, including coherent updraft and downdraft, for cloud fraction has not been fully examined. In this study, we use machine learning to implicitly learn the subgrid-scale organization of shallow clouds and assess the information gained from including these subgrid features in a neural network-based parametrization. For this work, we use data from the Predicting INteractions of Aerosol and Clouds in Large Eddy Simulations (PINACLES) model for domains in the Eastern North Atlantic, Northeastern Pacific, Southern Great Plains, and Southern Ocean. Particular focus is given to the varying importance of subgrid-scale variability across shallow cloud regimes. THURSDAY || OCTOBER 10, 2024 Watch on YouTube BIN YU UC Berkeley Interpreting Deep Learning Models Wavelet-like filters arise when sparse dictionary learning optimization is fed with natural image patches, and they correspond to the V1 primary visual cortex of the human brain. Moreover, wavelets as local Fourier Transforms are interpretable in physical sciences and beyond. In this talk, we will first describe adaptive wavelet distillation (AWD) to turn black-box deep learning models interpretable in cosmology, cellular biology, and climate science problems while improving predictive performance. Time permitting, we will briefly describe deep learning results about feature learning in simple settings and improving LoRA for fine-tuning deep learning models through LoRA+. THURSDAY || OCTOBER 24, 2024 Watch on YouTube SHAWN LI Columbia University / Data Science Institute LEAP Research Update: ‚ÄúA Distribution-Driven Probabilistic Data Assimilation Framework for Climate Science‚Äù Traditional data assimilation techniques in weather forecasting focus on matching model parameters to observed state trajectories, such as daily temperature series. However, for climate modeling, capturing the long-term distributions of state variables‚Äîlike the frequency and range of temperatures‚Äîis crucial. This talk introduces a new probabilistic framework designed to address this need by emphasizing probability density functions (PDFs) of state variables, using the Lorenz‚Äô96 system as a toy model. We have developed a PDF emulator utilizing conditional normalizing flow models, which are adept at transforming simple initial distributions into complex data distributions through a series of invertible mappings. This emulator efficiently replicates the state distributions of climate variables without simulating every possible state trajectory, offering a significant computational advantage. Building on this foundation, our framework includes a sophisticated, distribution-driven approach to model parameter inference. This method aligns the emulated PDFs with observed data distributions and incorporates uncertainty quantification, enhancing model reliability and applicability in various climatic conditions. The framework‚Äôs potential extends to applications such as extreme value estimation and joint quantile analysis, which are pivotal for understanding and predicting climate extremes. By focusing on the tails and joint distributions of state variables, our approach promises improvements over traditional data assimilation methods, presenting a new tool for advanced climate modeling and analysis. THURSDAY || OCTOBER 31, 2024 Watch on YouTube ROBERT PINCUS Columbia University Shaping the Future of Data, Computation, and Collaboration at LEAP This week‚Äôs Lecture in Data Science isn‚Äôt a lecture at all but a discussion facilitated by Tian Zheng (LEAP Deputy Director) and Robert Pincus about the evolving needs of LEAP‚Äôs broad community. We would especially like to identify possible areas where we can share technical expertise ‚Äì for example, around the training of ML models or practices for reproducibility. Based on your input, the LEAP Center will try to nucleate short- or long-term communities of practice around particular tasks and, following on the examples of ClimSim and ClimSim-Online, focusing effort on specific broadly-useful tasks. We hope for lots of input from the research community but especially from those involved in education and knowledge transfer (writ broadly). THURSDAY || NOVEMBER 7, 2024 Watch on YouTube THEA HEIMDAL Columbia University ABBY SHAUM Columbia University LEAP Research Update: ‚ÄúUsing CMIP6 models to better understand the ocean carbon sink‚Äù In order to understand climate impacts from rising carbon emissions, it is critical to accurately quantify the air-sea CO 2 exchange and the ocean carbon uptake (i.e., ocean sink) in time and space. Measurements of ocean carbon are sparse because of the vast size of the ocean and limited resources. It is possible to reconstruct ocean carbon values where measurements do not exist by using Machine Learning (ML). However, current estimates of ocean carbon uptake are uncertain and show a large spread. Earth system models (ESMs) can be used as ‚Äòtestbeds‚Äô to better understand the uncertainties in the ocean sink estimates. The target (ocean carbon) and driver variables (e.g., ocean temperature) needed for the ML reconstruction are sampled from the testbed in a time and space pattern identical to real-world measurements. The benefit of a testbed comes from the fact that the full reconstruction can be compared against a correct solution ‚Äì the full testbed model field ‚Äì and not just a test set (typically 20% of available data). Furthermore, a testbed can be used to explore various ocean carbon sampling strategies without actually doing the measurements, which would save time and money. We have used the Pangeo-ESGF CMIP6 Zarr Data 2 (Busecke and Stern, 2024) as a testbed to reconstruct ocean carbon globally over the period 1982-2022. Our work represents a case study on how to utilize the CMIP6 catalogue available on the LEAP Pangeo platform. We will show how current and future sampling, as well as algorithmic design choices, can impact ML reconstructions and ocean sink estimates. THURSDAY || NOVEMBER 14, 2024 Watch on YouTube JANNI YUVAL Google Research Neural general circulation models for weather and climate Recent advancements in machine learning (ML) have led to various data-driven approaches aimed at enhancing weather prediction and climate modeling. Typically, an ML-only method is used to improve weather prediction. While current state-of-the-art ML approaches achieve lower errors at medium-range lead times, physics-based models demonstrate superior physical consistency and better forecast accuracy at longer lead times. In the realm of climate modeling, ML is often used in a hybrid approach, where ML components replace uncertain parameterizations while still adhering to the governing equations for the scales they resolve. Despite advancements in atmospheric hybrid models, current attempts face challenges such as instability over extended periods, limited applicability to idealized scenarios, and only modest improvements on longer time scales in realistic scenarios. In this talk, I will explore the effectiveness of a novel approach to atmospheric modeling suitable for both weather forecasting and climate modeling. This approach involves the development of a differentiable atmospheric model, named NeuralGCM, which for the first time combines ML techniques with governing equations in an end-to-end training using ERA5 data. NeuralGCM competes with ML models for 1-10 day forecasts and matches the European Centre for Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts. Over longer time scales, NeuralGCM displays emergent phenomena such as seasonal cycles, monsoon patterns, and tropical cyclone formation, while achieving comparable spatial bias to a global cloud-resolving model. Additionally, we present the first successful simulation of an AMIP-like experiment using a hybrid atmospheric model. THURSDAY || NOVEMBER 21, 2024 Watch on YouTube ARVIND RENGANATHAN University of Minnesota LEAP Research Update: ‚ÄúAdvancing Environmental Modeling with Knowledge-Guided Encoder-Decoder Frameworks‚Äù Recent advancements in machine learning (ML) have led to various data-driven approaches for environmental system modeling. These approaches typically implement direct mapping from drivers to response instead of leveraging more suitable encoder-decoder architectures. In this talk, I will explore how the encoder-decoder paradigm provides additional key capabilities for environmental modeling while providing avenues for additional knowledge guidance. The paradigm naturally handles context while enabling the processing of varied information types, from entity characteristics to historical observations and meteorological drivers. The sequence-to-sequence design separates context processing from query handling, enabling flexible query integration. Adding to these capabilities, the encoder allows the modeling of multi-scale processes and data assimilation more naturally while supporting uncertainty quantification for prediction outputs. Building on these advantages, the framework enables the transfer of knowledge across entities through a systematic approach. The encoder processes contextual information and learns transferable representations from data-rich entities, while the decoder adapts these representations for predictions in data-sparse scenarios. This design enables few-shot learning for entities with limited observations and zero-shot prediction for unobserved entities. Further, the ability to handle varied context information makes this architecture suitable for foundation model development in environmental sciences. To demonstrate these capabilities in practice, we implement this paradigm across multiple environmental applications: GPP prediction in sparsely observed locations, operational streamflow forecasting, uncertainty quantification for hydrological modeling, and time series forecasting. Our results validate the encoder-decoder architecture‚Äôs effectiveness for environmental ML systems, showing the advantages of context integration through an encoder-decoder approach. THURSDAY || DECEMBER 5, 2024 Watch on YouTube TIM HERMANS Utrecht University Investigating the Use of Neural Networks to Project Changes in Extreme Storm Surges in Europe Projections of atmospherically driven changes in extreme storm surges are often based on small ensembles of climate model simulations because they are computed with computationally expensive hydrodynamic models. Consequently, these projections are sensitive to internal climate variability and inter-model differences. Data-driven storm surge models are emerging as a promising, computationally cheaper alternative to translate simulated changes in atmospheric variables to changes in storm surges. However, the ability of data-driven models to predict extreme storm surges in Europe is unclear because previous studies did not address the underrepresentation of extremes in the training data. Therefore, we investigated how well neural networks can reconstruct extreme storm surges when addressing the imbalance in the training data using the cost-sensitive learning approach in which samples with a lower density contribute to the training loss more. We find that density-based weighting improves both the RMSE and F1 score of predictions v.s. observed extremes, although the optimal weights depend on the location. Our neural networks typically outperform an existing multi-linear regression model, but perform less well than a state-of-the-art hydrodynamic model (GTSM). Using a ConvLSTM- instead of LSTM layer reduces both the underestimation of the highest extremes and the number of false positive predictions, improving the error metrics of the neural networks at almost all selected locations. In light of the potential application of neural networks to global climate model simulations to project changes in extreme storm surges, I will also discuss preliminary results of the application of our neural networks to atmospheric forcing from a global climate model participating in HighResMIP. THURSDAY || DECEMBER 5, 2024 Watch on YouTube VERONIKA EYRING DLR / University of Bremen Understanding and Modeling the Earth System with Machine Learning Earth System Models (ESMs) are fundamental to understanding and projecting climate change. The models have continued to improve over the years, but systematic errors and large uncertainties in their projections remain. A large contribution to this uncertainty stems from the representation of processes such as clouds and convection that occur at scales smaller than the resolved model grid. This impacts the models‚Äô ability to accurately project global and regional climate change, climate variability, and extremes. New approaches are required with breakthroughs expected in particular from the combination of high-resolution simulations that can resolve small-scale and fast processes, the wealth of Earth observations, and machine learning (ML) techniques. High-resolution, cloud resolving models with horizontal grid resolution of a few kilometers alleviate many biases of coarse-resolution models for deep clouds and convection, wave propagation and precipitation, but they cannot be run at climate timescales for multiple decades or longer due to high computational costs. Yet short simulations from high-resolution models can serve as information to develop ML-based parametrisations that are then incorporated into hybrid ESMs that promise to have significantly reduced systematic errors and enhanced projection capability compared to current ESMs. In contrast to km- scale climate models, ESMs incorporate important Earth system processes and feedbacks while still being fast enough to provide large ensembles important to simulate internal variability and extremes and to improve attribution and understanding. This combination can drive a paradigm shift in current Earth system modelling and analyses towards a new data-driven, yet still physics-aware, science. The key goal is a hybrid modelling approach that maintains physical consistency and realistically extrapolates to unseen climate regimes while reducing climate projection uncertainties and improving Earth system understanding. The talk presents progress in hybrid modelling work with the ICOsahedral Non-hydrostatic (ICON) atmospheric model from the European Research Council (ERC) Synergy Grant on ‚ÄúUnderstanding and Modelling the Earth System with Machine Learning (USMILE)‚Äù as well as key challenges and visions on AI-empowered next-generation multiscale climate modeling for mitigation and adaptation. THURSDAY || DECEMBER 19, 2024 Joseph Lockwood: Watch on YouTube Mohammadreza Mousavi Kalan: Watch on YouTube JOSEPH LOCKWOOD Columbia University / Data Science Institute LEAP Research Update: ‚ÄúModeling Tropical Cyclone Wind Fields and Deep Convection Drivers with Generative AI and Explainable Models‚Äù This presentation focuses on two projects: (1) generative super-resolution¬†for downscaling tropical cyclone (TC) wind fields, and (2) explainable AI to refine integrated moisture convergence indices for deep convection cloud regimes¬†and identify key environmental drivers. The first project introduces a deep learning framework that downscales TC wind fields from ERA5 data (0.25¬∞) to high-resolution observations (0.05¬∞). The¬†framework includes a debiasing neural network and a conditional denoising diffusion probabilistic model (DDPM) for generative super-resolution. This approach¬†captures fine-scale spatial details, enabling accurate high-resolution wind field prediction from coarse data. In the second project, we refine the environmental indices governing integrated moisture convergence for deep convection cloud regimes. Existing indices¬†such as saturation fraction and moist convective instability are often arbitrarily¬†defined. Using a variational autoencoder (VAE), we uncover latent representations of deep convection cloud regimes, and symbolic regression identifies key¬†environmental variables controlling moisture convergence. Preliminary results¬†demonstrate the value of explainable AI in improving predictions of deep convection cloud processes. MOHAMMADREZA MOUSAVI KALAN Columbia University LEAP Research Update: ‚ÄúTransfer Neyman-Pearson Algorithm for Outlier Detection‚Äù We consider the problem of transfer learning in outlier detection where target abnormal data is rare. While transfer learning has been considered extensively in traditional balanced classification, the problem of transfer in outlier detection and more generally in imbalanced classification settings has received less attention. We propose a general algorithmic approach which is shown theoretically to yield strong guarantees with respect to a range of changes in abnormal distribution, and at the same time amenable to practical implementation. To validate our approach, we apply the proposed algorithmic procedure to climate data for heavy rain detection, demonstrating that it outperforms several baseline methods. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/leap-agu24/",
    "transcript": "LEAP @ AGU24 - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP @ AGU24 FALL MEETING Find LEAP at the American Geophysical Union (AGU) 2024 Fall Meeting Below is a guide to notable presentations and other events from LEAP colleagues at the Dec. 9-13, 2024, meeting of the American Geophysical Union , the world‚Äôs largest gathering of earth and space scientists. The meeting will take place in Washington, DC, and online across the globe. Presentations are listed in chronological order. This list will be updated as additional information is gathered. Ti mes listed below are U.S. Eastern (EDT). ( Note : If you call up an abstract, the time displayed may default to your own local time if you attend remotely.) MONDAY || DEC. 9, 2024 8:00 ‚Äì 17:30 || NG01-06: Deep Generative Data Assimilation in Multimodal Setting JUAN NATHANIEL (Columbia) with LEAP colleagues: Yongquan Qu, Shuolin Li, Pierre Gentine iPoster Gallery (Online) 8:00 ‚Äì 17:30 || NG01-10: Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming YONGQUAN QU (Columbia) with LEAP colleague: Pierre Gentine iPoster Gallery (Online) 8:30 ‚Äì 12:20 || B11M: The Global Carbon Cycle and Its Feedbacks with Anthropogenic Change (Poster) FORREST HOFFMAN (Oak Ridge Nat‚Äôl Laboratory) with LEAP colleague: Galen McKinley Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || B11M-1505: Internal Climate Variability Modulates Decadal Changes in Ocean Anthropogenic Storage HOLLY OLIVAREZ (Louisiana State) with LEAP colleague: Galen McKinley Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || B11M-1510: Scale-Dependent Drivers of Air-Sea CO2 Flux Variability Using the ECCO-Darwin Model AMANDA FAY (Columbia) with LEAP colleague: Galen McKinley Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || B11M-1514: Using Sub-sampling Experiments to Better Understand the Ocean Carbon Sink THEA HEIMDAL (Columbia) with LEAP colleagues: Galen McKinley, Abby Shaum Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || GC11H-0034: Probabilistic Distribution-Driven Data Assimilation Framework for Climatic and Hydrological Science SHUOLIN LI (Columbia) with LEAP colleagues: Tian Zheng, Pierre Gentine Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || GC11T: Perturbed Parameter Ensembles (PPEs) for Understanding Processes and Quantifying Uncertainty in Earth System Models (Poster) GREGORY ELSAESSER (Columbia / NASA GISS) with LEAP colleagues: Katie Dagon, Qingyuan Yang, Kaitlyn Loftus Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || GC11T-0155: Insights from Emulating CAM6 PPE with Neural Network and an Additive Emulator QINGYUAN YANG (Columbia) with LEAP colleagues: Gregory Elsaesser, Marcus van Lier-Walqui, Linnia Hawkins Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || GC11T-0160: Perturbed Parameter Ensembles and Precipitation Quantile Uncertainty KYLE McEVOY (UCLA, LEAP Momentum Fellow 2024)) with LEAP colleagues: Qingyuan Yang, Gregory Elsaesser, Brian Medeiros Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || P11E-3005: Can Oscillating Cloud Albedo and Surface Temperature Explain 55 Cancri e‚Äôs ‚ÄúExotic‚Äù Atmosphere? KAITLYN LOFTUS (Columbia) Hall B-C (Poster Hall) (Convention Center) 8:40 ‚Äì 8:50 || NG11A-02: A Differentiable Framework to Reduce Structural and Parametric Uncertainty in Cloud Microphysics Parameterizations Online KARA LAMB (Columbia) with LEAP colleagues: Jatan Buch, Maggie Powell, Joseph Ko, Juan Nathaniel, Pierre Gentine Marquis 12-13 (Marriott Marquis) 8:42 ‚Äì 8:52 || H11E-02: Knowledge-guided Machine Learning for Modeling Multi-scale Processes and Data Assimilation: An Application to Streamflow Forecasting for River Basins VIPIN KUMAR (Univ. of Minnesota) with LEAP colleague: Arvind Renganathan 143 A-C (Convention Center 8:50 ‚Äì 9:00 || IN11A-03: Assessing Reproducibility Standards in Earth Science: Insights from LEAP Lab Research CANDACE AGONAFIR (Columbia) with LEAP colleague: Tian Zheng Marquis 3-4 (Marriott Marquis) 9:45 ‚Äì 9:55 || H11D-07: A Nonstationary Stochastic Simulator for Clustered Regional Hydroclimatic Extremes to Characterize Compound Flood Risk ADAM NAYAK (Columbia) with LEAP colleague: Pierre Gentine 145 B (Convention Center) 11:40 ‚Äì 11:50 || A12C-09: Optimizing Cloud Seeding with a Denoising Diffusion Model JATAN BUCH (Columbia) with LEAP colleagues: Kara Lamb, Pierre Gentine 152B (Convention Center) 15:02 ‚Äì 15:12 || C13D-06: Deep Learning for Identifying Functional Relationships Involving Basal Friction and Ice Fluidity RACHEET MATAI (Columbia) with LEAP colleagues: Dave Porter, Jonathan Kingslake, Laure Zanna Salon I (Convention Center) 16:00 ‚Äì 16:10 || GC14D-01: A Climate Model Calibrated Physics Ensemble (CPE) as One Foundation for Climate Observing System Simulation Experiments (invited) GREGORY ELSAESSER (Columbia / NASA GISS) with LEAP colleague: Marcus van Lier-Walqui Salon A (Convention Center) 16:00 ‚Äì 17:30 || B14D: The Global Carbon Cycle and Its Feedbacks with Anthropogenic Change (Oral) FORREST HOFFMAN (Oak Ridge Nat‚Äôl Laboratory) with LEAP colleague: Galen McKinley, Chair + Convener 151A (Convention Center) 16:00 ‚Äì 17:30 || GC11E: Perturbed Parameter Ensembles (PPEs) for Understanding Processes and Quantifying Uncertainty in Earth System Models (Oral) GREGORY ELSAESSER (Columbia / NASA GISS) with LEAP colleagues: Katie Dagon, Qingyuan Yang, Kaitlyn Loftus, Chairs + Conveners Salon C (Convention Center) 17:12 ‚Äì 17:22 || GC14E-08: Challenges and Opportunities for Unified Top-down and Bottom-up Constraint of ESM Parameterizations MARCUS van LIER-WALQUI (Columbia) with LEAP colleagues: Kaitlyn Loftus, Gregory Elsaesser Salon C (Convention Center) TUESDAY || DEC. 10, 2024 8:00 ‚Äì 17:30 || NG01-06: Deep Generative Data Assimilation in Multimodal Setting JUAN NATHANIEL (Columbia) with LEAP colleagues: Yongquan Qu, Shuolin Li, Pierre Gentine iPoster Gallery (Online) 8:00 ‚Äì 17:30 || NG01-10: Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming YONGQUAN QU (Columbia) with LEAP colleague: Pierre Gentine iPoster Gallery (Online) 8:30 ‚Äì 12:20 || A21D-1721: Data-driven Multiscale Modeling of Flow in Plant Canopies JAEYOUNG JUNG (Columbia) with LEAP colleagues: Ensheng Weng, David Lawrence, Marco Giometto Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || B21N-1472: Global Characterization and Forecasting of Leaf Phenology Using Deep Learning Methods and Remote Sensing AYA LAHLOU (Columbia) with LEAP colleagues: Pierre Gentine, Linnia Hawkins Hall B-C (Poster Hall) (Convention Center) 9:20 ‚Äì 9:30 || GC21A-06: Ocean Carbon Sink Uncertainties at the Scale of mCDR GALEN McKINLEY (Columbia) with LEAP colleagues: Thea Heimdal, Abby Shaum, Viviana Acquaviva Salon B (Convention Center) 10:55 ‚Äì 11:05 || GC22A-04: Can ML Beat Chaos? PIERRE GENTINE (Columbia) with LEAP colleague: Juan Nathaniel Salon A (Convention Center) 13:40 ‚Äì 17:30 || NH23C-2302: Visualizing Flood Impacts in NYC Neighborhoods Due to Climate Change FATOUMATA CAMARA (Columbia) with LEAP colleagues: Candace Agonafir, Tian Zheng Hall B-C (Poster Hall) (Convention Center) 14:10 ‚Äì 15:40 || IN23D-04: Interpretable Machine Learning-based Radiation Emulation for ICON KATHARINA HAFNER (Univ. of Bremen) with LEAP colleagues: Sara Shamekh, Pierre Gentine, Robert Pincus eLightning Theater 6 (Convention Center) 16:02 ‚Äì 16:12 || GC14E-01: Land Carbon Sink Uncertainty in a Perturbed Parameter Ensemble DANIEL KENNEDY (NCAR) with LEAP colleagues: Linnia Hawkins, Katie Dagon, David Lawrence Salon C (Convention Center) WEDNESDAY || DEC. 11, 2024 8:00 ‚Äì 17:30 || NG01-06: Deep Generative Data Assimilation in Multimodal Setting JUAN NATHANIEL (Columbia) with LEAP colleagues: Yongquan Qu, Shuolin Li, Pierre Gentine iPoster Gallery (Online) 8:00 ‚Äì 17:30 || NG01-10: Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming YONGQUAN QU (Columbia) with LEAP colleague: Pierre Gentine iPoster Gallery (Online) 8:30 ‚Äì 12:20 || A31B-1687: Emulating Lagrangian Ice Crystal Evolution in Cirrus Clouds Using Supervised Machine Learning ASHLEY NGUYEN (UC Berkeley) with LEAP colleagues: Joseph Ko, Kara Lamb Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || A31B-1693: How Well Can Power Laws Describe Microphysical Processes in Bulk Schemes? ARTHUR HU (Columbia) with LEAP colleagues: Marcus van Lier-Walqui, Kaitlyn Loftus Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || A31G-1810: Discovering a Model for Depositional Ice Growth from Observations Using Neural Ordinary Differential Equations and Symbolic Regression KARA LAMB (Columbia) Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || NG31B-2089: Extreme Value Theory: A Suitable Tool for the Rarest Extreme Weather Events? QINGYUAN YANG (Columbia) Hall B-C (Poster Hall) (Convention Center) 9:50 ‚Äì 10:00 || GC31E-09: Investigating the Use of Neural Networks to Project Changes in Extreme Storm Surges in Europe TIM HERMANS (Utrecht) with LEAP colleague: Julius Busecke 202 A (Convention Center) 10:55 ‚Äì 11:05 || H32B-04: Leveraging Comprehensive Terrestrial Ecosystem Data and Temporal Deep Learning to Enhance Understanding of Climate-Ecosystem Interactions in Mediterranean Regions MITRA ASADOLLAHI (Columbia) with LEAP colleagues: Juan Nathaniel, Pierre Gentine 140 A-B (Convention Center) 13:40 ‚Äì 17:30 || OS33C-0672: Super-resolution of Sea Surface Height (SSH) Using Machine Learning WILLIAM MANRIQUEZ (LEAP REU‚Äô24) with LEAP colleague: Dhruv Balwada Hall B-C (Poster Hall) (Convention Center) 15:30 ‚Äì 15:40 || A33J-08: Spatiotemporal Machine Learning Models for Emulation of Atmospheric Composition MOHAMMED ERFANI (Columbia) with LEAP colleagues: Kara Lamb, Marcus van Lier-Walqui Salon H (Convention Center) 16:00 ‚Äì 16:10 || NH34A-01: Structured Exploration of Machine Learning Model Complexity for Spatio-temporal Forecasting of Urban Flooding CANDACE AGONAFIR (Columbia) with LEAP colleague: Tian Zheng Capitol/Congress (Marriott Marquis) THURSDAY || DEC. 12, 2024 8:00 ‚Äì 17:30 || NG01-06: Deep Generative Data Assimilation in Multimodal Setting JUAN NATHANIEL (Columbia) with LEAP colleagues: Yongquan Qu, Shuolin Li, Pierre Gentine iPoster Gallery (Online) 8:00 ‚Äì 17:30 || NG01-10: Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming YONGQUAN QU (Columbia) with LEAP colleague: Pierre Gentine iPoster Gallery (Online) 8:41 ‚Äì 8:52 || NG41A-02: ClimSim-Online: A Large Multi-scale Dataset and Framework for Hybrid ML-Physics Climate Emulation AKSHAY SUBRAMANIAN (NVIDIA) with LEAP colleagues: Jerry Lin, Julius Busecke, Tian Zheng, Savannah Ferretti, Pierre Gentine, Stephan Mandt, Laure Zanna, Wayne Chuang, Sara Shamekh, Mike Pritchard Marquis 12-13 (Marriott Marquis) 13:40 ‚Äì 17:30 || A43H-2089: The Overlooked Sub-grid Air-Sea Flux in Climate Models JULIUS BUSECKE (Columbia) with LEAP colleagues: Dhruv Balwada, Prani Nalluri Hall B-C (Poster Hall) (Convention Center) 13:40-17:30 || NG43A-2292: Stable Machine-Learning Parameterization of Subgrid Processes with Real Geography and Full-physics Emulation ZEYUAN HU (Harvard) with LEAP colleagues: Jerry Lin, Mike Pritchard Hall B-C (Poster Hall) (Convention Center) 13:40-17:30 || NG43A-2294: Sampling Hybrid Climate Simulation at Scale to Reliably Improve Machine Learning Parameterization JERRY LIN (UC Irvine) with LEAP colleagues: Pierre Gentine, Mike Pritchard Hall B-C (Poster Hall) (Convention Center) 14:10 ‚Äì 14:20 || A43Q-01: Do We Need Parameterizations in the Era of Generative AI? (invited) PIERRE GENTINE (Columbia) with LEAP colleagues: Joseph Lockwood, Maggie Powell, Yongquan Qu, Juan Nathaniel, Stephan Mandt 154 A-B (Convention Center) 16:20 ‚Äì 16:30 || A44A-03: The Death of Autoconversion? KAITLYN LOFTUS (Columbia) with LEAP colleague: Marcus van Lier-Walqui 201 (Convention Center) 17:20 ‚Äì 17:30 || A44A-09: Exploring Parametric and Structural Uncertainties of Bulk and Lagrangian Ice Microphysical Schemes in Large Eddy Simulations JOSEPH KO (Columbia) with LEAP colleagues: Marcus van Lier-Walqui, Kara Lamb 201 (Convention Center) 18:00 ‚Äì 19:00 || TH45A: Idealized Modeling Within the Community Earth System Model ISLA SIMPSON (NCAR) with LEAP colleague: David Lawrence, moderator Marquis 12-13 (Marriot Marquis) FRIDAY || DEC. 13, 2024 8:00 ‚Äì 17:30 || NG01-06: Deep Generative Data Assimilation in Multimodal Setting JUAN NATHANIEL (Columbia) with LEAP colleagues: Yongquan Qu, Shuolin Li, Pierre Gentine iPoster Gallery (Online) 8:00 ‚Äì 17:30 || NG01-10: Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming YONGQUAN QU (Columbia) with LEAP colleague: Pierre Gentine iPoster Gallery (Online) 8:30 ‚Äì 12:20 || GC51X-0297: Improved Evaluation Methods for Earth System Models GABRIELE ACCARINO (Columbia) with LEAP colleagues: Viviana Acquaviva, Sara Shamekh Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || OS51B-0691: Using in-situ Data to Evaluate Estimated Labrador Sea pCO2 and CO2 Flux LAUREN MOSELEY (Columbia) with LEAP colleague: Galen McKinley Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || OS51B-0696: Climatological Implications of the LDEO-HPD pCO2 Method on CMIP6 Models CAROLINE ‚ÄúABBY‚Äù SHAUM (Columbia) with LEAP colleague: Galen McKinley Hall B-C (Poster Hall) (Convention Center) 8:30 ‚Äì 12:20 || OS51B-0711: Data Requirements for Quantifying the Background Ocean Carbon Sink THEA HEIMDAL (Columbia) with LEAP colleagues: Galen McKinley, Caroline ‚ÄúAbby‚Äù Shaum Hall B-C (Poster Hall) (Convention Center) 13:40 ‚Äì 17:30 || EP53D-1528: Parameterizing Suspended Sediment Flux in Vegetated Turbulent Channel Flows Using Machine Learning Models SHUOLIN LI (Columbia) with LEAP colleagues: Tian Zheng, Pierre Gentine Hall B-C (Poster Hall) (Convention Center) 14:45 ‚Äì 14:55 || OS53D-04: Tiny Ocean: Physically Meaningful Dimensionality Reduction for pCO2 Reconstruction VIVIANA ACQUAVIVA (Columbia) with LEAP colleagues: Thea Heimdal, Galen McKinley, Tian Zheng 156 (Convention Center) Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/year-4-annual-meeting-main/",
    "transcript": "Year 4 Annual Meeting - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP YEAR 4 || ANNUAL MEETING October 16 - 18, 2024 LEAP‚Äôs Year 4 Annual Meeting was held at the Edith Macy Center in Briarcliff Manor, NY. We offer a special thanks to the members of LEAP‚Äôs Junior Scientists Advisory Group (JSAG) for their close collaboration in shaping this year‚Äôs Meeting. And thank you, Annual Meeting participants and special guests, for a wonderful time spent together! Previous Next Agenda Annual Meeting Agenda (final) Meeting Resources Access to Presentations and Posters is password-protected. The password was provided to all participants in attendance at the start of Annual Meeting on Wednesday, October 16, 2024. Please contact Catherine Cha at cc102@columbia.edu with any questions. Presentations || Thursday, October 17, 2024 Presentations || Friday, October 18, 2024 Posters Edith Macy Center Edith Macy Meeting Rooms Map Edith Macy Center Guest Parking + Grounds Map Engage + Connect List of Participants Social Media: Slack Twitter LinkedIn Hashtags: #LEAPSTC #LEAPAnnMtg24 #LEAPEducation #LEAPKT #LEAPResearch #LEAPDEI Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/year-4-annual-meeting/",
    "transcript": "YEAR 4 ANNUAL MEETING - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP YEAR 4 || ANNUAL MEETING October 16 - 18, 2024 LEAP‚Äôs Year 4 Annual Meeting was held at the Edith Macy Center in Briarcliff Manor, NY. We offer a special thanks to the members of LEAP‚Äôs Junior Scientists Advisory Group (JSAG) for their close collaboration in shaping this year‚Äôs Meeting. And thank you, Annual Meeting participants and special guests, for a wonderful time spent together! Previous Next Agenda Annual Meeting Agenda (final) Meeting Resources Access to Presentations and Posters is password-protected. The password was provided to all participants in attendance at the start of Annual Meeting on Wednesday, October 16, 2024. Please contact Catherine Cha at cc102@columbia.edu with any questions. Presentations || Thursday, October 17, 2024 Presentations || Friday, October 18, 2024 Posters Edith Macy Center Edith Macy Meeting Rooms Map Edith Macy Center Guest Parking + Grounds Map Engage + Connect List of Participants Social Media: Slack Twitter LinkedIn Hashtags: #LEAPSTC #LEAPAnnMtg24 #LEAPEducation #LEAPKT #LEAPResearch #LEAPDEI Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/year-4-annual-meeting-main/yr4-annmtg-day1/",
    "transcript": "Protected: YEAR 4 ANNUAL MEETING: DAY 1 - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP This content is password protected. To view it please enter your password below: Password: Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/year-4-annual-meeting-main/yr4-annmtg-day2/",
    "transcript": "Protected: YEAR 4 ANNUAL MEETING: DAY 2 - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP This content is password protected. To view it please enter your password below: Password: Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/year-4-annual-meeting-main/yr4-annmtg-posters/",
    "transcript": "Protected: YEAR 4 ANNUAL MEETING: POSTERS - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP This content is password protected. To view it please enter your password below: Password: Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/pastevents/",
    "transcript": "Past Events - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Past Events Fall 2024 Fall Lecture in Climate Data Science: Research Update Thursday || November 21, 2024 || 12:00pm ARVIND RENGANATHAN (Univ. of Minnesota) ‚ÄúAdvancing Environmental Modeling with Knowledge-Guided Encoder-Decoder Frameworks‚Äù YouTube link forthcoming Fall Lecture in Climate Data Science Thursday || November 14, 2024 || 12:00pm JANNI YUVAL (Google Research) ‚ÄúNeural General Circulation Models for Weather + Climate‚Äù YouTube link forthcoming Fall Lecture in Climate Data Science: Research Update Thursday || November 7, 2024 || 12:00pm THEA HEIMDAL (Columbia) + ABBY SHAUM (Columbia) ‚ÄúUsing CMIP6 Models to Better Understand the Ocean Carbon Sink‚Äù YouTube link forthcoming Fall Lecture in Climate Data Science Thursday || October 31, 2024 || 12:00pm ROBERT PINCUS (Columbia) ‚ÄúShaping the Future of Data, Computation, and Collaboration at LEAP‚Äù YouTube link Fall Lecture in Climate Data Science: Research Update Thursday || October 24, 2024 || 12:00pm SHAWN LI (Columbia) ‚ÄúA Distribution-Driven Probabilistic Data Assimilation Framework for Climate Science‚Äù YouTube link Fall Lecture in Climate Data Science Thursday || October 10, 2024 || 12:00pm BIN YU (Berkeley) ‚ÄúInterpreting Deep Learning Models‚Äù YouTube link Fall Lecture in Climate Data Science: Research Update Thursday || October 3, 2024 || 12:00pm MAGGIE POWELL (Columbia) ‚ÄúLeveraging Subgrid-Scale Spatial Organization and Variability for Improved Cloud Fraction Parameterization Using PINACLES Simulations‚Äù YouTube link Fall Lecture in Climate Data Science: Climate Week at Columbia Engineering Thursday || September 26, 2024 || 12:00pm ‚ÄúClimate Change and Flood Risk: A Panel‚Äù Panelists : George Deodatis (Vice Dean of Research for Columbia Engineering, Santiago and Robertina Calatrava Family Professor of Civil Engineering and Professor of Earth and Environmental Engineering, Columbia University) Candace Agonafir (Postdoctoral Research Scientist, Columbia Water Center) Fatoumata Camara (SP^2 Science Pathways Scholar, Barnard College) Adam Nayak (PhD Student, Earth and Environmental Engineering, Columbia University) Facilitator : Savannah Thais (Associate Research Scientist, Data Science Institute, Columbia University) YouTube link Fall Lecture in Climate Data Science Thursday || September 19, 2024 || 12:00pm BRIAN MEDEIROS (NCAR) ‚ÄúHops, Skips, and Jumps: Development Activities Around CAM7‚Äù YouTube link Fall Lecture in Climate Data Science Thursday || September 12, 2024 || 12:00pm ANDRE PERKINS (Allen Institute for AI) ‚ÄúACEv2 and the New Frontier of AI-Based Emulators for Climate Simulation‚Äù YouTube link Fall Lecture in Climate Data Science: Research Update Thursday || September 5, 2024 || 12:00pm JULIA SIMPSON (Columbia) ‚ÄúImproving Parameterizations of Heat + Momentum Air-Sea Fluxes‚Äù YouTube link Summer 2024 Summer Lecture in Climate Data Science Thursday || August 1, 2024 || 12:00pm GENEVA LIST (Columbia): ‚ÄúLandscape Assessment of AI for Climate and Nature‚Äù YouTube link Summer Lecture in Climate Data Science Thursday || July 25, 2024 || 12:00pm MARCUS van LIER-WALQUI (Columbia): ‚ÄúData-Driven Constraint of Cloud Microphysics Uncertainty at Global and Process-Level Scales‚Äù YouTube link Summer Lecture in Climate Data Science Thursday || July 18, 2024 || 12:00pm CHAD SMALL (Univ. of Washington): ‚ÄúThe Art (and Science) of Science Communication for Earth Scientists‚Äù YouTube link 2024 Summer Institute: Integrating Climate Education in NYC Public Schools Monday || July 15, 2024 ‚Äì Friday || July 19, 2024 Summer Lecture in Climate Data Science Thursday || July 11, 2024 || 12:00pm JOSH DeVINCENZO (NCDP / Columbia): ‚ÄúClimate, Mental Models, and Data for Disaster Preparedness‚Äù YouTube link Summer Lecture in Climate Data Science Thursday || June 13, 2024 || 3:00pm COURTNEY COGBURN (Columbia): ‚ÄúClimate, Science, and Social Justice‚Äù Summer Lecture in Climate Data Science Thursday || May 30, 2024 || 12:00pm JULIUS BUSECKE (Columbia) + TIM HERMANS (Utrecht): ‚ÄúProjecting Changes in the Drivers of Compound Flooding in Europe Using CMIP6 Models‚Äù 2024 Summer Education Programs: REU + Momentum Fellowship May 27 ‚Äì August 2, 2024 RESEARCH PRESENTATIONS Project 1: ‚ÄúQuantifying Epistemic + Aleatoric Uncertainty in Climate Data‚Äù ‚Äì Jimeng Shi (Momentum Fellow); Omar Abdel Azim, Mia Montrose (REU) Project 2: ‚ÄúUnderstanding + Modeling Turbulent Flow in the Atmosphere‚Äù ‚Äì Antony Sikorski (Momentum Fellow); Laura Pong, Greta Van Zetten (REU) Project 3: ‚ÄúAnalysis of Climate Model Perturbed Physics Ensembles‚Äù ‚Äì Kyle McEvoy (Momentum Fellow); Nicoline Joenson, Anthony Guzman (REU) Project 4: ‚ÄúUnderstanding Ice Crystal Growth + Evolution in the Atmosphere‚Äù ‚Äì Obin Sturm (Momentum Fellow); Patrick Dicus, Ashley Nguyen (REU) Project 5: ‚ÄúUnderstanding + Modeling the Impact of Air-Sea Heterogeneity on Surface Fluxes‚Äù ‚Äì Prani Nalluri (Momentum Fellow); Jack Alfandre, Tony Manriquez (REU) Spring 2024 Spring Lecture in Climate Data Science: Research Updates Thursday || May 9, 2024 || 12:00pm YONGQUAN QU (Columbia): ‚ÄúJoint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming‚Äù JIARONG WU (NYU): ‚ÄúData-driven probabilistic air-sea flux model using in-situ direct measurements‚Äù Spring Lecture in Climate Data Science Thursday || April 25, 2024 || 3:00pm IGNACIO LOPEZ-GOMEZ (Google): ‚ÄúGenerative Emulation of Weather Forecast Ensembles with Diffusion Models‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || April 25, 2024 || 12:00pm CHAD SMALL (Univ. of Washington): ‚ÄúMadden-Julian Oscillation and Atmospheric Rivers: Toward a New Paradigm for S2S Forecast of High-Impact Weather Extremes‚Äù YouTube link Spring Lecture in Climate Data Science: Research Updates Thursday || April 11, 2024 || 12:00pm AYA LAHLOU (Columbia): ‚ÄúModeling phenology under climate change using deep learning‚Äù DION HO (Columbia): ‚ÄúData science and physical modeling of energy flows in the atmosphere‚Äù Spring Lecture in Climate Data Science Thursday || March 26, 2024 || 12:00pm ANDREW GETTELMAN (PNNL): ‚Äú‚ÄòMachine‚Äô Learning Cloud Physics: Where Do We Stand?‚Äù YouTube link Spring Lecture in Climate Data Science: Research Updates Thursday || March 14, 2024 || 12:00pm FLORENCIO PORTOCARRERO (Columbia Business School): ‚ÄúCommunication Frames and Actors‚Äô Responses to Sustainability- and Climate-related Information‚Äù YouTube link JAEYOUNG JUNG (Columbia): ‚ÄúA Multiscale Framework for Airflow-Canopy Interaction‚Äù YouTube link LEAP ML Climate Journal Club Thursday || March 7, 2024 || 12:00pm OBIN STURM (USC): ‚ÄúConservation Laws in a Neural Network Architecture: A Flux-based Framework for Physically Consistent Hard Constraints in Machine Learning Models‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || February 29, 2024 || 12:00pm RYAN ABERNATHY (Columbia): ‚ÄúThe Future of Earth-System Data Infrastructure‚Äù YouTube link Spring Lecture in Climate Data Science: Research Updates Thursday || February 15, 2024 || 12:00pm SUNGDUK YU (UC Irvine): ‚ÄúClimSim2: Coupling Global Climate Models with Neural Network Cloud Emulators‚Äù YouTube link QINGYUAN YANG (Columbia): ‚ÄúFlexible Use of Additive Gaussian Processes as a Powerful Tool for More Interpretable Analysis and Emulation of Climate Model PPEs‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || February 1, 2024 || 12:00pm CARL VONDRICK (Columbia): ‚ÄúMultimodal Learning from Pixels to People‚Äù YouTube link LEAP ML Climate Journal Club Friday || January 26, 2024 || 11:00am SYLWESTER ARABAS (Univ. of Krakow): ‚ÄúPyPartMC Aerosol Dynamics Package: Engineering Python-to-Fortran Bindings in C++ for Use in Julia and Matlab‚Äù YouTube link Spring Lecture in Climate Data Science: Research Updates Thursday || January 18, 2024 || 12:00pm MOHAMMED AZIZ BHOURI (Columbia): ‚ÄúMulti-fidelity climate model parameterization for extrapolation beyond training data‚Äù YouTube link KAITLYN LOFTUS (Columbia): ‚ÄúParameterizing cloud microphysics with machine learning-enabled Bayesian parameter influence‚Äù YouTube link 2024 Winter Momentum Bootcamp Wednesday || January 10, 2024 ‚Äì Thursday || January 11, 2024 Details Fall 2023 Fall Lecture in Climate Data Science Thursday || November 16, 2023 || 3:30pm MARIANA CLARE (ECMWF): ‚ÄúThe Rise of Machine Learning in Weather Forecasting‚Äù Fall Lecture in Climate Data Science: Research Updates Thursday || November 16, 2023 || 3:30pm CANDACE AGONAFIR (Columbia): ‚ÄúRecent Advances in Urban Flooding Research‚Äù CHRISTINA TORRES / NOA URBACH (Teachers College): ‚ÄúNew York City Educators‚Äô Perceptions of Students‚Äô Engagement with Climate Change‚Äù LEAP ML Climate Journal Club Thursday || October 26, 2023 || 3:30pm BLANKA BALOGH (M√©t√©o-France / Columbia): ‚ÄúCalibrating a Dynamical System with Neural Network Based Physics‚Äù Fall Lecture in Climate Data Science Thursday || November 2, 2023 || 3:30pm ADITYA GROVER (UCLA): ‚ÄúClimaX: A Foundation Model for Weather and Climate‚Äù YouTube Link LEAP ML Climate Journal Club Thursday || October 26, 2023 || 3:30pm PAULA HARDER (Fraunhofer ITWM): ‚ÄúPhysics-Constrained Deep Learning for Downscaling and Emulation‚Äù YouTube link LEAP Year 3 Annual Meeting Sunday || October 15, 2023 ‚Äì Tuesday || October 17, 2023 Details LEAP ML Climate Journal Club Thursday || October 12, 2023 || 3:30pm EMILIANO D√çAZ (Univ. of Valencia): ‚ÄúIdentifying the Causes of Pyrocumulonimbus (PyroCb)‚Äù Fall Lecture in Climate Data Science Thursday || October 5, 2023 || 3:30pm JIAN PEI (Duke): ‚ÄúData Valuation in Federated Learning‚Äù YouTube link LEAP ML Climate Journal Club Thursday || September 28, 2023 || 3:30pm LILY-BELLE SWEET (UFZ): ‚ÄúCross-validation Strategy Impacts the Performance and Interpretation of Machine Learning Models‚Äù YouTube link Fall Lecture in Climate Data Science Thursday || September 21, 2023 || 3:30pm PO-LUN MA (PNNL): ‚ÄúImproving Aerosol in an Earth System Model with AI / ML‚Äù YouTube link Fall Lecture in Climate Data Science: Research Updates Thursday || September 7, 2023 || 3:30pm LINNIA HAWKINS (NCAR/Columbia): ‚ÄúCommunity Land Model Parameter Estimation‚Äù YouTube link JOSEPH KO (Columbia): ‚Äú3D Reconstruction + Unsupervised Clustering of Ice Crystal Particles to¬† Inform Depositional Growth Parameterizations‚Äù YouTube link Summer 2023 LEAP ML Journal Club Thursday || August 31, 2023 || 3:30pm KARAN JAKHAR (Rice): ‚ÄúLearning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges‚Äù LEAP ML Journal Club Thursday || August 17, 2023 || 3:30pm TAKUYA KURIHANA (Univ. of Chicago): ‚ÄúSelf-supervised Learning-based Applications Toward Democratization of Multi-decades of MODIS Cloud Images‚Äù 2023 REU Research Presentations + Closing Reception Thursday || July 27, 2023 || 1:00pm 2023 SUMMER REU PRESENTATIONS Samarth Agrawal: ‚ÄúCapturing Convective Atmospheric Profiles Using Variational Encoder-Decoders‚Äù Thomas Chen: ‚ÄúVisualizing Interpretability for Precipitation Prediction Using Shapley Additive Explanations‚Äù Mark Irby-Gill: ‚ÄúEvaluating XGBoost as a Baseline Model for Spatially-Informed Precipitation Predictions‚Äù Rebecca Porter: ‚ÄúExploring Data-driven Equation Discovery to Model Moisture Flux‚Äù Amanda Sun: ‚ÄúImproving Subgrid-Parameterization with Causal Discovery‚Äù Subashree Venkatasubramanian: ‚ÄúA Bayesian-Gamma Deep Learning Approach to Capture Heavy-Tailed Behavior in the ClimSim Dataset‚Äù 2023 Summer Institute: Integrating Climate Education in NYC Public Schools Monday ‚Äì Thursday || July 17-20, 2023 2023 Summer Momentum Fellowship June 5 ‚Äì August 11, 2023 2023 SUMMER MOMENTUM FELLOWS: Matthew Beveridge, Mohammad Erfani, Yu Huang 2023 SUMMER MOMENTUM FELLOWSHIP FACULTY: Dhruv Balwada, Kara Lamb, Pierre Gentine, Carl Vondrick 2023 Summer REU Program June 5 ‚Äì July 29, 2023 2023 SUMMER REU COHORT: Samarth Agrawal, Thomas Chen, Mark Irby-Gill, Rebecca Porter, Amanda Sun, Subashree Venkatasubramanian MOMENTUM BOOTCAMP INSTRUCTORS: Candace Agonafir, Yu Huang, Tian Zheng IN-PERSON RESEARCH MENTORS: Pierre Gentine, Stephan Mandt, Mike Pritchard Special Presentation Thursday || June 15, 2023 || 1:00pm REBECCA GJINI (UC San Diego): ‚ÄúMapping Meteorological Conditions to Predator Prey Dynamics‚Äù Spring 2023 LEAP ML Journal Club Thursday || May 25, 2023 || 12:00pm ROMANA BOIGER + ROB MODINI (PSI): ‚ÄúRetrieval of Aerosol Properties from in-situ, Multi-angle Light Scattering Measurements Using Invertible Neural Networks‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || May 25, 2023 || 3:30pm STEPHAN MANDT (UC Irvine): ‚ÄúFrom Compression to Convection: a Latent Variable Perspective‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || May 18, 2023 || 3:30pm MIKE PRITCHARD (UC Irvine / NVIDIA): ‚ÄúAdventures in Physics / AI Climate Modeling and Full AI Weather Prediction‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || May 4, 2023 || 3:30pm ADJI BOUSSO DIENG (Princeton): ‚ÄúMeasuring and Enforcing Diversity in Machine Learning‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || April 27, 2023 || 3:30pm DAVE LAWRENCE (NCAR): ‚ÄúParameter Estimation for Improved Capacity of the Community Land Model for Actionable Science‚Äù YouTube link LEAP ML Climate Journal Club Thursday || April 20, 2023 || 12:00pm PETER JAN VAN LEEUWEN (Colorado State): ‚ÄúUncertainty Quantification in ML and Connections with Data Assimilation‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || April 13, 2023 || 3:30pm CHRIS BRETHERTON (Allen Institute for AI): ‚ÄúCorrective Machine Learning for Interpretable Improvement of Climate Models‚Äù YouTube link LEAP ML Climate Journal Club Thursday || April 6, 2023 || 12:00pm PRAKHAR SRIVASTAVA (UC Irvine): ‚ÄúDenoising Diffusion Probabilistic Models‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || March 30, 2023 || 3:30pm ROSE YU (UC San Diego): ‚ÄúPhysics-Guided Deep Learning for Climate Dynamics‚Äù YouTube link LEAP ML Climate Journal Club Thursday || March 9, 2023 || 12:00pm SAM SILVA (USC): ‚ÄúEmulating Cloud Droplets in a Climate Model‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || March 2, 2023 || 3:00pm HOD LIPSON (Columbia): ‚ÄúAutomating Discovery: From Cognitive Robotics to Particle Physics‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || February 16, 2023 || 3:00pm JEAN-NO√â LANDRY (Concordia Univ., Montr√©al): ‚ÄúClimate Knowledge Transfer in a Time of Crises, Transitions, and New Collaborations‚Äù YouTube link AI FOR GOOD PROGRAMME Wednesday || February 15, 2023 || 11:00am PIERRE GENTINE (Columbia): ‚ÄúPhysics to Machine Learning, and Machine Learning Back to Physics‚Äù LEAP ML Climate Journal Club Thursday || February 9, 2023 || 12:00pm KEVIN XIA (Columbia): ‚ÄúThe Causal-Neural Connection: Expressiveness, Learnability, and Inference‚Äù YouTube link Spring Lecture in Climate Data Science Thursday || February 2, 2023 || 3:00pm PIERRE GENTINE (Columbia): ‚ÄúPhysics to Machine Learning, and Machine Learning Back to Physics‚Äù YouTube link LEAP ML Climate Journal Club Thursday || January 26, 2023 || 12:00pm AUR√âLIEN RIBES (CNRM / M√©t√©o-France): ‚ÄúAn Adaptation of Kalman Filtering for Climate Change‚Äù YouTube link LEAP Year 2 Annual Meeting Monday || January 16, 2023 ‚Äì Wednesday || January 18, 2023 Day 1 Day 2 LEAP ML Climate Journal Club Thursday || January 12, 2023 || 12:00pm BLANKA BALOGH (CNRM, Universit√© de Toulouse M√©t√©o-France): ‚ÄúHow to Calibrate a Dynamical System with Neural Network Based Physics‚Äù YouTube link 2023 Winter Momentum Bootcamp Thursday || January 5, 2023 ‚Äì Friday || January 6, 2023 Day 1 Day Fall 2022 Fall Lecture in Climate Data Science Thursday || December 8, 2022 || 3:00pm JAKOB RUNGE (German Aerospace Center / TU Berlin): ‚ÄúCausal Inference, Causal Discovery, and Machine Learning‚Äù YouTube link LEAP ML Climate Journal Club Thursday || December 8, 2022 || 12:00pm SARA SHAMEKH (Columbia): ‚ÄúImplicit Learning of Convective Organization Explains Precipitation Stochasticity‚Äù YouTube link CAIT Distinguished Lecture Wednesday || December 7, 2022 || 12:00pm PIERRE GENTINE (Columbia): ‚ÄúPhysics to Machine Learning and Machine Learning Back to Physics‚Äù YouTube link Fall Lecture in Climate Data Science Thursday || November 17, 2022 || 3:00pm PEDRAM HASSANZADEH (Rice): ‚ÄúLearning Data-driven Subgrid-scale Models for Geophysical Turbulence‚Äù YouTube link LEAP ML Climate Journal Club Thursday || November 10, 2022 || 12:00pm JOSEPH BAKARJI (Univ. of Washington): ‚ÄúDimensionally Consistent Learning with Buckingham Pi‚Äù YouTube link Fall Lecture in Climate Data Science Thursday || November 3, 2022 || 3:00pm ** THIS LECTURE WAS POSTPONED AND WILL BE RESCHEDULED ** MIKE PRITCHARD (UC Irvine): ‚ÄúAdventures in Hybrid Physics: Machine Learning for Multi-scale Climate Modeling, AI-assisted Climate Model Inter-comparison, and ‚Ä¶ NVIDIA‚Äù SOARS: an Example of Research, Mentoring, and Community Thursday || November 3, 2022 || 11:00am KADIDIA THIERO (SOARS Principal Investigator / Program Lead) Fall Lecture in Climate Data Science Thursday || October 20, 2022 || 3:00pm ELIZABETH BARNES (Colorado State Univ.): ‚ÄúExplainable AI for Climate Science: Detection, Prediction, and Discovery‚Äù YouTube link LEAP ML Climate Journal Club Thursday || October 13, 2022 || 12:00pm GRIFFIN MOOERS (UC Irvine): ‚ÄúComparing Storm Resolving Models and Climates via Unsupervised Machine Learning‚Äù YouTube link Fall Lecture in Climate Data Science Thursday || October 6, 2022 || 3:00pm VIPIN KUMAR (Univ. of Minnesota): ‚ÄúInverse Modeling via Knowledge-Guided Self-Supervised Learning: An Application in Hydrology‚Äù YouTube link Fall Lecture in Climate Data Science Thursday || September 22, 2022 || 3:00pm LAURE ZANNA (NYU): ‚ÄúMachine Learning for Ocean + Climate Modeling: Advances, Challenges, and Outlook‚Äù YouTube link LEAP ML Climate Journal Club Monday || September 19, 2022 || 12:00pm ALBAN FARCHI (CEREA): ‚ÄúUsing Machine Learning to Correct Model Error in Data Assimilation and Forecast Applications‚Äù Fall Lecture in Climate Data Science Thursday || September 8, 2022 || 3:00pm DAVID ROLNICK (McGill Univ.): ‚ÄúMachine Learning in Climate Change Mitigation and Adaptation‚Äù YouTube link Summer 2022 LEAP ML Climate Journal Club Monday || August 22, 2022 || 12:00-1:00pm LEAP ML Climate Journal Club Monday || August 8, 2022 || 12:00-1:00 pm Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-internal/",
    "transcript": "Internal - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Internal Resources Below, please find a collection of frequently accessed internal resources. If you‚Äôre looking for something and can‚Äôt find it, please reach out to Molly Lopez , Managing Director. People @ LEAP Onboarding Handbook Ethics Handbook Center Documents LEAP Community Calendar LEAP Style Guide LEAP Pangeo Professional Development Reimbursements LEAP Travel Reimbursement Columbia University Policies Meeting Minutes Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-internal/internal-sub/",
    "transcript": "Internal Resources - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Internal Resources Below, please find a collection of frequently accessed internal resources. If you‚Äôre looking for something and can‚Äôt find it, please reach out to Molly Lopez , Managing Director. People @ LEAP Onboarding Handbook Ethics Handbook Center Documents LEAP Community Calendar LEAP Style Guide LEAP Pangeo Professional Development Reimbursements LEAP Travel Reimbursement Columbia University Policies Meeting Minutes Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/leap-internal/year-3-nsf-site-visit/",
    "transcript": "Protected: Year 3 NSF Site Visit - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP LEAP Year 3 NSF Site Visit This page is password-protected. Please enter the password to access the site. Enter your password Enter Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/subscribe/",
    "transcript": "Subscribe - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP We Want to Hear From You! Subscribe to LEAP Thank you for your interest in LEAP! Please sign up below to receive our news, event invitations, and updates. We‚Äôre looking forward to a great semester! Subscribe * indicates required Email Address * First Name * Last Name * GitHub Username * Please indicate \"N/A\" if you don't have one. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/support-leap-3/",
    "transcript": "Support LEAP - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Support LEAP MAKE YOUR GIFT LEAP‚Äôs multi-institutional, trans-disciplinary research community of leading scientists addresses the fast-evolving and complex challenges in climate modeling, climate projection, and climate adaptation. LEAP has a vision to merge physical models with machine learning, and sync the work of climate scientists and data scientists, in order to stimulate convergence between the two disciplines and develop new machine learning algorithms for studying and solving interesting, data-intensive problems. We are also creating a cloud platform, LEAP Pangeo, to provide climate data more widely, and engage with our colleagues in the field and beyond to see what is actually useful. Our Center is focused on ensuring that everyone ‚Äî from policy makers to business leaders to educators ‚Äî has access to accurate climate projections to inform their own decision-making. It is critical that we share this information with the public and private sectors in a user-friendly way. LEAP uses gifts to drive our main thrusts: Bi-directional Knowledge Transfer Convergent Research Education + Broad Participation by adhering to and infusing our Core Values throughout: Innovation Legacy Impact Your gift will help LEAP revolutionize climate projections for informed climate adaptation so that a large and broad range of public and private stakeholders will have the required tools necessary for informed decision making when facing climate change. MAKE YOUR GIFT Join LEAP Interested in joining LEAP and helping to develop the next generation of climate modeling and climate projection for tailored adaptation? Browse opportunities to be part of the LEAP Center! Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/nyc-summer-institute/",
    "transcript": "2023 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2023 Summer Institute: Integrating Climate Education in NYC Public Schools from July 17 ‚Äì 20, 2023 . A vital part of LEAP‚Äôs education programming, the 2023 Summer Institute constitutes four full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. The 2023 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. APPLICATIONS CLOSED Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of K-5 teachers ** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2023 Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. ** Subsequent Summer Institutes will focus on preparing middle and high school teachers. Application, Dates, and Other Details K-5 teachers throughout NYC are eligible to apply to the 2023 Summer Institute. Application deadline : May 1, 2023 Decisions released by : May 15, 2023 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/nyc-summer-institute/#content",
    "transcript": "2023 NYC Summer Institute - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 NYC Summer Institute What if, in just one week, you could develop a better understanding of climate change, become a more confident educator, build a network of wonderful colleagues from around New York City, and return to your school with new ideas about how to engage students with climate change? Integrating Climate Education in NYC Public Schools The Center for Sustainable Futures at Teachers College , Columbia University , and NYC Public Schools Office of Sustainability are partnering to present the 2023 Summer Institute: Integrating Climate Education in NYC Public Schools from July 17 ‚Äì 20, 2023 . A vital part of LEAP‚Äôs education programming, the 2023 Summer Institute constitutes four full days of professional learning, bringing together faculty, doctoral students, expert teachers, and leaders from the NYC Department of Education to create a learning community that will explore, make sense of, and provide tangible support for incorporating climate change across different subject areas. The 2023 Summer Institute is free. Participants will receive a $1,000 stipend, and may earn up to 28 CTLE credits. APPLICATIONS CLOSED Summer Institute Goals As the need and demand grows for the integration of climate change education in schools, LEAP‚Äôs Summer Institute will help catalyze the preparation of K-5 teachers ** to develop and lead lessons aligned with standards- and place-based learning. The goals of the 2023 Summer Institute are: To develop new knowledge and understanding of climate change among teachers. To empower teachers to engage their students with climate change across different subject areas, including: English Language Arts (ELA), Math, Social Studies, Arts, and Science. To co-design innovative teaching materials about climate change in New York City. To establish a learning community of teachers from different schools and neighborhoods. ** Subsequent Summer Institutes will focus on preparing middle and high school teachers. Application, Dates, and Other Details K-5 teachers throughout NYC are eligible to apply to the 2023 Summer Institute. Application deadline : May 1, 2023 Decisions released by : May 15, 2023 Early decisions are available upon request Questions? Please contact the Summer Institute team . To request disability-related accommodations, please complete the Accommodations Request Form or contact the Office of Access and Services for Individuals with Disabilities (OASID) [phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/2023-leap-summer-reu-program/",
    "transcript": "2023 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 LEAP Summer REU Program Overview LEAP‚Äôs 2023 REU program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with¬†the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. We invite all students to apply. 2023 Summer REU Cohort, with instructors Tian Zheng, Candace Agonafir, and Yu Huang Program The 3-week Momentum Bootcamp (online, June 5‚Äì23, 2023) will be led by: Tian Zheng (Chief Convergence Officer & Education Director, LEAP; Professor & Chair of the Department of Statistics, Columbia University; Affiliate Member of the Data Science Institute) Candace Agonafir (Postdoc, Depts. of Data Science + Civil Engineering, Columbia University) Yu Huang (PhD Student, Dept. of Earth + Environmental Engineering, Columbia University) The Summer 2023 Research Experience (in-person, June 25 ‚Äì July 29, 2023) will be led by: Pierre Gentine (Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University) Mike Pritchard (Institutional Integration Director, LEAP; Associate Professor of Earth Systems Science, University of California at Irvine) Stephan Mandt (Associate Professor of Computer Science and Statistics, University of California at Irvine). 2023 REU Participants Sammy Agrawal (Massachusetts) Computer Science, Columbia University Thomas Chen (New Jersey) Computer Science, Columbia University Mark Irby-Gill (New York) Geological Engineering, Red Rocks Community College Rebecca Porter (Kansas) Interdisciplinary Studies, University of Saint Mary Amanda Sun (California) Computer Science + Environmental Studies, Dartmouth College Subashree Venkatasubramanian (Washington) Computer Science, Columbia University Project Scope Topic : Machine learning (ML) to break deadlock in achieving climate simulations with high-resolution physics. Short Description: Climate model predictions are full of uncertainty. Some of it is because processes like cloud formation are crudely represented simply because they are too computationally intensive to model explicitly. Multi-scale climate models that embed small realizations of these explicit physics provide an opportunity to sidestep Moore‚Äôs law, by learning these physics with machine learning models. Once trained, the machine learning models can be coupled to the climate predictions, with fast inference allowing high-resolution physics in climate simulations ahead of schedule. An open research challenge is finding reproducible, reliable ways to achieve performant ML workflows in situations of real-world operational complexity. In this context, LEAP has innovated a ML training data set harvested from a state-of-the-art climate simulator, which exposes REU students to the relevant pipeline issues in a real-world research setting at the frontier of climate science and data science. Learning Outcomes: Familiarity with datasets used in modern climate simulations, baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Potential Continuation of Research After REU: Successful or standout fits could be considered for follow-on work that goes to the next step of coupling well-performing architectures to global climate simulators. The resulting ‚Äúhybrid‚Äù (ML + physics) climate simulators may exhibit interesting pathologies or physically desirable behaviors, either of which provide¬† opportunities for collaborative analysis of climate prediction output. Important Dates March 10, 2023: REU Application deadline March 31, 2023: REU Application decision notification June 5 ‚Äì June 23, 2023: Momentum Bootcamp (online) June 25, 2023: Move in to Columbia University campus June 26 ‚Äì July 28, 2023: In-Person Research Experience July 29, 2023: Move out Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/2023-leap-summer-reu-program/#content",
    "transcript": "2023 LEAP Summer REU Program - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 LEAP Summer REU Program Overview LEAP‚Äôs 2023 REU program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The REU Program is presented in partnership with¬†the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR . The Program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. We invite all students to apply. 2023 Summer REU Cohort, with instructors Tian Zheng, Candace Agonafir, and Yu Huang Program The 3-week Momentum Bootcamp (online, June 5‚Äì23, 2023) will be led by: Tian Zheng (Chief Convergence Officer & Education Director, LEAP; Professor & Chair of the Department of Statistics, Columbia University; Affiliate Member of the Data Science Institute) Candace Agonafir (Postdoc, Depts. of Data Science + Civil Engineering, Columbia University) Yu Huang (PhD Student, Dept. of Earth + Environmental Engineering, Columbia University) The Summer 2023 Research Experience (in-person, June 25 ‚Äì July 29, 2023) will be led by: Pierre Gentine (Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University) Mike Pritchard (Institutional Integration Director, LEAP; Associate Professor of Earth Systems Science, University of California at Irvine) Stephan Mandt (Associate Professor of Computer Science and Statistics, University of California at Irvine). 2023 REU Participants Sammy Agrawal (Massachusetts) Computer Science, Columbia University Thomas Chen (New Jersey) Computer Science, Columbia University Mark Irby-Gill (New York) Geological Engineering, Red Rocks Community College Rebecca Porter (Kansas) Interdisciplinary Studies, University of Saint Mary Amanda Sun (California) Computer Science + Environmental Studies, Dartmouth College Subashree Venkatasubramanian (Washington) Computer Science, Columbia University Project Scope Topic : Machine learning (ML) to break deadlock in achieving climate simulations with high-resolution physics. Short Description: Climate model predictions are full of uncertainty. Some of it is because processes like cloud formation are crudely represented simply because they are too computationally intensive to model explicitly. Multi-scale climate models that embed small realizations of these explicit physics provide an opportunity to sidestep Moore‚Äôs law, by learning these physics with machine learning models. Once trained, the machine learning models can be coupled to the climate predictions, with fast inference allowing high-resolution physics in climate simulations ahead of schedule. An open research challenge is finding reproducible, reliable ways to achieve performant ML workflows in situations of real-world operational complexity. In this context, LEAP has innovated a ML training data set harvested from a state-of-the-art climate simulator, which exposes REU students to the relevant pipeline issues in a real-world research setting at the frontier of climate science and data science. Learning Outcomes: Familiarity with datasets used in modern climate simulations, baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Potential Continuation of Research After REU: Successful or standout fits could be considered for follow-on work that goes to the next step of coupling well-performing architectures to global climate simulators. The resulting ‚Äúhybrid‚Äù (ML + physics) climate simulators may exhibit interesting pathologies or physically desirable behaviors, either of which provide¬† opportunities for collaborative analysis of climate prediction output. Important Dates March 10, 2023: REU Application deadline March 31, 2023: REU Application decision notification June 5 ‚Äì June 23, 2023: Momentum Bootcamp (online) June 25, 2023: Move in to Columbia University campus June 26 ‚Äì July 28, 2023: In-Person Research Experience July 29, 2023: Move out Resources Campus information Housing Shared facilities Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/02/REU-2023_-FAQs.pdf",
    "transcript": "Error fetching PDF: 404"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/leap-summer-reu-program/",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/leap-summer-reu-program/#content",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/02/2024-MomFell-Project-Descriptions.pdf",
    "transcript": "\nPROJECT 1\nQUANTIFYING EPISTEMIC + ALEATORIC UNCERTAINTY IN CLIMATE DATA\nThe Team /  MentorsStephan MandtMike Pritchard\nJerry Lin\nNew York City / Remote\nEliot Wong-Toi\nMike Pritchard\nNew York City / Remote\nEliot Wong-Toi\n\nPROJECT 1\nQUANTIFYING EPISTEMIC + ALEATORIC UNCERTAINTY IN CLIMATE DATA\nThis project delves into uncertainty quantification in machine learning for climate, distinguishing between model limitations and intrinsic data noise. We pose a pivotal question: in data-driven weather and climate projections, which uncertainty dominates? Using the acclaimed ClimSim dataset (NeurIPS 2023 best paper award) coming from an atmospheric model, we seek to understand how prediction errors scale as we increase the data set size and model complexities. Beyond better predictions, our mission is to empower machine learning models with improved calibration performance‚Äîenabling them to accurately gauge uncertainty levels and \"know what they don't know.\" In this interdisciplinary project between climate science and statistics, you'll dive into probabilistic machine learning concepts like Bayesian neural networks, deep ensembles, Laplace approximations, and variational inference and learn to apply them in a practical climate modeling context.\nLearning Outcomes + Deliverables\npractical experience in advanced machine learning applications\nunderstanding the intersection of climate science + data science\ncontribution to ongoing research projects\ncontribution to ongoing research projects\n\nPROJECT 2\nUNDERSTANDING + MODELING TURBULENT FLOW IN THE ATMOSPHERE\nThe Team /  MentorsSara Shamekh\nYongquan Qu\nNew York City \nNew York City \nYongquan Qu\n\nPROJECT 2\nUNDERSTANDING + MODELING TURBULENT FLOW IN THE ATMOSPHERE\nTurbulent flux, a key concept in atmospheric science and fluid dynamics, describes the transfer of properties like heat, momentum, or moisture through turbulent flow in the atmosphere. This flow is characterized by chaotic, swirling movements of air or fluid. In the study of these phenomena, particularly in large-scale models like weather or climate simulations, the term 'unresolved turbulent flow' often arises. This refers to the models' limitation, due to computational constraints, in capturing the finest details of turbulence, such as the smallest eddies and swirls. To address this, a process called 'parameterization' is used, where the effects of these tiny, chaotic motions are estimated or approximated. This approach enhances the accuracy of the models by including the impacts of small-scale turbulent flows without the need for immense computational power to resolve every minute aspect of turbulence.\nThe Fellow  may f oc us o n predic tin g key parame ters lik e tu rbulen t mixing l ength and late ral entra inm ent rate, c rucia l for\n developin g p arameteri zations of turbule nt fl ux,  by employing sy mboli c reg res sion to uncove r c omplex re lationsh\nips within  th ese par ameters , t here by dee pen ing th eir underst anding of atm osp heric dynami cs. A s econd  project  wi ll allo\nw the Fellow to l evera ge an extens ive datase t to m odel t he growt h of the atm osp heri c bounda ry layer. Th is w ill  involve expl\noring its dependencies on various large-scale environmental conditions.\nA fi nal pro ject  invol ves  using gene rati ve models t o rep licate tur bulent flow as  mod eled by high-resolut ion si mulation s o\nf the atmosphere. Through these projects, the Fellow will gain firsthand experience in atmospheric science, apply their \nmathematical skills in practical scenarios, and develop expertise in creating data-driven models for atmospheric proces\nses .\nA final project involves using generative models to replicate turbulent flow as modeled by high-resolution simulations of the\natmosphere. Through these projects, the Fellow will gain firsthand experience in atmospheric science, apply their\nmathematical skills in practical scenarios, and develop expertise in creating data-driven models for atmospheric processes.\n\nPROJECT 2\nUNDERSTANDING + MODELING TURBULENT FLOW IN THE ATMOSPHERE\nLearning Outcomes + Deliverables\ndeeper understanding of atmospheric boundary layer dynamics\nhands-on experience with cutting-edge computational techniques\nRelevant Links\nParameterization in Weather and Climate Models\nMentorship\nUnder the mentorship of Sara Shamekh, the project's hired personnel, and Yongquan Qu (LEAP PhD student), the Fellow\nwould employ techniques like neural networks or symbolic regression to develop predictive models.\n\nPROJECT 3\nANALYSIS OF CLIMATE MODEL PERTURBED PHYSICS ENSEMBLES\nThe Team /  Mentors\nLocation\nNew York City and/or Boulder, CO\nBrian Medeiros\nGreg Elsaesser\nQingyuan Yang\n\nPROJECT 3\nANALYSIS OF CLIMATE MODEL PERTURBED PHYSICS ENSEMBLES\nProject Description\nThe Fellow will explore existing perturbed physics ensemble output from climate models (e.g., NCAR/CAM, NASA-\nGISS/ModelE). Each ensemble member is a separate simulation that is distinct from the others by using a unique set of\nphysics parameter selections. By slightly altering the model physics through these parameter choices, the model produces\ndifferent climates. However, it is not known how dependent global extremes (such as rainfall, humidity, droughts) are to\nthese global physics parameter choices, nor is it known how dependent particular local or regional outputs (defined by a\nsmaller latitude-longitude domain) are on global physics parameter choices.\nThe Fellow, depending on their interests and skills, will have the option to focus on these unknowns, collaborating together,\nor working separately on aspects of the above topics. For example, addressing rainfall extremes would involve analyzing daily\nrainfall distributions, with a focus on the distribution tails (i.e., extremes), and understanding how dependent the\ndistributions are on global physics parameter choices. The Fellow could then modify an existing model emulator to\ndetermine if distribution characteristics can be emulated as a function of parameter settings. To address the topic of regional\ndependence on parameter choices, the Fellow would select several regions to understand which are most sensitive to\nparameter choices and whether particular parameters have more impact for some regions. An emulator would then be used\nto map regional characteristics to parameter settings.\nAll analyses contribute to climate model parameter estimation efforts, where it is necessary to have a model that has the\ncorrect selection of physics parameters to ensure that a climate model can simulate observed global extremes as well as a\nmodel that not only can simulate an accurate global mean, but can also simulate realistic regional climatological outputs.\n\nPROJECT 3\nANALYSIS OF CLIMATE MODEL PERTURBED PHYSICS ENSEMBLES\nLearning Outcomes + Deliverables\nunderstanding how climate model extremes or climate model outputs (e.g., rainfall, clouds, water vapor) in different parts\nof the world depend on global physics parameter choices\nhigh-quality images (that show the results) that can be used in PowerPoint presentations\ncode that can be made available, used by others, and built upon in the future for continuing efforts\nMentorship\nThe two project co-leads (Drs. Elsaesser and Medeiros), along with Dr. Yang, will mentor the Fellow, who will gain familiarity\nwith climate model output, and will have the opportunity to use machine learning surrogate models (neural networks,\nGaussian Processes) that emulate the climate model output.\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nThe Team /  Mentors\nLocation\nNew York City \nKara Lamb\nJoseph Ko\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nProject Description\nIce microphysics refers to processes that govern the growth and evolution of ice crystals inside of clouds. Parameterizing ice microphysics is an\nimportant component of climate models, as it impacts the radiative effects of clouds (how they scatter and absorb sunlight), atmospheric\ncirculation, the hydrological cycle, and the partitioning between water phases in the atmosphere. Ice microphysical processes are considered some\nof the most uncertain in terms of their impact on future climate projections, and also have significant potential to be influenced by anthropogenic\npollution.\nAs part of our effort to improve the representation of ice microphysical processes in climate models, we have several possible projects for Fellow  to\nwork on. Detailed, process-level models of clouds can represent ice microphysical processes very accurately. The current state-of-the-art approach\nfor modeling ice microphysical processes in clouds represents ice particles in a Lagrangian fashion (by tracking the evolution of individual ice\ncrystals in the cloud). However, climate models need to represent ice microphysical processes in a much more simplified manner in order to be\ncomputationally efficient enough to run over large spatial scales and for long periods of time. A Fellow could explore how reduced order modeling\nand machine learning based dimensionality reduction techniques could be applied to data sets from detailed process-level cloud models of ice\nclouds in order to answer fundamental questions about the diversity of ice crystals that grow in the atmosphere. Understanding the diversity of ice\ncrystals that grow in the atmosphere will inform our understanding of how to represent the complexity of ice microphysics in climate models.\nA second possible project would involve applying physics-informed machine learning methods to improve physical understanding of ice\nmicrophysical processes using ice crystal images acquired during NASA and DOE aircraft campaigns. Since we observe ice crystals at single points\nin time from aircraft observations, it is challenging to infer ice growth histories by only looking at ice crystal images, but such information could\nprovide insights into microphysical processes experienced by ice crystals growing in the atmosphere. One project that a Fellow could work on is\ninferring atmospheric histories (temperature, supersaturation) of observed ice crystals using generative machine learning models, such as\nTransformers. The Fellow would work with data sets from past NASA and DOE aircraft campaigns that measured ice crystals in the atmosphere, and\nlook at how information from back-trajectory calculations of sampled air could be used to better understand how ice crystals grow and evolve in\nthe atmosphere.\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nLearning Outcomes + Deliverables\nresearch papers submitted in scientific journals and/or in papers submitted to machine learning conferences\nMentorship\nThe selected Fellow for this project would be based at Columbia University. They would be co-mentored by Dr. Kara Lamb\nand Dr. Joseph Ko, and interact with a team of researchers at LEAP focused on using a combination of models and\nobservations to reduce both structural and parametric uncertainty in physics-based models of aerosol and cloud\nmicrophysics. The overarching goal of our research is to put stronger constraints on microphysical processes across a range\nof different scales in atmospheric models. The Fellow would be involved in planning and presenting research, and would\nmeet on a weekly basis with researchers at Columbia to discuss and refine research ideas.\n\nPROJECT 5\nUNDERSTANDING + MODELING THE IMPACT OF AIR-SEA HETEROGENEITY\nON SURFACE FLUXES\nThe Team /  Mentor\nLocation\nNew York City \nDhruv Balwada\n\nPROJECT 5\nUNDERSTANDING + MODELING THE IMPACT OF AIR-SEA HETEROGENEITY\nON SURFACE FLUXES\nProject Description\nThe air-sea interface, where the Earth's atmosphere meets the ocean, is very dynamic and critical for the coupled climate\nsystem. This interface plays a central role in regulating long term climate, shaping weather, and maintaining marine\necosystems through the exchange of heat, momentum and gases. These exchanges are a result of an intricate and coupled\ninterplay between oceanic and atmospheric flows, and significant variability across scales in these flows poses a challenge for\naccurate modeling of these exchanges. Using high resolution atmosphere-ocean coupled models, research over the last two\ndecades has highlighted the significance of heterogeneity in the air-sea exchange, and the impact this has on the boundary\nlayers, surface flows, winds, clouds and rainfall. \nAs part of this effort, the Fellow will work on analyzing the small scale hetrogeneity that emerges in these simulations, and\nuse machine learning methods (like symbolic regression) to find links and derive functional relationship between the\nimpacts of this heterogeneity on the large scale atmosphere-ocean exchange and the large scale flow features in the\natmosphere and ocean. These projects aim to teach students about important processes in air-sea exchange and climate\nscience, and help them develop physical intuition and data-driven models for these processes.\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nLearning Outcomes + Deliverables\nresearch papers submitted in scientific journals and/or in papers submitted to machine learning conferences\nMentorship\nThe selected researcher for this project would be based at Columbia University and be mentored by Dr. Dhruv Balwada\n(https://dhruvbalwada.github.io/). \n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2025/02/2025-REU-Project-Descriptions-1.pdf",
    "transcript": "\nPROJECT 1\nUSING MACHINE LEARNING TO IMPROVE CLIMATE PREDICTIONS: REDUCING\nUNCERTAINTY IN THE LAND CARBON SINK \nThe Team / Mentor\nLocation\nBoulder, CO, or New York, NY\nLinnia Hawkins\nKatie Dagon\nDaniel Kennedy\n\nThe Community Land Model (CLM) is a key component of Earth System Models used to simulate land surface processes,\nincluding carbon, energy, and water cycles. Parameter calibration remains a major challenge in CLM development, as\nparametric uncertainty directly affects uncertainty in model projections of the land carbon sink. Traditional calibration\napproaches are computationally expensive and limited in their ability to explore the full parameter space. Machine learning\n(ML) offers a powerful alternative by enabling efficient, data-driven approximation of model behavior.\nThis project will focus on training ML-based emulators using an existing perturbed parameter ensemble, a dataset that\nsystematically explores the sensitivity of CLM to a wide range of parameter values. This dataset is uniquely suited for ML\napplications as it provides a comprehensive and structured sampling of the CLM parameter space. The emulators will be\ndesigned to accurately reproduce CLM outputs, allowing for rapid parameter sensitivity analysis and efficient tuning against\nobservational constraints.\nThis project provides a unique opportunity for students to gain hands-on experience at the intersection of machine learning\nand Earth system modeling. Participants will contribute to advancing methods for reducing uncertainty in land carbon cycle\nprojections and participate in cutting-edge climate research at the National Center for Atmospheric Research. \nProject Description\nPROJECT 1\nLEVERAGING ML TO CONSTRAIN LAND CARBON SINK UNCERTAINTY\n\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nParticipants will gain hands-on experience with\nmachine \nlearning \ntechniques, \nuncertainty\nquantification, and land surface modeling.\nStudents will contribute to developing a practical\nML-based emulator for CLM, which can be used\nfor future model tuning and carbon cycle studies.\nBackground in ecology, hydrology, physics, mathematics, or\nstatistics\nExperience with programming and data analysis in python\nFamiliarity with machine learning frameworks such as\nTensorFlow or PyTorch.\nFamiliarity with climate science or climate data analysis.\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nStudents will be mentored by Linnia Hawkins, Katie Dagon, and Daniel Kennedy, and have the opportunity to collaborate\nwith LEAP researchers and Earth System Model developers at the National Center for Atmospheric Research. At the start of\nthe program, students will also attend a two-week data science bootcamp in New York City.\nPROJECT 1\nUSING MACHINE LEARNING TO IMPROVE CLIMATE PREDICTIONS: REDUCING\nUNCERTAINTY IN THE LAND CARBON SINK \n\nPROJECT 2\nUNDERSTANDING CLOUD MICROPHYSICAL PROCESS IN CLIMATE MODELS\nThe Team /  Mentors\nLocation\nBoulder, CO, or New York, NY\nDa Fan\nDavid John Gagne II\nMike Pritchard\n\nProject Description\nCloud microphysics refers to processes that affect cloud and precipitation across scales, playing a critical role in climate and\nweather modeling. Machine learning has been leveraged to improve empirical microphysics representation in climate\nmodels, such as CAM (Community Atmosphere Model)/CESM3 (Community Earth System Model version 3). However,\nempirical and machine learning based microphysics parameterization yielded different precipitation and cloud properties\nwithin climate modeling. This project aims to explore the differences in microphysical processes in climate simulations,\nutilizing distinct microphysics parameterizations through artificial intelligence (AI). In this project, you will dive into machine\nlearning concepts like artificial neural networks, transformers, and explainable AI, and their applications in practical climate\nmodel challenges.\nPROJECT 2\nUNDERSTANDING CLOUD MICROPHYSICAL PROCESS IN CLIMATE MODELS\n\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nFamiliarity with climate model outputs\nOpportunity to use machine learning methods\n(neural networks, explainable AI) to tackle\npractical climate model challenges\nprogramming skills (preferably Python)\nbackground in machine learning\ninterest in climate science\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nThe two project co-leads (Drs. David John Gagne II and Mike Pritchard), together with Dr. Da Fan (LEAP postdoc) will provide\nmentorship. \nPROJECT 2\nUNDERSTANDING CLOUD MICROPHYSICAL PROCESS IN CLIMATE MODELS\n\nPROJECT 3\nUSING GENERATIVE AI TO IMPROVE MODELING OF CIRRUS CLOUD PROCESSES\nThe Team /  Mentors\nLocation\nNew York, NY\nKara Lamb\nJoseph Ko\n\nProject Description\nA major source of uncertainty in current climate models is the parameterization of subgrid-scale cloud processes, including the small-scale\nevolution of ice crystals in clouds. Cirrus clouds (pure ice clouds that form high in the Earth‚Äôs atmosphere) play an important role in the\nEarth‚Äôs radiation budget, but are difficult to accurately model because of uncertainties about the microphysical processes that govern their\nlarge-scale evolution. Our project will focus on using generative AI to improve our ability to model these clouds. \nLarge eddy simulations (LES) are computational models that allow us to model the evolution of cirrus clouds at high resolutions. While LES\nsimulations can use simplified bulk parameterizations (Bulk-LES, similar to the parameterizations typically used in large scale climate\nmodels) to model ice cloud microphysical processes, LES simulations with much more accurate super-droplet (SDM) based cloud\nmicrophysics schemes have recently been developed (Chandrakar et al. 2024). These SDM-LES simulations model cirrus clouds by tracking\nrepresentative ice particles during their atmospheric lifetime; however these simulations are extremely computationally expensive, limiting\nthe microphysical parameter space that can be explored when modeling cirrus cloud evolution. \nOption #1: build on recent work using score-based latent data assimilation based on a diffusion model (Qu et al. 2024) to correct the Bulk-\nLES simulations to reproduce the much more accurate SDM-LES simulations. We will explore conditioning the latent data assimilation\nprocess on different ice microphysical parameterizations. This approach will allow us to accurately simulate cirrus cloud evolution using\ngenerative AI to bias-correct the much less computationally expensive Bulk-LES, while also varying the ice microphysical parameterization. \n \nOption #2: explore the potential for combining in situ observations, simulated back-trajectories, and generative AI to predict the growth\nhistories of ice crystals from single snapshots in time. Inspired by recent innovations in using generative AI for robotics design (Xu et al.\n2024), we propose using a dynamics-guided diffusion model to infer a distribution of probable microphysical histories from a single target\ncrystal image. This project could result in two key outcomes: (1) The creation of a benchmark dataset that fuses in situ observations with\nsimulated environmental histories, and (2) the development of generative AI that can infer crystal growth\nhistories in a probabilistic fashion. \nPROJECT 3\nUSING GENERATIVE AI TO IMPROVE MODELING OF CIRRUS CLOUD PROCESSES\n\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nResearch papers submitted in scientific journals\nor machine learning conferences\nConference \npresentations \nat \ngeophysical\nconferences or machine learning conferences\nGood background in physics and mathematics\nExperience with programming and data analysis in python\nExperience with deep learning frameworks such as pytorch,\ntensorflow, or jax\nExperience with ML architectures including transformers\nExperience with generative AI, including diffusion models \nAlso nice to have:\nExperience with atmospheric models or data \nBackground in atmospheric science and cloud physics\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nThe selected Momentum Fellow will be based at Columbia University and bee co-mentored by Dr. Kara Lamb and Dr. Joseph\nKo. The two REU students will be mentored by the Momentum Fellow as well as Dr. Lamb and Dr. Ko. The Momentum Fellow\nand REU students will interact with a team of LEAP researchers focused on using a combination of models and observations\nto reduce both structural and parametric uncertainty in physics based models of cloud processes. The Momentum Fellow\nand REU students will be involved in planning and presenting research, and will meet weekly with researchers at Columbia\nto discuss and define research ideas throughout the summer.\nPROJECT 3\nUSING GENERATIVE AI TO IMPROVE MODELING OF CIRRUS CLOUD PROCESSES\n\nPROJECT 4\nTROPICAL FOREST DYNAMICS IN A CHANGING CLIMATE\nThe Team / Mentor\nLocation\nNew York, NY\nEnsheng Weng\n\nProject Description\nTropical forests are experiencing significant changes due to climate changes, disturbances, and human activities. Increases\nin droughts and fire frequency and intensity can trigger abrupt state shifts of tropical ecosystems, and thus lead to decline\nin vegetation productivity and ecosystem carbon storage, and shifts of species compositions. These changes can feedback\nto climate systems by altering the land surface characteristics and carbon cycle, leading to cascading changes in global\nclimates.\nThe REU students and Momentum Fellows, depending on your interests, will have the options to work on remote sensing\ndata analysis, vegetation and ecosystem modeling, data-model integration, and machine learning approaches to improve\nforest modeling. For example, you can use GEDI data to analyze how forest structure changes with a precipitation gradient;\nyou can use remote-sensed fire and precipitation data to analyze how savanna changes; you can use a vegetation model to\nexplore how the competition between species shape forest compositions and structure at different climatic conditions.\nAnd, also, you can use data assimilation and machine learning approaches to improve model performance with data from\nsatellites and observatory networks in tropical regions.\nPROJECT 4\nTROPICAL FOREST DYNAMICS IN A CHANGING CLIMATE\n\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nLearn \nto \nuse \na \nwell-designed \nvegetation\ndemographic model\nGain knowledge about tropical forests with\nmodeling and data analysis approaches\nHave the opportunity to use machine learning\nand data assimilation approaches to improve\nmodel performance\nEcosystem ecology\nProgramming with Python and/or Fortran\nInterests in climate change and tropical forests\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nThe REU students will be mentored by Dr. Weng and the Momentum Fellows. \nPROJECT 4\nTROPICAL FOREST DYNAMICS IN A CHANGING CLIMATE\n\nPROJECT 5\nPROBABILISTIC DATA ASSIMILATION ENHANCES THE PREDICTION OF\nOBSERVED TEMPERATURE EXTREMES\nThe Team /  Mentors\nLocation\nNew York, NY\nPierre Gentine\nShuolin \"Shawn\" Li\nYongquan Qu\n\nProject Description\nThis project explores the use of probabilistic data assimilation to improve predictions of temperature extremes by\nintegrating observational data with numerical models. The focus is on enhancing forecast reliability through Bayesian\ninference, ensemble methods, and machine learning-based uncertainty quantification. Students will gain hands-on\nexperience with state-of-the-art data assimilation techniques, learning how to integrate observational data with models for\nimproved climate predictions.\nPROJECT 5\nPROBABILISTIC DATA ASSIMILATION ENHANCES THE PREDICTION OF OBSERVED\nTEMPERATURE EXTREMES\n\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nDevelopment \nand \nvalidation \nof \na \nprobabilistic\nframework for temperature extreme prediction\nImplementation of data assimilation algorithms for\nclimate model updates\nComparative analysis of probabilistic vs. deterministic\nforecasts\nA final report and presentation summarizing findings\nProgramming: Proficiency in Python (NumPy, SciPy,\nPyTorch, or TensorFlow)\nMathematical Background: Probability, statistics, and\nlinear algebra\nMachine Learning & Data Science: Familiarity with\nBayesian methods, uncertainty quantification, or deep\nlearning is a plus\nClimate Science/Physics: Basic understanding of numerical\nmodeling and earth system processes\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nPierre Gentine, the Principal Investigator (PI), will oversee the overall research direction and project execution, and provide\nhigh-level guidance on research methodology and scientific approach. Shawn Li and Yongquan Qu, co-mentors, will host\nweekly meetings to discuss progress, challenges, and next steps; give feedback on technical work, research design, and data\nanalysis; offer additional technical guidance and support for coding, data processing, and analysis; and provide hands-on\ntraining in data assimilation and model implementation. The Momentum Fellow assigned to this project will provide\nadditional mentoring support to ensure effective student guidance.\nPROJECT 5\nPROBABILISTIC DATA ASSIMILATION ENHANCES THE PREDICTION OF OBSERVED\nTEMPERATURE EXTREMES\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/2023-momentum-fellowship/",
    "transcript": "2023 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science who are interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling, and developing research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship Faculty work closely with fellows throughout the summer (June ‚Äì August 2023) on a well-defined, yet open-ended, machine learning research problem in climate data science. Faculty also guide their fellows to present their summer research at future LEAP events and other workshops / conferences. Tian Zheng, LEAP‚Äôs Education Director/Chief Convergence Officer, and Carl Vondrick, LEAP‚Äôs Data Science Director 2023 Summer Momentum Fellows Mohammad Erfani Mohammad is graduate research assistant currently pursuing his PhD in the Department of Civil and Environmental Engineering at the University of South Carolina. He graduated with a MSc in Civil and Environmental Engineering at Ferdowsi University of Mashhad, Iran. Since joining iWERS in August 2019, Mohammad has been applying Computer Vision and Deep Learning techniques to develop real-time flood mapping and early warning systems. His work has the potential to mitigate the devastating impacts of flooding by providing a framework that captures flood information with a higher spatiotemporal resolution and renders visual outcomes easier to interpret for decision-makers, emergency responders + the public. Yu Huang Yu is a PhD student in the Department of Earth and Environmental Engineering at Columbia University, and a member of the Gentine Lab. She received a B.Sc. degree in Atmospheric Sciences from Nanjing University, and is  now studying land-atmospheric interactions. She is especially interested in how water cycle and carbon cycle are coupled through land-atmosphere processes. Using climate models, machine learning techniques, and causal inference in her research, her professional goal is to be a professor or research scientist in either academia or industry. Matthew Beveridge Matt is an incoming doctoral student at Columbia University in the Columbia Imaging and Vision Lab (CAVE) with Prof. Shree Nayar and a Visiting Researcher at the University of Colorado Boulder with Prof. Morteza Karimzadeh. He completed his Master of Engineering (M.Eng.), advised by Prof. Daniela Rus, and Bachelor of Science (B.S.) in Electrical Engineering and Computer Science (EECS) at MIT, with a double major in Mathematics and minor in Theater Arts. Matt's research focuses on computer vision and machine learning for robust perception and its application to the science of the physical environment. Matt has also been involved with startups in the field of autonomy, organized community events around energy and climate, and worked on spaceflight at NASA. 2023 Summer Momentum Fellowship Faculty Dhruv Balwada Assistant Research Professor, Columbia Climate School / Lamont-Doherty Earth Observatory Kara Lamb Associate Research Scientist, Department of Earth + Environmental Engineering at Columbia University Carl Vondrick Associate Professor of Computer Science, Columbia University Pierre Gentine Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University 2023 Summer Momentum Fellowship Research Projects PROJECT I: ESTIMATING OCEAN CURRENTS FROM MEASUREMENTS OF SEA SURFACE HEIGHT (SSH) Project Lead : Dhruv Balwada Project Description : One of the best ways to observe the Earth‚Äôs ocean is from space. Space-based observations allow for spatial and temporal coverage that is hard to achieve from in-situ observations, which are usually point measurements made by ships or other in-water platforms. While we routinely observe the sea level (also called sea surface height ‚Äì SSH), temperature and color from space, other variables of interest, such as the ocean currents, are not directly observed. Estimating these currents is important since they transport materials, like heat, carbon, phytoplankton, and better knowledge of these currents can help to improve climate predictions, fisheries management, and oil spill monitoring. The LEAP momentum fellow will work on the task of estimating ocean currents from measurements of SSH. This will be in service of a new SSH measuring satellite, SWOT, which was recently launched to measure SSH at unprecedented spatial resolution. However, these new measurements come with a challenge ‚Äì the conventional physics based approaches for estimating surface currents from SSH do not work at the finer scales that will be observed. The reason being that the SSH at these scales is usually dominated by internal waves, and the currents and SSH corresponding to these waves are not linked by simple mathematical relationships. Recent work has shown that ML based techniques can potentially help address this challenge (eg. Xiao, Balwada et al 2023 , Wang et al 2022 ). However, important questions remain about the response of the ML models to wave amplitude, the generalization of the ML models trained on simulation data to real flows, the potential to apply transfer learning to improve predictions using in-situ observations, and the ability to estimate the errors in the predicted fields. The fellow will tackle some of these questions. Learning Outcomes + Deliverables : A well organized github repository showcasing the jupyter notebooks and codes used during the internship. A final research paper written with the goal of submission to an academic journal. *** PROJECT II: EQUIVARIANT GRAPH NEURAL NETWORKS FOR EFFICIENT EMULATION OF AEROSOL OPTICAL PROPERTIES Project Leads : Kara Lamb and Carl Vondrick Project Description : Forest fires and fossil fuel combustion are an important source of aerosols (small particulate matter) to the atmosphere. Aerosols from these sources strongly absorb sunlight, which can influence the atmospheric temperature profile, impact cloud formation, and cause snow to melt more quickly‚Äì all of which have important effects on the climate. Understanding the role aerosols play in the climate remains a major challenge due to limitations in our modeling and observational capacities. Aerosols are a variety of sizes, shapes, and chemical compositions, all of which impact their optical properties. Atmospheric models and observational retrievals typically approximate these particles as spheres or ellipsoids, which leads to biases in determining how much sunlight aerosols absorb and scatter. Accurate methods (such as the Multiple Sphere T-Matrix Method, MSTM) to calculate aerosol optical properties for arbitrarily shaped particles are very computationally expensive, often requiring hours or days to compute the optical properties of single particles with complex shapes. Our recent work demonstrated that graph neural networks (GNN) are a promising approach for emulating expensive aerosol optical properties codes such as MSTM that may be capable of generalizing to new (and previously unseen) particle shapes. The goal of this summer project would be to extend and improve this GNN modeling approach by including physical constraints in the machine learning model architecture (such as equivariant neural network layers or spherical harmonic basis functions). Novel training methodologies for random orientation calculations of aerosol optical properties (calculating optical properties over all possible orientations and polarization states) will be explored. The Fellow will explore methods to improve the zero-shot performance (i.e. without re-training) of this GNN model. Learning Outcomes + Deliverables : This research will lead to research papers submitted in scientific journals and/or in papers submitted to machine learning conferences. It would also contribute to the development of a code base to train and test machine learning methods for aerosol optical properties codes. *** PROJECT III: INSTRUCTION + MENTORING FOR 2023 SUMMER REU (RESEARCH EXPERIENCE FOR UNDERGRADUATES) PROGRAM Project Lead : Tian Zheng and Pierre Gentine Project Description : Presented¬†in partnership with the Summer at SEAS program, the DSI Scholars program and the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR , LEAP‚Äôs 2023 REU Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. This summer, six (6) undergraduate students from around the United States will participate in the REU Program, starting with a 3-week-long Momentum Bootcamp during which they will gain familiarity and skills with the ClimSim dataset , baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit, and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Momentum Bootcamp will be followed by 5 weeks of on-campus research at Columbia University, culminating in Final Research Presentations on July 27, 2023. The Momentum Fellow will join Tian Zheng and Candace Agonafir on the Bootcamp‚Äôs team of instructors, and mentor the REU students through their on-campus research experiences. Learning Outcomes + Deliverables : Measurable skill set in teaching and academic mentoring in the fields of climate science and machine learning. Maturation as a project manager and independent researcher with an eye toward establishment as a Principal Investigator capable of guiding the next generation of climate data science scholars. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/2023-momentum-fellowship/#content",
    "transcript": "2023 LEAP Summer Momentum Fellowship - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 Summer Momentum Fellowship Overview LEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science who are interested in having a summer research immersion in climate data science, with the opportunity to apply their data science/machine learning skills in climate modeling, and developing research interests in climate data science. Each fellow receives a summer stipend, travel support, and access to LEAP resources, such as LEAP Pangeo and workspace at Columbia University‚Äôs Innovation Hub . Momentum Fellowship Faculty work closely with fellows throughout the summer (June ‚Äì August 2023) on a well-defined, yet open-ended, machine learning research problem in climate data science. Faculty also guide their fellows to present their summer research at future LEAP events and other workshops / conferences. Tian Zheng, LEAP‚Äôs Education Director/Chief Convergence Officer, and Carl Vondrick, LEAP‚Äôs Data Science Director 2023 Summer Momentum Fellows Mohammad Erfani Mohammad is graduate research assistant currently pursuing his PhD in the Department of Civil and Environmental Engineering at the University of South Carolina. He graduated with a MSc in Civil and Environmental Engineering at Ferdowsi University of Mashhad, Iran. Since joining iWERS in August 2019, Mohammad has been applying Computer Vision and Deep Learning techniques to develop real-time flood mapping and early warning systems. His work has the potential to mitigate the devastating impacts of flooding by providing a framework that captures flood information with a higher spatiotemporal resolution and renders visual outcomes easier to interpret for decision-makers, emergency responders + the public. Yu Huang Yu is a PhD student in the Department of Earth and Environmental Engineering at Columbia University, and a member of the Gentine Lab. She received a B.Sc. degree in Atmospheric Sciences from Nanjing University, and is  now studying land-atmospheric interactions. She is especially interested in how water cycle and carbon cycle are coupled through land-atmosphere processes. Using climate models, machine learning techniques, and causal inference in her research, her professional goal is to be a professor or research scientist in either academia or industry. Matthew Beveridge Matt is an incoming doctoral student at Columbia University in the Columbia Imaging and Vision Lab (CAVE) with Prof. Shree Nayar and a Visiting Researcher at the University of Colorado Boulder with Prof. Morteza Karimzadeh. He completed his Master of Engineering (M.Eng.), advised by Prof. Daniela Rus, and Bachelor of Science (B.S.) in Electrical Engineering and Computer Science (EECS) at MIT, with a double major in Mathematics and minor in Theater Arts. Matt's research focuses on computer vision and machine learning for robust perception and its application to the science of the physical environment. Matt has also been involved with startups in the field of autonomy, organized community events around energy and climate, and worked on spaceflight at NASA. 2023 Summer Momentum Fellowship Faculty Dhruv Balwada Assistant Research Professor, Columbia Climate School / Lamont-Doherty Earth Observatory Kara Lamb Associate Research Scientist, Department of Earth + Environmental Engineering at Columbia University Carl Vondrick Associate Professor of Computer Science, Columbia University Pierre Gentine Director, LEAP; Maurice Ewing and J. Lamar Worzel Professor of Geophysics, Departments of Earth and Environmental Engineering and Earth and Environmental Sciences, Columbia University 2023 Summer Momentum Fellowship Research Projects PROJECT I: ESTIMATING OCEAN CURRENTS FROM MEASUREMENTS OF SEA SURFACE HEIGHT (SSH) Project Lead : Dhruv Balwada Project Description : One of the best ways to observe the Earth‚Äôs ocean is from space. Space-based observations allow for spatial and temporal coverage that is hard to achieve from in-situ observations, which are usually point measurements made by ships or other in-water platforms. While we routinely observe the sea level (also called sea surface height ‚Äì SSH), temperature and color from space, other variables of interest, such as the ocean currents, are not directly observed. Estimating these currents is important since they transport materials, like heat, carbon, phytoplankton, and better knowledge of these currents can help to improve climate predictions, fisheries management, and oil spill monitoring. The LEAP momentum fellow will work on the task of estimating ocean currents from measurements of SSH. This will be in service of a new SSH measuring satellite, SWOT, which was recently launched to measure SSH at unprecedented spatial resolution. However, these new measurements come with a challenge ‚Äì the conventional physics based approaches for estimating surface currents from SSH do not work at the finer scales that will be observed. The reason being that the SSH at these scales is usually dominated by internal waves, and the currents and SSH corresponding to these waves are not linked by simple mathematical relationships. Recent work has shown that ML based techniques can potentially help address this challenge (eg. Xiao, Balwada et al 2023 , Wang et al 2022 ). However, important questions remain about the response of the ML models to wave amplitude, the generalization of the ML models trained on simulation data to real flows, the potential to apply transfer learning to improve predictions using in-situ observations, and the ability to estimate the errors in the predicted fields. The fellow will tackle some of these questions. Learning Outcomes + Deliverables : A well organized github repository showcasing the jupyter notebooks and codes used during the internship. A final research paper written with the goal of submission to an academic journal. *** PROJECT II: EQUIVARIANT GRAPH NEURAL NETWORKS FOR EFFICIENT EMULATION OF AEROSOL OPTICAL PROPERTIES Project Leads : Kara Lamb and Carl Vondrick Project Description : Forest fires and fossil fuel combustion are an important source of aerosols (small particulate matter) to the atmosphere. Aerosols from these sources strongly absorb sunlight, which can influence the atmospheric temperature profile, impact cloud formation, and cause snow to melt more quickly‚Äì all of which have important effects on the climate. Understanding the role aerosols play in the climate remains a major challenge due to limitations in our modeling and observational capacities. Aerosols are a variety of sizes, shapes, and chemical compositions, all of which impact their optical properties. Atmospheric models and observational retrievals typically approximate these particles as spheres or ellipsoids, which leads to biases in determining how much sunlight aerosols absorb and scatter. Accurate methods (such as the Multiple Sphere T-Matrix Method, MSTM) to calculate aerosol optical properties for arbitrarily shaped particles are very computationally expensive, often requiring hours or days to compute the optical properties of single particles with complex shapes. Our recent work demonstrated that graph neural networks (GNN) are a promising approach for emulating expensive aerosol optical properties codes such as MSTM that may be capable of generalizing to new (and previously unseen) particle shapes. The goal of this summer project would be to extend and improve this GNN modeling approach by including physical constraints in the machine learning model architecture (such as equivariant neural network layers or spherical harmonic basis functions). Novel training methodologies for random orientation calculations of aerosol optical properties (calculating optical properties over all possible orientations and polarization states) will be explored. The Fellow will explore methods to improve the zero-shot performance (i.e. without re-training) of this GNN model. Learning Outcomes + Deliverables : This research will lead to research papers submitted in scientific journals and/or in papers submitted to machine learning conferences. It would also contribute to the development of a code base to train and test machine learning methods for aerosol optical properties codes. *** PROJECT III: INSTRUCTION + MENTORING FOR 2023 SUMMER REU (RESEARCH EXPERIENCE FOR UNDERGRADUATES) PROGRAM Project Lead : Tian Zheng and Pierre Gentine Project Description : Presented¬†in partnership with the Summer at SEAS program, the DSI Scholars program and the Significant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR , LEAP‚Äôs 2023 REU Program offers summer undergraduate research experiences (SUREs) on synergistic innovations in data science and climate science. The program hosts undergraduate researchers and offers a wide array of enrichment learning and networking opportunities. This summer, six (6) undergraduate students from around the United States will participate in the REU Program, starting with a 3-week-long Momentum Bootcamp during which they will gain familiarity and skills with the ClimSim dataset , baseline ‚ÄúML Operations‚Äù such as quality controlling and data engineering, building training pipelines to perform machine learning, assessing goodness of fit, and exploring new algorithms. For those who already come with this basic foundation, advanced learning outcomes could include experimenting with stochastic and generative deep learning algorithms that attempt to fit the non-deterministic component of chaotic cloud dynamics. Momentum Bootcamp will be followed by 5 weeks of on-campus research at Columbia University, culminating in Final Research Presentations on July 27, 2023. The Momentum Fellow will join Tian Zheng and Candace Agonafir on the Bootcamp‚Äôs team of instructors, and mentor the REU students through their on-campus research experiences. Learning Outcomes + Deliverables : Measurable skill set in teaching and academic mentoring in the fields of climate science and machine learning. Maturation as a project manager and independent researcher with an eye toward establishment as a Principal Investigator capable of guiding the next generation of climate data science scholars. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/momentum-fellowship/2023-momentum-fellowship/",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/momentum-fellowship/2023-momentum-fellowship/#content",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/momentum-fellowship/2024-leap-summer-momentum-fellowship/",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/education/momentum-fellowship/2024-leap-summer-momentum-fellowship/#content",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/02/2022-REU-Project-Descriptions.pdf",
    "transcript": "\nPROJECT 1\nWARM RAIN MICROPHYSICS EMULATION\nThe Team /  MentorsAndrew Gettelman\nDavid John Gagne\nTian Zheng\nAndrew Gettelman\nNew York City\nTian Zheng\n\nPROJECT 1\nWARM RAIN MICROPHYSICS EMULATION\nThe Warm Rain Microphysics project replaces the rain formation process in a climate model with a neural network emulator of a more detailed and expensive model. Training data access and test cases can be run with a single column model, and the access is through python. The REU project will focus on trying to retrain emulators based on training data, and run them in a single column model.\nLearning Outcomes + Deliverables\nbetter understanding of ML training\nsome understanding of how a climate model works\nnew neural network for use in a climate model\nsome understanding of how a climate model\nworks\nknowledge of Python and Jupyter notebooks\nknowledge of Python and Jupyter notebooks\n\nPROJECT 2\nDEEP LEARNING BASED INVERSE MODELING FOR PARAMETER ESTIMATION\nVipin Kumar\nCarl Vondrick\nNew York City \nDave Lawrence\nCarl VondrickNew York City \nDave Lawrence\n\nPROJECT 2\nDEEP LEARNING BASED INVERSE MODELING FOR PARAMETER ESTIMATION\nThe REU project will focus on a deep learning framework that can be used to identify or reconstruct static characteristics\nof an environmental system given its input and output over time.\nof an environmental system given its input and output over time.\nopportunity to work on a state-of-the-art methodology for inverse modeling using a deep-learning framework\ncustomize existing code and/or develop its new variations to reconstruct static characteristics of hydrologic basins using weather drivers and streamflow\nopportunity \nto \nwork \non \na \nstate-of-the-art\nmethodology for inverse modeling using a deep-\nlearning framework\nPython\nPyTorch\nwillingness to learn\nwillingness to learn\n\nPROJECT 3\nUNIFICATION OF LABORATORY + OBSERVATIONAL DATA VIA LEARNING\nALGORITHMS FOR ROBUST MODELS OF ICE MICROPHYSICS\nThe Team /  Mentors\nLocation\nNew York City\nMarcus van Lier-Walqui\nHugh Morrison\nKara Lamb\n\nPROJECT 3\nUNIFICATION OF LABORATORY + OBSERVATIONAL DATA VIA LEARNING\nALGORITHMS FOR ROBUST MODELS OF ICE MICROPHYSICS\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nworking familiarity with geophysical datasets\ncollected on ice crystals\nworking familiarity with how to apply ML\nalgorithms to use this data to improve models\nand understanding of ice growth physics\nfamiliarity with Python, including data analysis +\nplotting\nexperience with Tensorflow, Keras, or packages\nfor Bayesian inference (e.g., PyMC)\nProject Description\nThe REU project will focus on building better models of snow crystal growth, using machine learning to unify observations\nranging from electron microscopy to laboratory experiments to radar observations of ice clouds.\n\nPROJECT 4\nLEAP CRYO: LEARNING NEW ICE SHEET SLIDING LAWS WITH NEURAL NETS\nAND HIGH-RESOLUTION ICE MODELS\nThe Team /  Mentors\nLocation\nNew York City \nJonathan Kingslake \nDavid Porter\nLaure Zanna\n\nPROJECT 4\nLEAP CRYO: LEARNING NEW ICE SHEET SLIDING LAWS WITH NEURAL NETS\nAND HIGH-RESOLUTION ICE MODELS\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nknowledge about viscous fluid dynamics,\nnumerical simulations, and ML-based\nrepresentations of glacial processes\nsome experience with scientific computing in\nPython\nsome experience in physics-based numerical\nsimulation / modeling, geophysical processes,\nglaciology, or high-performance / cloud\ncomputing\nProject Description\nThe REU project will focus on simulating ice sheet flow. The flow of ice sheets controls sea level rise, but it depends on\ndifficult-to-observe sliding at the ice base, over bumpy ‚Äòsubglacial‚Äô topography. The REU‚Äôs primary goal will be setting up an\nefficient, autonomous workflow for running numerical simulations of ice flow for a wide range of input parameters. This will\nbe part of a larger project to improve how we represent ice flow in climate models using machine learning.\n\nPROJECT 5\nOPTIMIZED SYSTEMATIC CALIBRATION OF EARTH SYSTEM MODEL PHYSICS\nThe Team /  Mentor\nLocation\nNew York City \nGreg Elsaesser\nMarcus van Lier-Walqui\nAndrew Gettelman\n\nPROJECT 5\nOPTIMIZED SYSTEMATIC CALIBRATION OF EARTH SYSTEM MODEL PHYSICS\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nfamiliarity with reading + analyzing climate\nmodel data\nlearn about / enhance existing ML emulators\nmapping climate models inputs to outputs\nfamiliarity with Python, including data analysis +\nplotting\nexperience in using TensorFlow, Keras, or\npackages for Bayesian inference\nProject Description\nThe REU project will focus on using machine learning to improve automated methods for optimizing physics parameter\nsettings in climate models. The project will use a simplified version of the NASA GISS global climate model that contains one\nsingle atmospheric grid-cell column (known as a single column model, or SCM). The SCM is useful for understanding how\nclouds and precipitation are influenced by the local grid-cell environment and specified physics input parameters. The\ncandidate will build on an existing \"surrogate SCM\" or \"emulator\" that maps the physics input parameter settings to the\ncloud and precipitation outputs. Existing Bayesian Inference techniques will be applied to the surrogate SCM toward\ndetermining which parameter combinations are best for producing the most realistic clouds and precipitation.\n\nPROJECT 6\nCHARACTERIZING THE IMPACT OF SPATIAL HETEROGENEITY OF FOREST\nCANOPIES ONTO CANOPY TURBULENCE + ASSOCIATED TRANSPORT PROCESSES\nThe Team /  Mentor\nLocation\nNew York City \nMarco Giometto\nEnsheng Weng\nDave Lawrence\n\nPROJECT 6\nCHARACTERIZING THE IMPACT OF SPATIAL HETEROGENEITY OF FOREST CANOPIES\nONTO CANOPY TURBULENCE + ASSOCIATED TRANSPORT PROCESSES\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nunderstanding how to process results from\ncomputation fluid dynamics simulations\nlearning how the plant canopy density +\nstructure affect the airflow and mass (gasses)\ntransport therein\nlearning how to visualize 3-D fluid dynamics\ndatasets\nability to evaluate selected airflow statistics, and\ndetermine how the canopy structure affects the\nairflow structure + associated transport\nprocesses\nfamiliarity with Python\na fluid mechanics course\nProject Description\nThe REU project will focus on characterizing the structure of the airflow over and within a forest canopy, making use of\nresults from computational fluid dynamics simulations of flow over an idealized forest.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/02/2024-REU-Project-Descriptions.pdf",
    "transcript": "\nPROJECT 1\nQUANTIFYING EPISTEMIC + ALEATORIC UNCERTAINTY IN CLIMATE DATA\nThe Team /  MentorsStephan MandtMike Pritchard\nJerry Lin\nNew York City / Remote\nEliot Wong-Toi\nMike Pritchard\nNew York City / Remote\nEliot Wong-Toi\n\nPROJECT 1\nQUANTIFYING EPISTEMIC + ALEATORIC UNCERTAINTY IN CLIMATE DATA\nThis project delves into uncertainty quantification in machine learning for climate, distinguishing between model limitations and intrinsic data noise. We pose a pivotal question: in data-driven weather and climate projections, which uncertainty dominates? Using the acclaimed ClimSim dataset (NeurIPS 2023 best paper award) coming from an atmospheric model, we seek to understand how prediction errors scale as we increase the data set size and model complexities. Beyond better predictions, our mission is to empower machine learning models with improved calibration performance‚Äîenabling them to accurately gauge uncertainty levels and \"know what they don't know.\" In this interdisciplinary project between climate science and statistics, you'll dive into probabilistic machine learning concepts like Bayesian neural networks, deep ensembles, Laplace approximations, and variational inference and learn to apply them in a practical climate modeling context.\nLearning Outcomes + Deliverables\npractical experience in advanced machine learning applications\nunderstanding the intersection of climate science + data science\ncontributi\non to ongoi\nng \nresearch \nproject\ns\nunderstanding \nthe \nintersection \nof \nclimate\nscience + data science\nstrong background in machine learning\nprogramming skills (preferably Python)\ncoursework in machine learning, climate science, or related disciplines\n** As you review these projects, we encourage yo\nu to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\n    to learn those skills by taking advantage of those resources.\n\nPROJECT 2\nUNDERSTANDING + MODELING TURBULENT FLOW IN THE ATMOSPHERE\nThe Team /  MentorsSara Shamekh\nYongquan Qu\nNew York City \nNew York City \nYongquan Qu\n\nPROJECT 2\nUNDERSTANDING + MODELING TURBULENT FLOW IN THE ATMOSPHERE\nTurbulent flux, a key concept in atmospheric science and fluid dynamics, describes the transfer of properties like heat, momentum, or moisture through turbulent flow in the atmosphere. This flow is characterized by chaotic, swirling movements of air or fluid. In the study of these phenomena, particularly in large-scale models like weather or climate simulations, the term 'unresolved turbulent flow' often arises. This refers to the models' limitation, due to computational constraints, in capturing the finest details of turbulence, such as the smallest eddies and swirls. To address this, a process called 'parameterization' is used, where the effects of these tiny, chaotic motions are estimated or approximated. This approach enhances the accuracy of the models by including the impacts of small-scale turbulent flows without the need for immense computational power to resolve every minute aspect of turbulence.\nAs part of  a lar ge r pr oject ai med  at improvin g the re pres entati on of smal l-scale ph ysic s in atmo sph eric scienc e, th e sum\nmer of 202 4 o ffers sev eral exc iting REU oppor tun itie s. These opp ortun ities  ar e designed to tea ch studen ts about\n important  pr oces ses  in atm osp heri c scie nce  and h elp them de velop physica l a nd data-driv en mode ls fo r these pro cesses.\n One opportun ity focus es on predic ting key p aramet ers li ke turbu lent mixing len gth and late ral entrainm ent rat e, crucial fo\nr developing parameterizations of turbulent flux. Students will employ symbolic regression to uncover complex relationships wi\nthin th ese parameters, the reb y deep ening thei r unders tan ding o f atmo spheric dynamics . In  the secon d p roject, partic ipan\nts in the REU program will leverage an extensive dataset to model the growth of the atmospheric boundary layer. This will\n involve exploring its dependencies on various large-scale environmental conditions.\nA final project involves using generative models to replicate turbulent flow as modeled by high-resolution simulations of t\nhe atmos phere. Through these pro ject s, students gai n firs thand experien ce i n atmosph eri c scie nce, appl y thei r mathema\ntical skil ls in practical  scenari os, and d evelo p expert ise in cr eati ng data-driv en mode ls f or atm ospheric pr oce\nsses.\nA final project involves using generative models to replicate turbulent flow as modeled by high-resolution simulations of the\natmosphere. Through these projects, students gain firsthand experience in atmospheric science, apply their mathematical\nskills in practical scenarios, and develop expertise in creating data-driven models for atmospheric processes.\n\nPROJECT 2\nUNDERSTANDING + MODELING TURBULENT FLOW IN THE ATMOSPHERE\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\ndeeper understanding of atmospheric boundary\nlayer dynamics\nhands-on \nexperience \nwith \ncutting-edge\ncomputational techniques\nfoundation in mathematical modeling\nprogramming skills\nbasic understanding of physics or atmospheric\nscience\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nUnder the mentorship of Sara Shamekh, the project's hired personnel, and Yongquan Qu (LEAP PhD student),\nREU participants would employ techniques like neural networks or symbolic regression to develop predictive models. \nRelevant Links\nParameterization in Weather and Climate Models\n\nPROJECT 3\nANALYSIS OF CLIMATE MODEL PERTURBED PHYSICS ENSEMBLES\nThe Team /  Mentors\nLocation\nNew York City and/or Boulder, CO\nBrian Medeiros\nGreg Elsaesser\nQingyuan Yang\n\nPROJECT 3\nANALYSIS OF CLIMATE MODEL PERTURBED PHYSICS ENSEMBLES\nProject Description\nThe REU student(s) will explore existing perturbed physics ensemble output from climate models (e.g., NCAR/CAM, NASA-\nGISS/ModelE). Each ensemble member is a separate simulation that is distinct from the others by using a unique set of\nphysics parameter selections. By slightly altering the model physics through these parameter choices, the model produces\ndifferent climates. However, it is not known how dependent global extremes (such as rainfall, humidity, droughts) are to\nthese global physics parameter choices, nor is it known how dependent particular local or regional outputs (defined by a\nsmaller latitude-longitude domain) are on global physics parameter choices.\nThe student(s), depending on their interests and skills, will have the option to focus on these unknowns, collaborating\ntogether, or the students can work separately on aspects of the above topics. For example, addressing rainfall extremes\nwould involve analyzing daily rainfall distributions, with a focus on the distribution tails (i.e., extremes), and understanding\nhow dependent the distributions are on global physics parameter choices. The student(s) could then modify an existing\nmodel emulator to determine if distribution characteristics can be emulated as a function of parameter settings. To address\nthe topic of regional dependence on parameter choices, the student(s) would select several regions to understand which are\nmost sensitive to parameter choices and whether particular parameters have more impact for some regions. An emulator\nwould then be used to map regional characteristics to parameter settings.\nAll analyses contribute to climate model parameter estimation efforts, where it is necessary to have a model that has the\ncorrect selection of physics parameters to ensure that a climate model can simulate observed global extremes as well as a\nmodel that not only can simulate an accurate global mean, but can also simulate realistic regional climatological outputs.\n\nPROJECT 3\nANALYSIS OF CLIMATE MODEL PERTURBED PHYSICS ENSEMBLES\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nunderstanding how climate model extremes or\nclimate model outputs (e.g., rainfall, clouds,\nwater vapor) in different parts of the world\ndepend on global physics parameter choices\nhigh-quality images (that show the results) that\ncan be used in PowerPoint presentations\ncode that can be made available, used by others,\nand built upon in the future for continuing\nefforts\ninterest in climate science and physics\nsome experience in statistics\nfamiliarity or knowledge of statistical approaches\nused \nfor \nanalyzing \ndistribution \ntails \nand\nextremes, or infrequently occurring events\ngood grasp on basic data analysis with Python, R,\nor an equivalent high-level language\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nThe two project co-leads (Drs. Elsaesser and Medeiros), along with Dr. Yang, will mentor the students. The students will gain\nfamiliarity with climate model output, and will have the opportunity to use machine learning surrogate models (neural\nnetworks, Gaussian Processes) that emulate the climate model output.\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nThe Team /  Mentors\nLocation\nNew York City \nKara Lamb\nJoseph Ko\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nProject Description\nIce microphysics refers to processes that govern the growth and evolution of ice crystals inside of clouds. Parameterizing ice microphysics is an\nimportant component of climate models, as it impacts the radiative effects of clouds (how they scatter and absorb sunlight), atmospheric\ncirculation, the hydrological cycle, and the partitioning between water phases in the atmosphere. Ice microphysical processes are considered some\nof the most uncertain in terms of their impact on future climate projections, and also have significant potential to be influenced by anthropogenic\npollution.\nAs part of our effort to improve the representation of ice microphysical processes in climate models, we have several possible projects for REU\nstudents to work on. Detailed, process-level models of clouds can represent ice microphysical processes very accurately. The current state-of-the-art\napproach for modeling ice microphysical processes in clouds represents ice particles in a Lagrangian fashion (by tracking the evolution of individual\nice crystals in the cloud). However, climate models need to represent ice microphysical processes in a much more simplified manner in order to be\ncomputationally efficient enough to run over large spatial scales and for long periods of time. An REU student could explore how reduced order\nmodeling and machine learning based dimensionality reduction techniques could be applied to data sets from detailed process-level cloud models\nof ice clouds in order to answer fundamental questions about the diversity of ice crystals that grow in the atmosphere. Understanding the diversity\nof ice crystals that grow in the atmosphere will inform our understanding of how to represent the complexity of ice microphysics in climate models.\nA second possible project would involve applying physics-informed machine learning methods to improve physical understanding of ice\nmicrophysical processes using ice crystal images acquired during NASA and DOE aircraft campaigns. Since we observe ice crystals at single points\nin time from aircraft observations, it is challenging to infer ice growth histories by only looking at ice crystal images, but such information could\nprovide insights into microphysical processes experienced by ice crystals growing in the atmosphere. One project that an REU student could work\non is inferring atmospheric histories (temperature, supersaturation) of observed ice crystals using generative machine learning models, such as\nTransformers. The student would work with data sets from past NASA and DOE aircraft campaigns that measured ice crystals in the atmosphere,\nand look at how information from back-trajectory calculations of sampled air could be used to better understand how ice crystals grow and evolve\nin the atmosphere.\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nresearch papers submitted in scientific journals\nand/or in papers submitted to machine learning\nconferences\ngood background in physics + mathematics\nexperience with programming and data analysis\nin Python\nexperience \nwith \nmachine \nlearning \nmodels\n(particularly with Transformers, variational auto-\nencoders, or graph neural networks would be a\nplus)\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nThe selected researcher for this project would be based at Columbia University. The REU student would be co-mentored by\nDr. Kara Lamb and Dr. Joseph Ko. The REU student would interact with a team of researchers at LEAP focused on using a\ncombination of models and observations to reduce both structural and parametric uncertainty in physics-based models of\naerosol and cloud microphysics. The overarching goal of our research is to put stronger constraints on microphysical\nprocesses across a range of different scales in atmospheric models. The REU student would be involved in planning and\npresenting research, and would meet on a weekly basis with researchers at Columbia to discuss and refine research ideas.\n\nPROJECT 5\nUNDERSTANDING + MODELING THE IMPACT OF AIR-SEA HETEROGENEITY\nON SURFACE FLUXES\nThe Team /  Mentor\nLocation\nNew York City \nDhruv Balwada\n\nPROJECT 5\nUNDERSTANDING + MODELING THE IMPACT OF AIR-SEA HETEROGENEITY\nON SURFACE FLUXES\nProject Description\nThe air-sea interface, where the Earth's atmosphere meets the ocean, is very dynamic and critical for the coupled climate\nsystem. This interface plays a central role in regulating long term climate, shaping weather, and maintaining marine\necosystems through the exchange of heat, momentum and gases. These exchanges are a result of an intricate and coupled\ninterplay between oceanic and atmospheric flows, and significant variability across scales in these flows poses a challenge for\naccurate modeling of these exchanges. Using high resolution atmosphere-ocean coupled models, research over the last two\ndecades has highlighted the significance of heterogeneity in the air-sea exchange, and the impact this has on the boundary\nlayers, surface flows, winds, clouds and rainfall. \nAs part of this effort, summer students will work on analyzing the small scale hetrogeneity that emerges in these\nsimulations, and use machine learning methods (like symbolic regression) to find links and derive functional relationship\nbetween the impacts of this heterogeneity on the large scale atmosphere-ocean exchange and the large scale flow features\nin the atmosphere and ocean. These projects aim to teach students about important processes in air-sea exchange and\nclimate science, and help them develop physical intuition and data-driven models for these processes.\n\nPROJECT 4\nUNDERSTANDING ICE CRYSTAL GROWTH + EVOLUTION IN THE ATMOSPHERE\nLearning Outcomes + Deliverables\nHelpful Skills to Have **\nresearch papers submitted in scientific journals\nand/or in papers submitted to machine learning\nconferences\nsome experience with basic coding, physics,\nmathematics, or statistics\naspects specific to climate data science will be\nlearned as part of the summer work\n** As you review these projects, we encourage you to bear in mind the self-guided resources available to you.\n    Please don‚Äôt disqualify yourself based on skills you do not have yet, as you may still have the opportunity\n    to learn those skills by taking advantage of those resources.\nMentorship\nThe selected researcher for this project would be based at Columbia University and be mentored by Dr. Dhruv Balwada\n(https://dhruvbalwada.github.io/). The REU student would be co-advised by the Momentum Fellow and Dr. Balwada, who\nwould be involved in planning and presenting research and would meet on a weekly basis with researchers at Columbia to\ndiscuss and refine research ideas.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2025/01/2025-LEAP-Momentum-Bootcamp-Agenda.pdf",
    "transcript": "Warning: PDF may be image-based or PyMuPDF couldn't extract text."
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/01/2024-Momentum-Bootcamp-News.pdf",
    "transcript": "Error fetching PDF: 404"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/11/IN-BRIEF_-Winter-2024-Train-the-Trainer-Bootcamp-Announcement-2.pdf",
    "transcript": "WINTER 2024 : MOMENTUM BOOTCAMP\nON CLIMATE DATA SCIENCE\nCall for Applications\nDate:\nJanuary 10-11, 2024\nSchedule: 9am‚Äì5pm (EDT), both days\nVenue:\nSmith Learning Theater at Teachers College, Columbia University\nThe Center for Learning the Earth with Artificial Intelligence and Physics (LEAP) at\nColumbia University is excited to announce its Winter 2024 Momentum Bootcamp\non Climate Data Science. This two-day immersive, hands-on workshop is designed\nby\nLEAP\nresearchers\nas\npart\nof\nLEAP‚Äôs\neducation\nprogramming\nto\nfoster\nconvergence of climate science and data science, vertically integrate research and\neducation, and forge a LEAP research and learning community including K-12\nteachers, LEAP doctoral students, postdocs, faculty, and other stakeholders.\nThe 2024 Winter Bootcamp aims to teach Climate Data Science in the cloud using\npython tools with an emphasis on reproducibility and collaboration. Participants will\nlearn through a variety of activities including lectures, labs, and team hacking,\nwhich will prepare participants for machine learning-enabled climate research and\nmore advanced and intensive research-oriented workshops.\nApplications are welcome from:\n‚óè\nFaculty members and research scientists from LEAP institutions (Columbia,\nNYU, University of Minnesota, and University of California at Irvine)\n‚óè\nPostdocs and PhD students\n‚óè\nResearch scientists\n‚óè\nNYC Department of Education teachers\n‚óè\nAmerican Museum of Natural History partners\n‚óè\nThe general public\n\nPre-requisites:\n‚óè\nBasic knowledge of scientific computing with Python: numpy, matplotlib,\npandas\n‚óè\nAccess to LEAP-Pangeo Hub (participants will be granted access upon\nacceptance)\nBootcamp participants will:\n‚óè\nDiscover,\naccess,\nand\nexplore\nopen-access\nclimate\ndatasets,\nincluding\nsatellite\nobservations\nand\nclimate\nsimulations, using the Xarray python\npackage.\n‚óè\nCalculate common climate statistics and diagnostics of variability and change\nusing Xarray.\n‚óè\nPerform interactive visualization of climate data using the Holoviews package.\n‚óè\nPerform machine learning on spatio-temporal climate data.\n‚óè\nCompare machine learning models‚Äô performance and prediction skill.\n‚óè\nPerform open science in the cloud using the LEAP-Pangeo Jupyter Hub.\n‚óè\nBe required to bring their own laptop / charger.\nAPPLICATION DEADLINE:\nThursday, December 21, 2023 at 11:59 pm (EDT).\n‚óè\nApplicants will be contacted by December 27, 2023 to complete registration and\nsecure their spot.\n‚óè\nThe registration fee ($400) will cover participant meals and operational costs.\n‚óè\nYou may be eligible for partial or full financial aid (e.g., LEAP researcher/students,\nNYC DoE teacher, non-profit partner). Please respond to the last question on the\napplication form for consideration.\nPlease contact LEAP with any questions.\nSeats are limited, so apply today!\nThe deadline for applications is December 21, 2023.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/01/2023-TTT-Bootcamp-News.pdf",
    "transcript": "Error fetching PDF: 404"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2022/12/Train-the-Trainer-Bootcamp-Winter-2023-Announcement.pdf",
    "transcript": "LEAP TRAIN-THE-TRAINER BOOTCAMP ON\nCLIMATE DATA SCIENCE\nWinter 2023\nDate:\nJanuary 5-6, 2023\nSchedule: January 5th: 9am‚Äì5pm; January 6th: 9am‚Äì5pm\nVenue:\nSmith Learning Theater at Teachers College, Columbia University\nThe Center for Learning the Earth with Artificial Intelligence and Physics\n(LEAP) at Columbia University is excited to announce its 2023 Train-the-Trainer\nBootcamp on Climate Data Science. This two-day immersive, hands-on workshop is\ndesigned by LEAP researchers as part of LEAP‚Äôs education programming to foster\nconvergence of climate science and data science, vertically integrate research and\neducation, and forge a LEAP research and learning community including K-12\nteachers, LEAP doctoral students, postdocs, faculty, and other stakeholders. The\nbootcamp aims to teach Climate Data Science in the cloud using python tools with\nan emphasis on reproducibility and collaboration.\nThis bootcamp is open to:\n‚óè\nFaculty members and research scientists from LEAP institutions (Columbia,\nNYU, University of Minnesota, and University of California at Irvine)\n‚óè\nPostdocs and PhD students from LEAP institutions\n‚óè\nResearch scientists from LEAP partner organizations\n‚óè\nNYC Department of Education high school teachers\n‚óè\nAmerican Museum of Natural History\nLearning Goals:\n‚óè\nDiscover,\naccess,\nand\nexplore\nopen-access\nclimate\ndatasets,\nincluding\nsatellite\nobservations\nand\nclimate\nsimulations, using the Xarray python\npackage\n‚óè\nCalculate common climate statistics and diagnostics of variability and change\nusing Xarray\n‚óè\nPerform interactive visualization of climate data using the Holoviews package\n‚óè\nPerform machine learning on spatio-temporal climate data\n‚óè\nCompare machine learning models‚Äô performance and prediction skill\n‚óè\nPerform open science in the cloud using the LEAP-Pangeo Jupyter Hub\n\nThe bootcamp will introduce climate data science to participants through a variety\nof activities including lectures, labs, and team hacking, which aim to prepare\nparticipants for machine learning-enabled climate research and more advanced and\nintensive\nresearch-oriented\napplications.\nContact\nleap@columbia.edu\nwith\nquestions.\nSeats are limited. Apply here.\nThe deadline for applications is December 21, 2022.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2022/04/In-Brief_-LEAP-Announces-May-2022-Momentum-Bootcamp-on-Climate-Data-Science.pdf",
    "transcript": "LEAP MOMENTUM BOOTCAMP ON\nCLIMATE DATA SCIENCE\nSummer 2022\n___________________________________________\nDate:\nMay 25-27, 2022\nSchedule: 6pm-8pm (May 25), 8am-8pm (May 26), 8am-4pm (May 27)\nVenue:\nEdith Macy Center (550 Chappaqua Road, Briarcliff Manor, NY 10510)\n___________________________________________\nThe Center for Learning the Earth with Artificial Intelligence and Physics (LEAP) at Columbia\nUniversity is excited to announce its Momentum Bootcamp on Climate Data Science. This\nthree-day, immersive, hands-on workshop is designed by LEAP researchers as part of LEAP‚Äôs\neducation programming to foster convergence of climate science and data science, to\nvertically integrate research and education, and to forge a LEAP research and learning\ncommunity including K-12 teachers, undergraduates, graduate students, postdocs, faculty,\nand other stakeholders. Click here for a program description and tentative schedule.\nThis bootcamp is open to:\n‚óè\nFaculty members and research scientists from LEAP institutions (Columbia, NYU,\nUniversity of Minnesota, and University of California at Irvine)\n‚óè\nGraduate students and postdocs from LEAP institutions\n‚óè\nResearch scientists from LEAP partner organizations\n‚óè\nLEAP Summer 2022 REU participants (students and research group members)\n‚óè\nSignificant Opportunities in Atmospheric Research and Science (SOARS) Prot√©g√©s\n‚óè\nNYC Department of Education high school teachers\nThe bootcamp will introduce climate data science to participants through a variety of\nactivities including lectures, labs, and team hacking, which aim to prepare participants for\nmachine\nlearning-enabled\nclimate\nresearch\nand\nmore\nadvanced\nand\nintensive\nresearch-oriented\nworkshops\n(AI4ES\nat\nNCAR\nand\nTAI4ES\nsummer\nschool).\nContact\nleap@columbia.edu with questions.\nSeats are limited. Apply here.\nThe deadline for applications is April 15, 2022.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/design-studio/",
    "transcript": "Design Studio - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Design Studio Research-Education Integration The Design Studio develops case studies and formal curricula for all LEAP education programs, while training graduate interns in curriculum development. Receiving course credit, interns will learn human-centered design, acquire experience with raw geoscience datasets, and develop metacognitive skills that advance their climate data science skills. The Design Studio will also organize an annual ‚Äútrain the trainer‚Äù two-day workshop. Summer 2022: Call for LEAP Design Studio Interns Summer 2022: TAI4ES Summer School Summer 2022: LEAP Momentum Bootcamp Spring 2022: LEAP Course Climate Prediction Challenges Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/design-studio/#content",
    "transcript": "Design Studio - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Design Studio Research-Education Integration The Design Studio develops case studies and formal curricula for all LEAP education programs, while training graduate interns in curriculum development. Receiving course credit, interns will learn human-centered design, acquire experience with raw geoscience datasets, and develop metacognitive skills that advance their climate data science skills. The Design Studio will also organize an annual ‚Äútrain the trainer‚Äù two-day workshop. Summer 2022: Call for LEAP Design Studio Interns Summer 2022: TAI4ES Summer School Summer 2022: LEAP Momentum Bootcamp Spring 2022: LEAP Course Climate Prediction Challenges Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/11/LEAP-Space-Policy_November-2023.pdf",
    "transcript": "Innovation Hub Space Policy\n(November 2023)\nLEAP‚Äôs Executive Committee is pleased to share one critical section of our\nOperations Manual: Innovation Hub Space Policy. If you have any questions\nor\ncomments\nabout\nthis\npolicy,\nplease\ncontact\nMolly\nLopez\nat\nml3122@columbia.edu.\n_____________________________________________________________\nSpace is limited and will be maximized to best advance LEAP‚Äôs goals. A\nregular review process is essential for optimizing space usage. Our priority is\nto keep our space filled with people who actively contribute to the LEAP\ncommunity and goals. Before applying for space at the Innovation Hub\n(application here), please carefully review the below policy. LEAP's Managing\nDirector, in consultation with the Director and Deputy Director, will review\napplications on a rolling basis and will respond in a timely manner.\nOverview\nLEAP‚Äôs assigned space in the Innovation Hub consists of: 10 desks and 3 offices.\nLEAP‚Äôs space is outlined in magenta in the below-linked document:\n2276 12th Ave Space Usage 2022.02.10.pdf\n1. Attributed Space for Center Leadership and Staff (3 offices, 5 desks)\n‚óè\nOffices\n‚óã\nCenter Director (210A)\n‚óã\nCenter Deputy Director (210B)\n‚óã\nManaging Director (210C)\n‚óè\nDesks\n‚óã\nAssociate Director of Education Programs\n‚óã\nManager of Finance and Operations\n‚óã\nSr. Manager of Communications and Knowledge Transfer\n‚óã\nManager of Data and Computing\n‚óã\nIntegration Engineer\n\n2. Remaining LEAP Space to be Assigned (5 desks)\n‚óè\nPriority will be given to researchers who:\n‚óã\nMake the Innovation Hub their primary office location at Columbia\n‚óã\nWill spend at least 3 days per week at the Innovation Hub\n‚óè\nPriority Ranking\n‚óã\nLEAP Tier 3 principal investigators\n‚óã\nVisiting scientists on extended stays with LEAP\n‚óã\nPostdocs with LEAP funding (Tier 3)\n‚óã\nPostdocs working on LEAP-related research, but not funded by LEAP\n(Tier 2)\n‚óã\nGraduate students\n‚óè\nRegular Review\n‚óã\nAt the end of each semester (Fall, Spring, Summer), users will be\nasked to self-report on their past and expected future use of their\nassigned space, and, if desired, to request continuation of this\nassignment.\n‚óã\nThe Center Director, Center Deputy Director, and Managing Director\nwill review reports and requests and adjust assignments at the start of\neach semester and as needed.\n‚óã\nThere is also a ‚Äúreasonable person‚Äù policy: If space has been\nconsistently underutilized at any point, and there is somebody who can\nbetter use the space, a reassignment will be considered.\n3. Open Access Desks\n‚óè\nThere are 8 touch-down high-top desks adjacent to the LEAP assigned area.\n‚óã\nThese are shared with other Innovation Hub occupants and are\navailable on a daily first-come first-serve basis.\n4. Other Considerations\n‚óè\nThe LEAP community is growing and to help us coordinate the dynamic needs\nof the space, we have created a priority list based on LEAP roles and\nresponsibilities. Those who are not working on research funded directly by\nLEAP may be asked to turn over their assigned space as, for instance, the\nneeds of LEAP-funded researchers evolve and as visiting scholars and new\nmembers join our community. We appreciate your understanding and we will\nnotify you of any impending changes in a timely manner.\n‚óè\nIf you will not be using your assigned desk on a particular day, please leave a\nnote on your desk: ‚ÄúAvailable for touch-down use.‚Äù\n\n‚óè\nPlease notify the Managing Director when you will be away for extended\nperiods (e.g., summer travel, sabbaticals) so that we may make temporary\nassignments of your assigned space, as needed.\n‚óè\nSharing of assigned desks can be arranged by users.\n5. Policy Review\n‚óè\nThis policy will be reviewed annually by the Executive Committee and\nmodified to best fit LEAP‚Äôs evolving needs.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/2023-spring-lectures/",
    "transcript": "2023 Spring Lectures - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 Spring Lectures in Climate Data Science Biweekly on Thursdays || Feb. 2, 2023 - May 25, 2023 3:30 ‚Äì 5:00 pm (EDT) IN-PERSON at the Tang Family Hall (Rm 202) at the Innovation Hub ( 2276 12th Ave, New York, NY ) VIRTUAL ATTENDANCE available Add to Calendar THURSDAY || FEB. 2, 2023 YouTube link PIERRE GENTINE Columbia University Physics to Machine Learning and Machine Learning Back to Physics Over the last couple of years, we have witnessed an explosion in the use of machine learning for Earth system science applications ranging from Earth monitoring to modeling. Machine learning has shown tremendous success in emulating complex physics such as atmospheric convection or terrestrial carbon and water fluxes using satellite or high-fidelity simulations in a supervised framework. However, machine learning, especially deep learning, is opaque (the so-called black box issue) and thus a question remains: what (new) understanding have we really developed? I will here illustrate the value of machine learning to understand or discover new processes in climate, with an application to rainfall organization. I will also present new tools merging causal discovery and machine learning to improve the trustworthiness and interpretability of machine learning for climate and physics applications. THURSDAY || FEB. 16, 2023 YouTube link JEAN-NO√â LANDRY Concordia University (Montr√©al) Climate Knowledge Transfer in a Time of Crises, Transitions, and New Collaborations Drawing from his recent experience at Columbia University‚Äôs Climate School, as well with environmental justice alliances and municipal public-private climate partnerships, Jean-No√© will identify knowledge transfer challenges in the context of shifts and transitions in inter-sectoral collaboration dynamics. From that basis, Jean-No√© will then discuss a series of considerations that strengthen trust, equity, and integrated action, including the new mediation competencies of data stewards, ecosystemic evaluation and monitoring processes, humility and community wisdom-centred approaches, and the strategic place of social infrastructure in knowledge exchanges. To conclude, Jean-No√© will situate these techniques and tactics in the emerging field and practice of intersystems mediation and describe the mutually reinforcing advantages of data mapping frameworks and community data agreements for future climate knowledge networks. THURSDAY || MAR. 2, 2023 YouTube link HOD LIPSON Columbia University Automating Discovery: From Cognitive Robotics to Particle Physics Can robots discover scientific laws automatically? Despite the prevalence of big data and machine learning, the process of distilling data into scientific laws has resisted automation. This talk will outline a series of recent research projects, starting with self-reflecting robotic systems, and ending with machines that can formulate hypotheses, design experiments, and interpret the results, to discover new physical variables and scientific laws. We will see examples from biology to cosmology, from classical physics to modern physics, from big science to small science. THURSDAY || MAR. 30, 2023 YouTube link ROSE YU UC San Diego Physics-Guided Deep Learning for Climate Dynamics Mathematical models and computer simulations are widely used tools for understanding complex dynamics in climate, materials and infectious disease. However, existing approaches are computationally expensive at high resolution, fundamentally hindering real-time decision making in science and engineering. While deep learning has shown tremendous promise in accelerating simulations, it remains a grand challenge to incorporate physical principles in a systematic manner into the design and training of such models. In this talk, I will demonstrate how to principally incorporate physics in deep learning models to accelerate simulations and assist decision-making. I will showcase the applications of these models to challenging problems in climate science and infectious disease modeling. THURSDAY || APR. 13, 2023 YouTube link CHRIS BRETHERTON Allen Institute for AI (AI2) Corrective Machine Learning for Interpretable Improvement of Climate Models AI2, with GFDL, has developed a corrective hybrid machine learning (ML) methodology to improve weather forecast skill and reduce climate biases in a computationally efficient coarse- grid climate model. The corrective ML is trained to emulate a time-dependent global reference by learning state-dependent ‚Äònudging tendencies‚Äô of temperature, moisture and winds. The reference can be a reanalysis (for present-climate simulation) or a finer-grid version of the same model that may be more trustworthy across a range of climates. The ML is interpreted as a correction to the combined physics parameterizations of the coarse-grid model. Unlike in other emulation approaches, heat and moisture conservation are built in. We train the ML on global 25 km reference simulations in multiple climates, and separately on a year-long 3 km simulation, and apply it in 200 km coarse-grid simulations. The ML reduces annual-mean land temperature and precipitation pattern biases by up to 50% and enhance s weather forecast skill. THURSDAY || APR. 27, 2023 YouTube link DAVID LAWRENCE NCAR Parameter Estimation for Improved Capacity of the Community Land Model for Actionable Science The Community Land Model (CLM) is the land component of the Community Earth System Model (CESM). As the science of climate change evolves from questions about how much global climate change there will be to questions of what we are going to do to mitigate and adapt to this climate, the requirements for Earth System models (ESMs) are changing. Here, I will review how land models in ESMs are evolving to be applicable to an ever-growing range of questions related to food and water security, effectiveness of nature-based carbon dioxide removal methods, ecosystem vulnerability, and changes in extremes. I will focus on how parameter estimation, coupled with other model advances, can improve the utility of CLM and CESM for this broadening range of climate science objectives. THURSDAY || MAY 4, 2023 YouTube link ADJI BOUSSO DIENG Princeton University Measuring And Enforcing Diversity In Machine Learning Diversity is important for many areas of machine learning, including generative modeling, reinforcement learning, active learning, and dataset curation. Yet, little effort has gone into formalizing and understanding how to effectively measure or enforce diversity. This talk will describe the Vendi Score, a new metric for measuring diversity that connects and extends ideas from ecology and quantum mechanics. The Vendi Score is defined as the Shannon entropy of the eigenvalues of a user-defined similarity matrix. It is general in that (1) it can be applied to any domain where similarity can be defined and (2) it doesn‚Äôt require defining a probability distribution over the collection to be evaluated for diversity. The Vendi Score can therefore be used to measure the diversity of datasets, samples from a generative model, outputs from decoding algorithms, or any collection for which we want to assess diversity. We will showcase the Vendi Score as a diversity evaluation metric in several domains and as a means to improve the exploration of molecular conformation spaces. THURSDAY || MAY 18, 2023 YouTube link MIKE PRITCHARD NVIDIA / UC Irvine Adventures in hybrid physics-AI climate modeling within academia and full AI weather prediction with NVIDIA. Low cloud forming turbulence is a key source of climate model prediction uncertainty that, despite seeming unapproachable to simulate on planetary scales, could soon come into computational range with hybrid machine learning methods. In Part 1, I will discuss a chain of recent work spanning UCI and Columbia driving in this direction that has tried to outsource explicit computations within ‚Äúmulti-scale‚Äù climate models to simple neural networks. Focus will be on the unsolved challenge of controlling stubborn prognostic error growth in such hybrid AI climate models and the emerging potential of physical renormalizations to achieve ‚Äúclimate invariance‚Äù and prognostic reliability. Some results emerging within LEAP trying to quantify the importance of such design decisions amidst the substantial noise of hyperparameter selection and prognostic pathologies will be included. In Part 2, I will discuss how sophisticated developments in Artificial Intelligence in industry have enabled professional data scientists to develop powerful scientific tools that greatly surpass the capabilities of traditional hand-written weather prediction codes. I will discuss NVIDIA‚Äôs AI powered medium range weather-forecast model FourCastNet, and its imminent successors, which exhibit skill on par with the top global NWP models, such as the Integrated Forecast System (IFS) model from the European Center for Medium Range Weather Prediction (ECMWF), but with nearly instantaneous results, using a tiny fraction of the computer hardware and power. Scientific implications for massive-ensemble predictions of tail risk hazard in a changing climate will be discussed. I will conclude with an outlook on how NVIDIA is leveraging AI and GPU-accelerated computing to prototype several new technologies relevant to the world‚Äôs efforts to develop digital twins that reflect the current and projected future states of our planet. THURSDAY || MAY 25, 2023 Register STEPHAN MANDT UC Irvine From Compression to Convection: a Latent Variable Perspective Latent variable models have been an integral part of probabilistic machine learning, ranging from simple mixture models to variational autoencoders to powerful diffusion probabilistic models at the center of recent media attention. Perhaps less well-appreciated is the intimate connection between latent variable models and compression, and the potential of these models for advancing natural science. I will begin by showcasing connections between variational methods and the theory and practice of neural data compression, ranging from constructing learnable codecs to assessing the fundamental compressibility of real-world data, such as images and particle physics data. I will then connect this lossy compression perspective to climate science problems, which often involve distribution shifts between unlabeled datasets, such as simulation data from different models or data simulated under different assumptions (e.g., global average temperatures). I will show that a combination of non-linear dimensionality reduction and vector quantization can assess the magnitude of these shifts and enable intercomparisons of different climate simulations. Additionally, when combined with physical model assumptions, this approach can provide insights into the implications of global warming on extreme precipitation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/2023-spring-lectures/#content",
    "transcript": "2023 Spring Lectures - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2023 Spring Lectures in Climate Data Science Biweekly on Thursdays || Feb. 2, 2023 - May 25, 2023 3:30 ‚Äì 5:00 pm (EDT) IN-PERSON at the Tang Family Hall (Rm 202) at the Innovation Hub ( 2276 12th Ave, New York, NY ) VIRTUAL ATTENDANCE available Add to Calendar THURSDAY || FEB. 2, 2023 YouTube link PIERRE GENTINE Columbia University Physics to Machine Learning and Machine Learning Back to Physics Over the last couple of years, we have witnessed an explosion in the use of machine learning for Earth system science applications ranging from Earth monitoring to modeling. Machine learning has shown tremendous success in emulating complex physics such as atmospheric convection or terrestrial carbon and water fluxes using satellite or high-fidelity simulations in a supervised framework. However, machine learning, especially deep learning, is opaque (the so-called black box issue) and thus a question remains: what (new) understanding have we really developed? I will here illustrate the value of machine learning to understand or discover new processes in climate, with an application to rainfall organization. I will also present new tools merging causal discovery and machine learning to improve the trustworthiness and interpretability of machine learning for climate and physics applications. THURSDAY || FEB. 16, 2023 YouTube link JEAN-NO√â LANDRY Concordia University (Montr√©al) Climate Knowledge Transfer in a Time of Crises, Transitions, and New Collaborations Drawing from his recent experience at Columbia University‚Äôs Climate School, as well with environmental justice alliances and municipal public-private climate partnerships, Jean-No√© will identify knowledge transfer challenges in the context of shifts and transitions in inter-sectoral collaboration dynamics. From that basis, Jean-No√© will then discuss a series of considerations that strengthen trust, equity, and integrated action, including the new mediation competencies of data stewards, ecosystemic evaluation and monitoring processes, humility and community wisdom-centred approaches, and the strategic place of social infrastructure in knowledge exchanges. To conclude, Jean-No√© will situate these techniques and tactics in the emerging field and practice of intersystems mediation and describe the mutually reinforcing advantages of data mapping frameworks and community data agreements for future climate knowledge networks. THURSDAY || MAR. 2, 2023 YouTube link HOD LIPSON Columbia University Automating Discovery: From Cognitive Robotics to Particle Physics Can robots discover scientific laws automatically? Despite the prevalence of big data and machine learning, the process of distilling data into scientific laws has resisted automation. This talk will outline a series of recent research projects, starting with self-reflecting robotic systems, and ending with machines that can formulate hypotheses, design experiments, and interpret the results, to discover new physical variables and scientific laws. We will see examples from biology to cosmology, from classical physics to modern physics, from big science to small science. THURSDAY || MAR. 30, 2023 YouTube link ROSE YU UC San Diego Physics-Guided Deep Learning for Climate Dynamics Mathematical models and computer simulations are widely used tools for understanding complex dynamics in climate, materials and infectious disease. However, existing approaches are computationally expensive at high resolution, fundamentally hindering real-time decision making in science and engineering. While deep learning has shown tremendous promise in accelerating simulations, it remains a grand challenge to incorporate physical principles in a systematic manner into the design and training of such models. In this talk, I will demonstrate how to principally incorporate physics in deep learning models to accelerate simulations and assist decision-making. I will showcase the applications of these models to challenging problems in climate science and infectious disease modeling. THURSDAY || APR. 13, 2023 YouTube link CHRIS BRETHERTON Allen Institute for AI (AI2) Corrective Machine Learning for Interpretable Improvement of Climate Models AI2, with GFDL, has developed a corrective hybrid machine learning (ML) methodology to improve weather forecast skill and reduce climate biases in a computationally efficient coarse- grid climate model. The corrective ML is trained to emulate a time-dependent global reference by learning state-dependent ‚Äònudging tendencies‚Äô of temperature, moisture and winds. The reference can be a reanalysis (for present-climate simulation) or a finer-grid version of the same model that may be more trustworthy across a range of climates. The ML is interpreted as a correction to the combined physics parameterizations of the coarse-grid model. Unlike in other emulation approaches, heat and moisture conservation are built in. We train the ML on global 25 km reference simulations in multiple climates, and separately on a year-long 3 km simulation, and apply it in 200 km coarse-grid simulations. The ML reduces annual-mean land temperature and precipitation pattern biases by up to 50% and enhance s weather forecast skill. THURSDAY || APR. 27, 2023 YouTube link DAVID LAWRENCE NCAR Parameter Estimation for Improved Capacity of the Community Land Model for Actionable Science The Community Land Model (CLM) is the land component of the Community Earth System Model (CESM). As the science of climate change evolves from questions about how much global climate change there will be to questions of what we are going to do to mitigate and adapt to this climate, the requirements for Earth System models (ESMs) are changing. Here, I will review how land models in ESMs are evolving to be applicable to an ever-growing range of questions related to food and water security, effectiveness of nature-based carbon dioxide removal methods, ecosystem vulnerability, and changes in extremes. I will focus on how parameter estimation, coupled with other model advances, can improve the utility of CLM and CESM for this broadening range of climate science objectives. THURSDAY || MAY 4, 2023 YouTube link ADJI BOUSSO DIENG Princeton University Measuring And Enforcing Diversity In Machine Learning Diversity is important for many areas of machine learning, including generative modeling, reinforcement learning, active learning, and dataset curation. Yet, little effort has gone into formalizing and understanding how to effectively measure or enforce diversity. This talk will describe the Vendi Score, a new metric for measuring diversity that connects and extends ideas from ecology and quantum mechanics. The Vendi Score is defined as the Shannon entropy of the eigenvalues of a user-defined similarity matrix. It is general in that (1) it can be applied to any domain where similarity can be defined and (2) it doesn‚Äôt require defining a probability distribution over the collection to be evaluated for diversity. The Vendi Score can therefore be used to measure the diversity of datasets, samples from a generative model, outputs from decoding algorithms, or any collection for which we want to assess diversity. We will showcase the Vendi Score as a diversity evaluation metric in several domains and as a means to improve the exploration of molecular conformation spaces. THURSDAY || MAY 18, 2023 YouTube link MIKE PRITCHARD NVIDIA / UC Irvine Adventures in hybrid physics-AI climate modeling within academia and full AI weather prediction with NVIDIA. Low cloud forming turbulence is a key source of climate model prediction uncertainty that, despite seeming unapproachable to simulate on planetary scales, could soon come into computational range with hybrid machine learning methods. In Part 1, I will discuss a chain of recent work spanning UCI and Columbia driving in this direction that has tried to outsource explicit computations within ‚Äúmulti-scale‚Äù climate models to simple neural networks. Focus will be on the unsolved challenge of controlling stubborn prognostic error growth in such hybrid AI climate models and the emerging potential of physical renormalizations to achieve ‚Äúclimate invariance‚Äù and prognostic reliability. Some results emerging within LEAP trying to quantify the importance of such design decisions amidst the substantial noise of hyperparameter selection and prognostic pathologies will be included. In Part 2, I will discuss how sophisticated developments in Artificial Intelligence in industry have enabled professional data scientists to develop powerful scientific tools that greatly surpass the capabilities of traditional hand-written weather prediction codes. I will discuss NVIDIA‚Äôs AI powered medium range weather-forecast model FourCastNet, and its imminent successors, which exhibit skill on par with the top global NWP models, such as the Integrated Forecast System (IFS) model from the European Center for Medium Range Weather Prediction (ECMWF), but with nearly instantaneous results, using a tiny fraction of the computer hardware and power. Scientific implications for massive-ensemble predictions of tail risk hazard in a changing climate will be discussed. I will conclude with an outlook on how NVIDIA is leveraging AI and GPU-accelerated computing to prototype several new technologies relevant to the world‚Äôs efforts to develop digital twins that reflect the current and projected future states of our planet. THURSDAY || MAY 25, 2023 Register STEPHAN MANDT UC Irvine From Compression to Convection: a Latent Variable Perspective Latent variable models have been an integral part of probabilistic machine learning, ranging from simple mixture models to variational autoencoders to powerful diffusion probabilistic models at the center of recent media attention. Perhaps less well-appreciated is the intimate connection between latent variable models and compression, and the potential of these models for advancing natural science. I will begin by showcasing connections between variational methods and the theory and practice of neural data compression, ranging from constructing learnable codecs to assessing the fundamental compressibility of real-world data, such as images and particle physics data. I will then connect this lossy compression perspective to climate science problems, which often involve distribution shifts between unlabeled datasets, such as simulation data from different models or data simulated under different assumptions (e.g., global average temperatures). I will show that a combination of non-linear dimensionality reduction and vector quantization can assess the magnitude of these shifts and enable intercomparisons of different climate simulations. Additionally, when combined with physical model assumptions, this approach can provide insights into the implications of global warming on extreme precipitation. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/2022fallseries/",
    "transcript": "2022 Fall Lectures - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2022 Fall Lectures in Climate Data Science Biweekly on Thursdays || Sept. 8, 2022 - Dec. 1, 2022 3:00 ‚Äì 5:00 pm (EDT) IN-PERSON at the Tang Family Hall (Rm 202) at the Innovation Hub ( 2276 12th Ave, New York, NY ) VIRTUAL ATTENDANCE available Add to Calendar THURSDAY || SEPT. 8, 2022 YouTube link DAVID ROLNICK McGill University Machine Learning¬† in Climate Change Mitigation and Adaptation Machine learning (ML) can be a powerful tool in helping society reduce greenhouse gas emissions and adapt to a changing climate. In this talk, we will explore opportunities and challenges in ML for climate action, from optimizing electrical grids to monitoring crop yield and biodiversity, with an emphasis on how to incorporate domain-specific knowledge into machine learning algorithms. We will also consider ways that ML is used in ways that contribute to climate change, and how to better align the use of ML overall with climate goals. THURSDAY || SEPT. 22, 2022 YouTube link LAURE ZANNA NYU Machine Learning for Ocean and Climate Modeling: advances, challenges and outlook Climate simulations, which solve approximations of the governing laws of fluid motions on a grid, remain one of the best tools to understand and predict global and regional climate change. Uncertainties in climate predictions originate partly from the poor or lacking representation of processes, such as ocean turbulence and clouds, that are not resolved in global climate models but impact the large-scale temperature, rainfall, sea level, etc. The representation of these unresolved processes has been a bottleneck in improving climate simulations and projections. The explosion of climate data and the power of machine learning (ML) algorithms are suddenly offering new opportunities: can we deepen our understanding of these unresolved processes and simultaneously improve their representation in climate models to reduce climate projections uncertainty? In this talk, I will discuss the advantages and challenges of using machine learning for climate projections. I will focus on our recent work in which we leverage machine learning tools to learn representations of unresolved ocean processes and improve climate simulations for illustration. Some of our work suggests that machine learning could open the door to discovering new physics from data and enhance climate predictions. Yet, many questions remain unanswered, making the next decade exciting and challenging for ML + climate modeling. THURSDAY || OCT. 6, 2022 YouTube link VIPIN KUMAR University of Minnesota Inverse Modeling via Knowledge-Guided Self-Supervised Learning: An application in Hydrology Machine Learning is beginning to provide state-of-the-art performance in a range of environmental a pplications such as streamflow prediction in a hydrologic basin. However, building accurate broad-scale models for streamflow remains challenging in practice due to the variability in the dominant hydrologic processes, which are best captured by sets of process-related basin characteristics. Existing basin c haracteristics suffer from noise and uncertainty, among many other things, which adversely impact model performance. To tackle the above challenges, in this talk, we present a novel Knowledge-guided Self-Supervised Learning (KGSSL) inverse modeling framework to extract system characteristics from driver(input) and response(output) data. This first-of-its-kind framework achieves robust performance even when characteristics are corrupted or missing. We evaluate the KGSSL framework in the context of stream flow modeling using CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) which is a widely used hydrology benchmark dataset. Specifically, KGSSL outperforms baseline by 16% in predicting missing characteristics. Furthermore, in the context of forward modelling, KGSSL inferred characteristics provide a 35% improvement in performance over a standard baseline when the static characteristic are unknown. THURSDAY || OCT. 20, 2022 YouTube link ELIZABETH BARNES Colorado State University Explainable AI for Climate Science: Detection, Prediction and Discovery Earth‚Äôs climate is chaotic and noisy. Finding usable signals amidst all of the noise can be challenging: be it predicting if it will rain, knowing which direction a hurricane will go, understanding the implications of melting Arctic ice, or detecting the impacts of human-induced climate warming. Here, I will demonstrate how explainable artificial intelligence (XAI) techniques can sift through vast amounts of climate data and push the bounds of scientific discovery. Examples include extracting robust indicator patterns of climate change and identifying Earth system states that lead to more predictable behavior weeks-to-years in advance. But machine learning models are only as capable as the scientists designing them. I will further discuss how climate science requires the crafting of domain specific XAI methods, both to gauge the trustworthiness of the XAI‚Äôs predictions and quantify uncertainty, but also to uncover predictable signals we didn‚Äôt know were there. Explainable AI can open doors to scientific understanding ‚Äî supporting scientists as we ask new questions about the coupled human-Earth climate system. THURSDAY || NOV. 3, 2022 ** AS OF 11/1/2022: This Lecture is postponed to a later date. ** MICHAEL PRITCHARD University of California at Irvine Adventures in Hybrid Physics: Machine Learning for Multi-scale Climate Modeling, AI-assisted Climate Model Inter-comparison, and ‚Ä¶ NVIDIA Low cloud forming turbulence is a key source of climate model prediction uncertainty that, despite seeming unapproachable to simulate on planetary scales, could soon come into computational range with hybrid machine learning methods. I will discuss a chain of recent work driving in this direction that has tried to outsource explicit computations within ‚Äúmulti-scale‚Äù climate models to simple neural networks. Focus will be on the unsolved challenge of controlling stubborn prognostic error growth in such hybrid AI climate models and especially the emerging potential of physical renormalizations to achieve ‚Äúclimate invariance.‚Äù Some emerging results trying to quantify the importance of such design decisions amidst the noise of hyperparameter selection will be included. Then, after a brief interlude on some fun ways to use more complicated forms of AI (VAEs) to help with analyzing high-resolution climate models, I will turn to an escalating adventure with industry, from my new perspective as a director of NVIDIA‚Äôs new climate simulation research group. Here, I will introduce the twin computational and societal missions of the company‚Äôs ‚ÄúEarth-2‚Äù climate initiative, and its current four-prong research strategy. This includes how a sophisticated data-driven global weather prediction model inspired by vision transformers, ‚ÄúFourCastNet,‚Äù is to be used to solve issues of latency and compression in serving high resolution climate predictions to society, following a vision by Bjorn Stevens, among other things. THURSDAY || NOV. 17, 2022 YouTube link PEDRAM HASSANZADEH Rice University Learning Data-driven Subgrid-Scale Models for Geophysical Turbulence The atmospheric and oceanic turbulent circulations involve a variety of nonlinearly interacting physical processes spanning a broad range of spatial and temporal scales. To make simulations of these turbulent flows computationally tractable, processes with scales smaller than the typical grid size of general circulation models (GCMs) have to be parameterized. Recently, there has been substantial interest (and progress) in using deep learning techniques to develop data-driven subgrid-scale (SGS) parameterizations for a number of key processes in the atmosphere, ocean, and other components of the climate system. However, for these data-driven SGS parameterizations to be useful and reliable in practice, a number of major challenges have to be addressed. These include: 1) instabilities arising from the coupling of data-driven SGS parameterizations to coarse-resolution solvers, 2) learning in the small-data regime, 3) interpretability, and 4) extrapolation to different parameters and forcings. Using several setups of 2D turbulence, as well as two-layer quasi-geostrophic turbulence, and Rayleigh-Benard convection as test cases, we introduce methods to address (1)-(4). These methods are based on combining turbulence physics and recent advances in theory and applications of deep learning. For example, we will use backscattering analysis to shed light on the source of instabilities and incorporate physical constraints to enable learning in the small-data regime. We will further introduce a novel framework based on spectral analysis of the neural network to interpret the learned physics and will show how transfer learning enables extrapolation to flows with very different physical characteristics. Time permitting, we will briefly mention some of the advances in supervised and semi-supervised learning of the SGS models, as well as the use of equation-discovery techniques. In the end, we will discuss scaling up these methods to more complex systems and real-world applications, e.g., for SGS modeling of atmospheric gravity waves. This presentation covers several collaborative projects involving Yifei Guan (Rice U), Ashesh Chattopadhyay (Rice U), Adam Subel (Rice U/NYU), Laure Zanna (NYU), and Andrew Ross (NYU). THURSDAY || DEC. 8, 2022 Register to attend JAKOB RUNGE German Aerospace Center (DLR) / TU Berlin Causal Inference, Causal Discovery, and Machine Learning In the past decades, machine learning has had a rapidly growing impact on many fields of natural-, life- and social sciences, as well as engineering. Machine learning excels at classification and regression tasks from complex heterogeneous datasets and can answer questions like, ‚ÄúWhat statistical associations or correlations can we see in the data?‚Äù, ‚ÄúWhat objects are in this picture?‚Äù, or ‚ÄúWhat is the most likely next data point?‚Äù But many questions in science, engineering, and politics are about ‚ÄúWhat are the causal relations underlying the data?‚Äù or ‚ÄúWhat if a certain variable changes or is changed?‚Äù or ‚ÄúWhat would have happened if some variable had another value?‚Äù Data-driven machine learning alone fails to answer such questions. Causal inference provides the theory and methods to learn and utilize qualitative knowledge about causal relations. Together with machine learning, it enables causal reasoning given complex data. Furthermore, causal methods can be used to intercompare and validate physical simulation models. In this talk, I will present an overview of this exciting and widely applicable framework and illustrate it with some examples from Earth sciences and beyond. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/events/2022fallseries/#content",
    "transcript": "2022 Fall Lectures - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP 2022 Fall Lectures in Climate Data Science Biweekly on Thursdays || Sept. 8, 2022 - Dec. 1, 2022 3:00 ‚Äì 5:00 pm (EDT) IN-PERSON at the Tang Family Hall (Rm 202) at the Innovation Hub ( 2276 12th Ave, New York, NY ) VIRTUAL ATTENDANCE available Add to Calendar THURSDAY || SEPT. 8, 2022 YouTube link DAVID ROLNICK McGill University Machine Learning¬† in Climate Change Mitigation and Adaptation Machine learning (ML) can be a powerful tool in helping society reduce greenhouse gas emissions and adapt to a changing climate. In this talk, we will explore opportunities and challenges in ML for climate action, from optimizing electrical grids to monitoring crop yield and biodiversity, with an emphasis on how to incorporate domain-specific knowledge into machine learning algorithms. We will also consider ways that ML is used in ways that contribute to climate change, and how to better align the use of ML overall with climate goals. THURSDAY || SEPT. 22, 2022 YouTube link LAURE ZANNA NYU Machine Learning for Ocean and Climate Modeling: advances, challenges and outlook Climate simulations, which solve approximations of the governing laws of fluid motions on a grid, remain one of the best tools to understand and predict global and regional climate change. Uncertainties in climate predictions originate partly from the poor or lacking representation of processes, such as ocean turbulence and clouds, that are not resolved in global climate models but impact the large-scale temperature, rainfall, sea level, etc. The representation of these unresolved processes has been a bottleneck in improving climate simulations and projections. The explosion of climate data and the power of machine learning (ML) algorithms are suddenly offering new opportunities: can we deepen our understanding of these unresolved processes and simultaneously improve their representation in climate models to reduce climate projections uncertainty? In this talk, I will discuss the advantages and challenges of using machine learning for climate projections. I will focus on our recent work in which we leverage machine learning tools to learn representations of unresolved ocean processes and improve climate simulations for illustration. Some of our work suggests that machine learning could open the door to discovering new physics from data and enhance climate predictions. Yet, many questions remain unanswered, making the next decade exciting and challenging for ML + climate modeling. THURSDAY || OCT. 6, 2022 YouTube link VIPIN KUMAR University of Minnesota Inverse Modeling via Knowledge-Guided Self-Supervised Learning: An application in Hydrology Machine Learning is beginning to provide state-of-the-art performance in a range of environmental a pplications such as streamflow prediction in a hydrologic basin. However, building accurate broad-scale models for streamflow remains challenging in practice due to the variability in the dominant hydrologic processes, which are best captured by sets of process-related basin characteristics. Existing basin c haracteristics suffer from noise and uncertainty, among many other things, which adversely impact model performance. To tackle the above challenges, in this talk, we present a novel Knowledge-guided Self-Supervised Learning (KGSSL) inverse modeling framework to extract system characteristics from driver(input) and response(output) data. This first-of-its-kind framework achieves robust performance even when characteristics are corrupted or missing. We evaluate the KGSSL framework in the context of stream flow modeling using CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) which is a widely used hydrology benchmark dataset. Specifically, KGSSL outperforms baseline by 16% in predicting missing characteristics. Furthermore, in the context of forward modelling, KGSSL inferred characteristics provide a 35% improvement in performance over a standard baseline when the static characteristic are unknown. THURSDAY || OCT. 20, 2022 YouTube link ELIZABETH BARNES Colorado State University Explainable AI for Climate Science: Detection, Prediction and Discovery Earth‚Äôs climate is chaotic and noisy. Finding usable signals amidst all of the noise can be challenging: be it predicting if it will rain, knowing which direction a hurricane will go, understanding the implications of melting Arctic ice, or detecting the impacts of human-induced climate warming. Here, I will demonstrate how explainable artificial intelligence (XAI) techniques can sift through vast amounts of climate data and push the bounds of scientific discovery. Examples include extracting robust indicator patterns of climate change and identifying Earth system states that lead to more predictable behavior weeks-to-years in advance. But machine learning models are only as capable as the scientists designing them. I will further discuss how climate science requires the crafting of domain specific XAI methods, both to gauge the trustworthiness of the XAI‚Äôs predictions and quantify uncertainty, but also to uncover predictable signals we didn‚Äôt know were there. Explainable AI can open doors to scientific understanding ‚Äî supporting scientists as we ask new questions about the coupled human-Earth climate system. THURSDAY || NOV. 3, 2022 ** AS OF 11/1/2022: This Lecture is postponed to a later date. ** MICHAEL PRITCHARD University of California at Irvine Adventures in Hybrid Physics: Machine Learning for Multi-scale Climate Modeling, AI-assisted Climate Model Inter-comparison, and ‚Ä¶ NVIDIA Low cloud forming turbulence is a key source of climate model prediction uncertainty that, despite seeming unapproachable to simulate on planetary scales, could soon come into computational range with hybrid machine learning methods. I will discuss a chain of recent work driving in this direction that has tried to outsource explicit computations within ‚Äúmulti-scale‚Äù climate models to simple neural networks. Focus will be on the unsolved challenge of controlling stubborn prognostic error growth in such hybrid AI climate models and especially the emerging potential of physical renormalizations to achieve ‚Äúclimate invariance.‚Äù Some emerging results trying to quantify the importance of such design decisions amidst the noise of hyperparameter selection will be included. Then, after a brief interlude on some fun ways to use more complicated forms of AI (VAEs) to help with analyzing high-resolution climate models, I will turn to an escalating adventure with industry, from my new perspective as a director of NVIDIA‚Äôs new climate simulation research group. Here, I will introduce the twin computational and societal missions of the company‚Äôs ‚ÄúEarth-2‚Äù climate initiative, and its current four-prong research strategy. This includes how a sophisticated data-driven global weather prediction model inspired by vision transformers, ‚ÄúFourCastNet,‚Äù is to be used to solve issues of latency and compression in serving high resolution climate predictions to society, following a vision by Bjorn Stevens, among other things. THURSDAY || NOV. 17, 2022 YouTube link PEDRAM HASSANZADEH Rice University Learning Data-driven Subgrid-Scale Models for Geophysical Turbulence The atmospheric and oceanic turbulent circulations involve a variety of nonlinearly interacting physical processes spanning a broad range of spatial and temporal scales. To make simulations of these turbulent flows computationally tractable, processes with scales smaller than the typical grid size of general circulation models (GCMs) have to be parameterized. Recently, there has been substantial interest (and progress) in using deep learning techniques to develop data-driven subgrid-scale (SGS) parameterizations for a number of key processes in the atmosphere, ocean, and other components of the climate system. However, for these data-driven SGS parameterizations to be useful and reliable in practice, a number of major challenges have to be addressed. These include: 1) instabilities arising from the coupling of data-driven SGS parameterizations to coarse-resolution solvers, 2) learning in the small-data regime, 3) interpretability, and 4) extrapolation to different parameters and forcings. Using several setups of 2D turbulence, as well as two-layer quasi-geostrophic turbulence, and Rayleigh-Benard convection as test cases, we introduce methods to address (1)-(4). These methods are based on combining turbulence physics and recent advances in theory and applications of deep learning. For example, we will use backscattering analysis to shed light on the source of instabilities and incorporate physical constraints to enable learning in the small-data regime. We will further introduce a novel framework based on spectral analysis of the neural network to interpret the learned physics and will show how transfer learning enables extrapolation to flows with very different physical characteristics. Time permitting, we will briefly mention some of the advances in supervised and semi-supervised learning of the SGS models, as well as the use of equation-discovery techniques. In the end, we will discuss scaling up these methods to more complex systems and real-world applications, e.g., for SGS modeling of atmospheric gravity waves. This presentation covers several collaborative projects involving Yifei Guan (Rice U), Ashesh Chattopadhyay (Rice U), Adam Subel (Rice U/NYU), Laure Zanna (NYU), and Andrew Ross (NYU). THURSDAY || DEC. 8, 2022 Register to attend JAKOB RUNGE German Aerospace Center (DLR) / TU Berlin Causal Inference, Causal Discovery, and Machine Learning In the past decades, machine learning has had a rapidly growing impact on many fields of natural-, life- and social sciences, as well as engineering. Machine learning excels at classification and regression tasks from complex heterogeneous datasets and can answer questions like, ‚ÄúWhat statistical associations or correlations can we see in the data?‚Äù, ‚ÄúWhat objects are in this picture?‚Äù, or ‚ÄúWhat is the most likely next data point?‚Äù But many questions in science, engineering, and politics are about ‚ÄúWhat are the causal relations underlying the data?‚Äù or ‚ÄúWhat if a certain variable changes or is changed?‚Äù or ‚ÄúWhat would have happened if some variable had another value?‚Äù Data-driven machine learning alone fails to answer such questions. Causal inference provides the theory and methods to learn and utilize qualitative knowledge about causal relations. Together with machine learning, it enables causal reasoning given complex data. Furthermore, causal methods can be used to intercompare and validate physical simulation models. In this talk, I will present an overview of this exciting and widely applicable framework and illustrate it with some examples from Earth sciences and beyond. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/LEAP-_-Climate-Impact-Scholars-June24.pdf",
    "transcript": "LEAP + Climatematch Academy:\nSupporting Climate Impact Scholars\nJune 19, 2024\nBy Catherine Cha\nLearning the Earth with Artificial Intelligence & Physics\n(LEAP),\na\nNational\nScience\nFoundation\n(NSF)-funded\nScience & Technology Center based at Columbia University,\nis thrilled to announce an additional step in our partnership\nwith Climatematch Academy, a global nonprofit on mission\nto expand equitable participation in scientific research.\nClimatematch Academy ‚Äì a part of Neuromatch, Inc., an\norganization\nwhose\nmission\nis\nto\naccelerate\nscientific\ninnovation by facilitating inclusive, collaborative, and global\nparticipation in the computational sciences ‚Äì represents a\ngrassroots\nnetwork\nof\nstakeholders\naround\nthe\nworld\ndedicated to creating a ‚Äúglobally diverse climate sciences community, trained on\ncutting-edge\ntechniques\nto\naccess\nand\nanalyze\nopen-source\nmodeled\nand\nobservational climate data.‚Äù\nEach summer, Climatematch hosts a global intensive online course, ‚ÄúComputational\nTools\nfor\nClimate\nScience,‚Äù\ndesigned\nto\nprovide\neducational\nand\nexperiential\nfoundations in computational climate science, accessing climate data sources, and\nhands-on climate modeling. Coursework converges with research projects and\nprofessional development opportunities, resulting in an ever-multiplying community\nof scientists and stakeholders whose goals are to strategically increase access to\neducation, mentor, and equip to create meaningful change.\nAs part of this effort, LEAP is\nexpanding\nits\ncollaboration\nwith Climatematch Academy\nto\nsupport\nClimatematch\nImpact\nScholars\nProgram\n(CISP)\nparticipants\nwith\nLEAP\nPangeo\nmembership\naccess\nso\nthat\ninterested\nstudents may build on their\nAcademy\nlearnings\nwith\na\nfirst\nresearch\nproject\n\nexperience. Current CISP research\nprojects - representing 56 Impact\nScholars\nfrom\n25 nations - and\ntheir geographical regions of focus\nmay be browsed here.\nTo date, seven (7) Climate Impact\nScholars\nhave\nsigned\nup to the\nLEAP Pangeo JupyterHub, gaining\naccess\nto\ncompute\nand storage*\nthat will\nenable them to take their\nprograms to the next level, and\ncontinue\ntheir\nresearch\ntoward\noutputs\nsuch\nas\npeer-reviewed\npublications,\ncurated\ndata\nsets,\nand published code. These research opportunities advance science by exploring new\nmethods of building collaborative research teams, and serve as significant steps in\ncareer development. LEAP-supported projects include ‚ÄúAssessment of Fire Events in\nArgentinian Andean-Patagonian Forests Between 2002-2020,‚Äù ‚ÄúObserving ECCO\nModel vs. Tidal Gauges Around Hurricane Maria,‚Äù and ‚ÄúOceanic Oscillations and\nCongo River Basin Climatology.‚Äù\nLearn more about Climatematch Academy.\nLearn more about LEAP Pangeo.\n* Special thanks to Google for providing compute and storage resources.\nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/2024-Momentum-Bootcamp-News.pdf",
    "transcript": "LEAP‚Äôs Winter 2024 Momentum Bootcamp in\nClimate Data Science Held at Columbia University‚Äôs\nTeachers College\nJanuary 12, 2024\nBy Catherine Cha\nOn January 10-11, 2024, a team from Learning the\nEarth\nwith\nAI\n&\nPhysics\n(LEAP), an NSF-funded\nScience and Technology Center (STC), presented the\nWinter 2024 Momentum Bootcamp on Climate\nData\nScience.\nThe\nBootcamp\nwas\noverseen\nby\nLEAP‚Äôs\neducation\nteam\nand\nTian\nZheng,\nLEAP‚Äôs\nDirector of Education, with instruction provided by\nCandace Agonafir, Postdoctoral Research Scientist at\nColumbia Climate School / LEAP, and Yu Huang, PhD\nstudent\nin\nColumbia\nEngineering‚Äôs\nGentine\nLab.\nJulius\nBusecke,\nAssistant\nResearch\nScientist\nat\nColumbia Climate School / Lamont-Doherty Earth Observatory‚Äôs Climate Data Science\nLab and LEAP‚Äôs Manager of Data + Computing, and Sammy Agrawal, LEAP Pangeo\nIntern, provided vital LEAP Pangeo technical support throughout Day 1.\n\nOver 30 students, scientists, and industry professionals from the private and public\nsectors, and as far away as Idaho and North Carolina, gathered at the Smith Learning\nCenter at Columbia University‚Äôs Teachers College for this two-day immersive, hands-on\neducational workshop designed to foster convergence of climate science and data\nscience, and promote vertically-integrated research and education. The intentionally\nintimate scope of this year‚Äôs Bootcamp facilitated a deeper dive into the concepts\ndiscussed,\npromoted\nnetworking\nand\ncollaboration\namong\nthe\ndiverse\nrange\nof\nattendees, and provided Huang and Agonafir with the opportunity to interact more richly\nwith participants.\nHuang kicked off the Bootcamp by introducing climate data and highlighting the utility of\nXarray ‚Äì a Python package designed to offer a more intuitive, more concise, and less\nerror-prone user experience when working with labeled multi-dimensional climate data.\nMeanwhile, climate topics ‚Äì such as the vertical profiles of the ocean and the El\nNi√±o-Southern Oscillation signal ‚Äì were discussed when exploring several datasets with\nXarray. She also led participants through the steps of accessing the extensive CMIP6\nclimate simulation datasets in the cloud with Pangeo tools and guided them through\npreprocessing, calculations, reductions, and visualization of multi-model outputs within\nXarray and xMIP packages. ‚ÄúI know it‚Äôs a lot of work to learn all of this in one day, but I\nhope they can feel more excited about the workflow of doing climate data science with\nthe help of the LEAP-Pangeo cloud platform,‚Äù Huang expressed.\nOn Day 2, Agonafir fostered a greater understanding of neural networks and machine\nlearning (ML), and their value in climate data science. She assisted participants in setting\nup their Jupyter environments, downloading the necessary data, and uploading codes,\nbefore leading them through lab sessions during which they created a basic neural\nnetwork model examining carbon dioxide and methane emissions. Together, the group\nsimulated\nprojected\nscenarios\nbased\non\nclimate\nchange\nmitigation\nstrategies\nto\ndetermine how air temperatures will be affected by these emissions in the future.\nAgonafir hopes that ‚Äúthe participants feel more comfortable using Python and Jupyter,\nand become excited about machine learning, artificial intelligence, coding, and the\nclimate.‚Äù\n\nThe instructors‚Äô goals were achieved, as attendees described increased comfort with ML\nand its practical applications. Sanketa Kadam, a first-year PhD student at Columbia\nClimate School / Lamont-Doherty Earth Observatory, came into the Bootcamp with a\nbasic understanding of what ML is, but ‚Äúit‚Äôs been really useful to really see it in practice,\nstep by step. [Reading about ML in papers is different from hearing Agonafir] go into\ndetails about how you validate, how there are\ndifferent\nactivation\nfunctions\nand\ndifferent\nthings you can do and tweak in your model.‚Äù\nKadam‚Äôs\ncolleague,\nYianna\nBekris,\napproached the Bootcamp from an extreme\nevents perspective, and found that ‚Äúlearning\nthe methods and how neural networks work\nhas been really helpful.‚Äù After the workshop,\nBekris\nbelieves\nthat\n‚ÄúML\ncould\nbe\nreally\nhelpful in answering a lot of the unanswered\nquestions in the biosphere.‚Äù\nJason\nJobe,\na\nColumbia\nBusiness\nSchool\nalumnus\nand\nnow a director at Slalom\nConsulting, attended the Bootcamp to learn about some of the tools being used and\npeople\nwho\nare\nworking\nto\ntackle\nclimate change-related issues that involve the\ninsurance industry. ‚Äú[This workshop] helps me to contextualize how close to potential\nsolutions we are with respect to risk identification and risk measurement,‚Äù he shared.\nGail Batutis, pursuing her Master‚Äôs degree at NYU, agreed: ‚ÄúA lot of this data is publicly\navailable; step one is to understand that data. Then you have to know the ML and layer\nit on top to gain insights from an overwhelming amount of data. With those two things\ntogether, you can create value‚Äù in industry and society.\nThe Bootcamp also made a huge impression on Jonathan Kingslake, a glaciologist and\nAssociate Professor in Columbia‚Äôs Department of Earth and Environmental Sciences and\nat the Lamont-Doherty Earth Observatory. ‚ÄúFor years, I was a typical geoscientist\nworking with hundreds and hundreds of Matlab scripts on my local computer, because I\nwas\nplaying\naround\nwith\ndatasets\nthat\nweren‚Äôt that large,‚Äù Kingslake recalls. ‚ÄúData\nin the cloud is new to me and to glaciology,\nbut\nthe\ncommunity\nis\ncatching\nup\n‚Äì\nglaciology datasets are in the cloud now. And\nwe finally have a large ice penetrating radar\ndataset\nthat\nneeds\nto\nget\ninto\na\nusable\nformat; the LEAP Pangeo project is a slick\nand\ncool\nnew way to process this data.‚Äù\nHaving never done any ML before, Kingslake\nis now working with LEAP postdoc Racheet\nMatai on a LEAP-funded research project,\n\nusing ML to improve descriptions of ice sheet-related processes. They expect to discover\nmore\nprecise\nlong\ntime-scale\npredictions\nof\nsea-level\nrise\ndue\nto\nmore\naccurate\ndescriptions of how ice slides over rock, resulting in better and more efficient planning\nfor things like building seawalls in the right place, at the right height.\nRelatedly, David Lee, legislative director for New York State Assemblymember Ron Kim,\nidentified\n‚Äúintegrating\nlegislative\nwork\nwith\nclimate\ndata\nscience,\nencouraging\ngovernment preparedness, being able to prognosticate extreme weather events, and\npursuing the appropriate infrastructure‚Äù as issues that governments should try to\naddress. Accordingly, ‚ÄúI‚Äôve just been really happy to be able to get right into challenging\nmaterial,‚Äù he stated.\nThe success of LEAP‚Äôs Winter 2024 Momentum Bootcamp underscores the growing\ninterest in leveraging ML and data science for climate research, and amplifies the\nimportance of collaborative efforts in addressing complex environmental challenges. As\nthe scientific community continues to address climate change, LEAP continues to provide\nopportunities\nsuch\nas\nthe\nBootcamp\nto\nserve\nas\nvaluable\nplatforms\nfor\nknowledge-sharing and skill-development, and drive the momentum toward utilizing\ntechnology for sustainable and resilient solutions in climate adaptation.\nClick here for more information\nabout LEAP‚Äôs education programs\nand upcoming events.\nCatherine Cha is the Senior Manager\nof Communications and Knowledge\nTransfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/November-2023-KLamb-Grant.docx.pdf",
    "transcript": "Kara Lamb and Pierre Gentine\nAwarded 3-Year Zegar Family Foundation\nGrant for ‚ÄúAI Cloud Seeding Project‚Äù\nNovember 10, 2023\nBy Catherine Cha\nDr.\nKara\nLamb,\nAssociate\nResearch\nScientist\nin\nColumbia\nUniversity‚Äôs\nDepartment of Earth and Environmental Engineering, and Dr. Pierre Gentine,\nMaurice\nEwing\nand\nJ.\nLamar Worzel Professor of Geophysics at Columbia\nUniversity, have been awarded a three-year $600,000 grant from the Zegar\nFamily Foundation to support their project, ‚ÄúOptimizing the Where‚Äôs and How‚Äôs\nof Cloud Seeding Using Artificial Intelligence‚Äù (the ‚ÄúAI Cloud Seeding Project‚Äù or\n‚Äúthe Project‚Äù).\nThe Zegar Family Foundation aims to make meaningful tangible impacts to\nimprove the world around us, and focuses its charitable giving in the key focus\nareas of environment and sustainability, and justice and human rights. Pursuant\nto its mission and giving focus, the Foundation is pleased to support the Project,\nbased at Columbia Engineering.\n\nThe three goals of the Project are:\n1. Improving the understanding of precipitation processes in wildfire-prone\nregions in the western United States, using machine learning;\n2. Evaluating strategies to optimize cloud seeding in atmospheric models\nthat simulate the atmospheric conditions present in the western U.S.; and\n3. Improving the understanding of aerosol-cloud-precipitation pathways in\nrealistic atmospheric conditions using the atmospheric models.\nAccordingly, the Project seeks to more efficiently predict where convection and\ncloud\nformation\noccurs,\nand understand how cloud seeding can maximize\ncumulative surface rainfall using AI.\nLamb, a PI co-leading a project on ice microphysics at the NSF-funded Science\n& Technology Center, Learning the Earth with Artificial Intelligence and Physics\n(LEAP), is personally interested in the wildfire mitigation question. ‚Äú[There are]\nseveral\nprevious\nstudies\non\nthe severity of wildfires and their air quality\nimpacts.\nUnderstanding\nthis\nproblem\nrequires\na\npretty\nsophisticated\nunderstanding of aerosol-cloud interactions, which is a very important question\nin\nthe\ncontext\nof\nclimate\nchange.‚Äù\nThe\nFoundation‚Äôs\ngrant\n‚Äúprovides\nan\nopportunity to explore fundamental science questions related to aerosol-cloud\ninteractions ‚Ä¶ [and] the methods we develop [in this Project] could also be used\nin the context of other similar problems in the geosciences [and help design]\nclimate adaptation and mitigation strategies.‚Äù\nClick here for more LEAP news and information about upcoming events.\nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/2023-Summer-REU-Review.pdf",
    "transcript": "LEAP‚Äôs Summer 2023 REU Program:\nConcluding Reflections\nAugust 7, 2023\nBy Catherine Cha\nLEAP‚Äôs Summer 2023 Research Experience for Undergraduates (REU) Program successfully\nconcluded with this year‚Äôs cohort offering final research presentations at the Columbia\nUniversity Innovation Hub on Thursday, July 27, 2023. The REU Program, conducted in\npartnership\nwith\nthe Summer at SEAS program, the DSI Scholars program, and the\nSignificant Opportunities in Atmospheric Research and Science (SOARS) program at UCAR,\nprovided undergraduate students from around the country the chance to pursue synergistic\ninnovations in data science and climate science, while engaging in enrichment, networking,\nand social activities.\nThis summer, six students ‚Äì Samarth ‚ÄúSammy‚Äù Agrawal (Massachusetts), Thomas Chen\n(New Jersey), Mark Irby-Gill (New York), Rebecca Porter (Kansas), Amanda Sun (California),\nand Subashree Venkatasubramanian (Washington) ‚Äì kicked off their REU immersion with a\n3-week-long\nMomentum\nBootcamp.\nUnder\nthe\ninstruction\nof\nCandace\nAgonafir\n(LEAP\npostdoc), Yu Huang (LEAP doctoral student\nand\n2023\nMomentum\nFellow),\nand Tian\nZheng (LEAP‚Äôs Chief Convergence Officer\nand Education Director), and with support\nfrom\nSungduk\nYu\n(LEAP\nResearch\nScientist)\nand\nJulius\nBusecke\n(LEAP‚Äôs\nManager of Data + Compute), the students\ngained skills and familiarity with the REU\ndataset, learned to apply machine learning\nto\nthis dataset, and developed research\n\nproposals. Then, they spent 5 weeks conducting their research under the guidance of\nresearch mentors Pierre Gentine (LEAP‚Äôs Director), Stephan Mandt (Associate Professor of\nComputer\nScience\nat\nUC\nIrvine),\nand\nMike\nPritchard\n(Director of Climate Simulation\nResearch at NVIDIA and LEAP‚Äôs Institutional Integration Director).\n‚ÄúIt\nwas\nan\ninvaluable\nexperience\nto\ngain\nexposure to machine learning concepts,‚Äù says\nAmanda,\na\nrising\nsenior\nstudying\nComputer\nScience\nand\nEnvironmental\nStudies\nat\nDartmouth College. Sammy, entering his junior\nyear\nin Columbia University‚Äôs Department of\nComputer\nScience,\nalso\nwanted\nto\ngain\nfamiliarity with the deep learning pipeline and a\nbetter understanding of climate dynamics: ‚ÄúI‚Äôve\nnever had a focused month to only think about\nresearch, and found it really rewarding. The REU definitely validated that research is\nsomething I love.‚Äù Mark was particularly thrilled about the ClimSim dataset with which the\ncohort was able to work: ‚ÄúThe foresight [the ClimSim team] had to create a dataset that will\nallow easier access to climate data will only increase the learning capabilities in this field of\nstudy.‚Äù\nIn addition to the intense learning atmosphere they shared,\nthe REU students also bonded as they roomed in neighboring\nsuites in Columbia‚Äôs East Campus dormitory. ‚ÄúI had no idea\nthat we would end up so close and that we would spend so\nmuch time together!‚Äù marvels Sammy. While special events\nlike an ‚Äòescape room‚Äô outing and a trip to see ‚ÄúHamilton‚Äù on\nBroadway\nwere\nsprinkled\nthroughout, the Summer 2023\ncohort\ntruly came together as they learned and studied\ntogether during the day and played cards or board games\nand explored local restaurants at night. ‚ÄúThere was a great\nbalance\nof\nshared\ninterest\nbut\ndiverse\nperspectives and\ngoals,‚Äù Sammy appreciated. Mark, a native New Yorker who\nwill pursue a project in preparation for his honors capstone\nat\nRed\nRocks\nCommunity\nCollege,\nalso\nloved\nthat ‚Äúthis\nsummer in NYC has allowed me to spend more time with my\nfamily than I have in over a decade.‚Äù\nThe students expressed particular gratitude for their REU instructors and mentors. ‚ÄúTian\nspent so long helping us, holding office hours, and giving really helpful suggestions that\nended up becoming our research projects,‚Äù reflects Amanda. Sammy gives a ‚Äúspecial\nshoutout to Yu Huang who put in many hours of overtime to help ensure our projects stayed\non track,‚Äù and Thomas agrees that ‚Äúthe PIs have been incredibly helpful.‚Äù ‚ÄúI would not have\nbeen about to accomplish what I did this summer without the help of the weekly PI\nmeetings and our meetings with our mentors,‚Äù adds Mark, who feels inspired to continue\nincorporating machine learning and AI into his future work.\n\nFor Huang, teaching in the REU Bootcamp\nrepresented\nher\nfirst\ntime\nteaching\nstudents, and she felt gratified to hear that\nthe\nstudents\nlearned\na\nlot.\nIt\nwas\nparticularly delightful ‚Äúto see the students\ndeveloping\nstronger\ncollaboration\nskills\nduring the process of sharing knowledge\nwith\neach\nother\nand\nsolving\ncoding\nproblems together.‚Äù\nAgonafir observed that the students ‚Äúhad the motivation to learn, and dove into the\ndatasets and the models, putting in good effort toward their individual projects.‚Äù Gentine\nwas also impressed by their ability to learn new concepts and develop research agendas,\nand enjoyed their fresh ideas as they discovered the field of climate data science. Mandt,\nthe cohort‚Äôs Data Science Research mentor, ‚Äúvalued how much they cared about complex\ntopics such as causal representation learning and generative modeling ‚Ä¶ they asked the\nright research questions.‚Äù\nWell done, Sammy, Thomas, Mark, Rebecca, Amanda,\nand Suba! It was a pleasure to host you this summer,\nand LEAP hopes you will stay in touch.\nThe Summer 2023 Research Presentations may be viewed below:\n‚óè\nSammy Agrawal: Capturing Convective Atmospheric Profiles Using Variational\nEncoder-Decoders\n‚óè\nThomas Chen: Visualizing Interpretability for Precipitation Prediction Using Shapley\nAdditive Explanations\n‚óè\nMark Irby-Gill: Evaluating XGBoost as a Baseline Model for Spatially-Informed\nPrecipitation Predictions\n‚óè\nRebecca Porter: Exploring Data-Driven Equation Discovery to Model Moisture Flux\n‚óè\nAmanda Sun: Improving Subgrid Parameterization with Causal Discovery\n‚óè\nSubashree Venkatasubramanian: A Bayesian-Gamma Deep Learning Approach to\nCapture Heavy-Tailed Behavior in the ClimSim Dataset\nClick here for more information about LEAP‚Äôs education programs and upcoming events.\nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/2023-Summer-Momentum-Fellowship.pdf",
    "transcript": "2023 SUMMER MOMENTUM FELLOWSHIP\nJuly 17, 2023\nBy Catherine Cha\nLEAP‚Äôs Summer Momentum Fellowship welcomes doctoral students in data science who\nare interested in participating in a summer research immersion in climate data science.\nMomentum Fellows are given the opportunity to apply their data science and/or machine\nlearning skills in climate modeling and develop research interests in climate data science.\nAs Fellows settle in to their workspaces at Columbia University‚Äôs Innovation Hub and\naccess LEAP resources such as LEAP Pangeo, Momentum Fellowship Faculty members,\nDhruv Balwada, Kara Lamb, Carl Vondrick, and Pierre Gentine, join them to work\nclosely on machine learning research problems in climate data science. Faculty members\nalso guide the Fellows to present their summer research at future LEAP events and other\nworkshops / conferences.\nThis summer, LEAP is thrilled to host three (3) Momentum Fellows who are pursuing various\nareas of research and professional development in climate data science:\n‚óè\nMatthew Beveridge\n‚óè\nMohammad Erfani\n‚óè\nYu Huang\nMatthew Beveridge\nBeveridge is a Visiting Researcher at the University of\nColorado-Boulder who will begin his doctoral program at\nColumbia University‚Äôs Columbia Imaging and Vision Lab\n(CAVE)\nwith\nProfessor\nShree\nNayar.\nHis\nchildhood\ninterests led Beveridge to MIT, where he earned his B.S. in\nElectrical Engineering and Computer Science and Master‚Äôs\ndegree in Engineering, while also studying Mathematics\nand Theater Arts. During his time at MIT, he worked on a\nrange of projects in computer vision, from imaging through\n\nscattering media to depth estimation. Building capable systems transformed into a desire to\nuse these systems to study phenomena, and he began to study the formation of Arctic sea\nice\nby\nmeans\nof\nsatellite\nimagery,\nnoticing\nthat\nEarth\nobservation\npresents\na great\nopportunity for computer vision to have a broader scientific impact. Beveridge‚Äôs current\nresearch focuses on computer vision, computational imaging, and machine learning for\nrobust perception and its application to the science of the physical environment. When not\nresearching, he has worked on spaceflight at NASA, contributes to startups in the field of\nautonomy, and organizes community events around energy and climate.\nThis summer, Beveridge joins Momentum Fellowship Faculty members, Kara Lamb and Carl\nVondrick, to work on their project, ‚ÄúEquivariant Graph Neural Networks for Efficient\nEmulation of Aerosol Optical Properties,‚Äù which focuses on improving how to estimate light\nabsorption and scattering from small aerosol particles. He has been researching how\nincluding physical constraints ‚Äì such as equivariance ‚Äì can improve the prediction accuracy\nof\ngraph\nneural\nnetworks,\nwhich\nemulate expensive aerosol optical properties codes.\nBeveridge is also exploring alternative machine\nlearning architectures, and thinking about what\nkinds of baselines and evaluation metrics can be\nused to evaluate the accuracy of these emulators.\nLamb\nobserves\nthat\n‚Äúit‚Äôs\nbeen\ngreat\nto\nsee\n[Matt‚Äôs] enthusiasm and motivation on the project\nso far - he has dived in and immediately started\nto master both the graph neural network code\nbase and the complexity of aerosol optics. I‚Äôm\nreally excited to see what other interesting ideas\nhe can bring to the project this summer.‚Äù\nMohammad Erfani\nErfani\nreceived\nhis\nMaster‚Äôs\ndegree\nin\nCivil\nand\nEnvironmental Engineering at Ferdowsi University (Iran)\nbefore joining the Department of Civil and Environmental\nEngineering at the University of South Carolina to work\nwith\nDr.\nErfan\nGoharian\nin\nthe\niWERS Lab. Erfani‚Äôs\nresearch\nfocused\non\nthe\ndevelopment\nof\nadvanced\nAI-based frameworks for hydrology and earth systems\nsciences, and he recently successfully defended his PhD\nin\ncivil\nengineering\nand\nhydrology. In the future, he\nhopes to work at the intersection of applied science,\nclimate, and machine learning.\nAs a 2023 Summer Momentum Fellow, Erfani is working with Dhruv Balwada on the\nproject, ‚ÄúEstimating Ocean Currents Using Measurements of Sea Surface Height (SSH).‚Äù\nTheir aim is to enhance the accuracy and efficiency of current estimation by harnessing the\npotential of artificial intelligence techniques. Erfani observes that ‚Äú[t]his integration of\nmachine learning and oceanographic data offers a promising avenue for improving our\n\nunderstanding\nof\ncomplex\noceanic\nprocesses.‚Äù\nHe\nhas\nbeen\ndeveloping\nconvolutional\narchitectures to address a problem similar to image denoising, for a particular challenge of\nthe project is that the SSH field is composed of signal from many different physical\nprocesses; thus, the research team must first identify the part that is most directly\nconnected to the ocean currents of interest. By the end of the\nFellowship, Erfani hopes to evaluate a range of machine learning\nmodels with varying complexity, and identify the aspects of the\nmachine learning pipeline to which the estimation problem is most\nsensitive. Throughout their collaboration, Balwada is particularly\nstruck by Erfani‚Äôs ‚Äúenthusiasm and eagerness to dive head-first\ninto challenging problems and new domains. Mohammad joined\nour team with little background in physical oceanography, [but]\nhas rapidly learned about the fields and quickly identified gaps\nwhere he could contribute, using his past machine learning and\nimage processing experience.‚Äù\nYu Huang\nHuang is a doctoral student in the Department of Earth +\nEnvironmental\nEngineering at Columbia University. She\nbegan\nher\njourney\nin\natmospheric\nsciences\nas\nan\nundergraduate student at Nanjing University (China), and\nher passion for studying land-atmosphere interactions and\nextreme events, such as heatwaves and heavy rainfall in\nthe\ncontext\nof a changing climate, grew further upon\njoining the Gentine Lab at Columbia. There, she gained\nextensive\nexperience\nin machine learning and remains\neager to stay abreast of the latest progress in big data,\nartificial intelligence and climate science.\nHuang‚Äôs professional development is profoundly impacted by her\nfather,\na\nmathematics\nteacher\nwho\ninspires\nand\ndrives\nher\nenthusiasm\nto\ncombine\nher\nexpertise\nin\nclimate\nscience and\nmachine learning for the benefit of other students. Under Pierre\nGentine‚Äôs\nadvisement,\nshe\nparticipates\nin\nthe\nMomentum\nFellowship as a PhD mentor and instructor for 2023 Summer\nREU (Research Experience for Undergraduates) Program\nparticipants.\nHuang\nis\napplying\nher\nown\ncomprehensive\nunderstanding of climate systems and climate models and helping\nsix REU students utilize the ClimSim dataset to explore how AI\nand machine learning techniques can enhance future climate projections and study physical\nmechanisms. She also collaborated on the creation of several tutorials aimed at familiarizing\nstudents\nwith\nclimate\nconcepts,\nteaching\nthe\nprocessing\nand\nvisualization\nof\nclimate\n(simulation) data, and preparing the climate data for machine learning algorithms. ‚ÄúBeing\nan instructor for the REU Bootcamp was a big challenge for me, monitoring the progress of\n\nsix students at the same time and addressing the wide range of technical and theoretical\ninquiries in different fields, [but] every ounce of effort I invested proved worthwhile when I\nwitnessed my students‚Äô growth, and they expressed they had learned a lot,‚Äù Huang shares.\n‚ÄúI enjoy working with knowledgeable PIs and creative undergraduate students from diverse\nbackgrounds. I am grateful for the opportunity to contribute to advancing climate research\nthrough mentoring and guiding the next generation of scholars.‚Äù\nLEAP‚Äôs 2023 Summer Momentum Fellowship concludes on August 11, 2023.\nClick here for more information about LEAP‚Äôs education programs and upcoming events.\nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/Sungduk-Yu-_-ClimSim-Paper.pdf",
    "transcript": "LEAP-funded paper pushes the limits of\nhybrid AI-physics climate modeling:\na chat with lead author Sungduk Yu\nJune 26, 2023\nBy Catherine Cha\nOn June 16, 2023, NSF Science and Technology Center, Learning the Earth with AI\n& Physics (LEAP) released our latest preprint, ‚ÄúClimSim: An Open Large-Scale\nDataset\nfor\nTraining\nHigh-Resolution\nPhysics\nEmulators\nin\nHybrid\nMulti-Scale Climate Simulators.‚Äù Led by Sungduk Yu, Assistant Project Scientist\nin the Department of Earth System Science at University of California Irvine, and in\ncollaboration\nwith\n52\nresearchers\nfrom\n19\ninstitutions,\nClimSim\nis\nthe most\nphysically\ncomprehensive\ndataset\nyet\npublished\nfor training machine learning\nemulators of atmospheric storms, clouds, turbulence, rainfall, and radiation for use\nin hybrid-ML climate simulation. It contains all inputs and outputs necessary for\ndownstream\ncoupling\nin a full-complexity multi-scale climate simulator, spans\nmultiple years at high sampling frequency, and is global in coverage.\n\nYu\nshared\nwith\nLEAP\nabout\nthe\nsignificance of\nClimSim\nand\nhis\nexperience\nin\ndeveloping\nthis\npaper with colleagues from around the world.*\nWhat is the significance of presenting the\nlargest-ever dataset designed for hybrid\nML-physics research?\nThe hybrid-ML simulators are not new at this point.\nFor example, for the super-parameterization\nproblem, several groups (e.g., Computational\nClouds and Climate Lab at UC Irvine, led by Mike\nPritchard, LEAP‚Äôs Institutional Integration Director, and the Gentine Lab at Columbia\nUniversity, led by Pierre Gentine, LEAP‚Äôs Director) have developed hybrid-ML climate\nsimulators.\nHowever,\nthe\nproblem\nis\nthat\nit\nis\nsomewhat\ndifficult\nto\ndistill\ngeneralizable\nlessons,\nwhich\ncan\nbe\nused\nfor\nadvancing\nhybrid-ML\nclimate\nsimulations, from the handful of existing studies that showcased hybrid-ML climate\nsimulators. Why? Because each hybrid-ML climate simulator was developed using\ndifferent\ningredients,\nfor\nexample,\ntraining\ndatasets\ngenerated\nfrom different\nclimate models and different sets of ML input and target variables. As a result, a\nclaim made from one case might not work in another case.\nThis is where ClimSim enters the picture. First, ClimSim is a publicly available\nstandardized dataset that anyone can use to test their new ideas about hybrid-ML\nsimulators. Second, ClimSim has a causatively complete set of input and target\nvariables for the hybrid-ML climate simulator, enabling a viable coupling strategy for\ncoupling to surface models. Finally, ClimSim is offered in a hierarchy of different\ncomplexities (e.g., high- vs. low-resolution, and real-geography vs. aquaplanet),\nallowing fast model prototyping and the possibility to measure ML algorithms‚Äô ability\nto learn physical systems at a different complexity.\nWhat excites you most about the potential uses + applications of ClimSim?\nWhat 'revolutions' in climate projection do you most hope to see?\nThe prospect of bringing in ML experts from other domains is very exciting. It is\nvirtually impossible to keep up with all the new ML algorithms and methods because\nthe\nfield\nis\ngrowing\nextremely\nfast. Hence, ClimSim aims to directly attract\ndedicated ML researchers to our climate problem. For this reason, we tried to\nminimize the use of jargon and to explain important climate modeling concepts that\nmight feel trivial to climate scientists. Likewise, we strategically chose a publishing\nplatform that is read widely in the ML community, NeurIPS, instead of dedicated\nearth science dataset journals.\n\nWhat revolutions do I hope to see? I want to see large ensemble climate projections\nfrom\nhybrid-ML\nclimate\nsimulators\ndeveloped\nusing\nClimSim.\nThis\nis\na\nlong-stretched goal, but I believe this is surely possible with collaborations between\nclimate scientists and ML experts from the computer science domain.\nTell us what it was like to collaborate with the 52 like-minded named\nauthors from 19 institutions on this paper.\nClimSim was developed by an intimidatingly long list of intimidatingly exceptional\nresearchers! I was so blessed to get to know them and to have a chance to work\nwith this amazing set of colleagues. Yes, we had a very short time to prepare for\nthe ClimSim manuscript, and as publication date approached, you can easily\nimagine that the work-life balance was nonexistent, especially during the last 2\nweeks. Looking back, I still can‚Äôt believe that we made it happen. I am as proud of\nour team as I am of ClimSim. Thank you all!!\nWhat\nwould\nyou\ntell\na\ngroup\nof\n1st-graders\nabout\nwhy\nClimSim\nis\nimportant?\nI would ask the 1st-graders to imagine they have an assignment to draw clouds in\nthe sky. They can draw any type of cloud. They can draw them using any medium:\npencil, watercolor, acrylic, charcoal, you name it. No matter what they choose, all\nstudents need a canvas where they can draw clouds. ClimSim is like the canvas ‚Äì\nan indispensable ingredient to finish a task. Then, I would tell them that ClimSim is\na huge ingredient for making accurate climate forecasts (as an analogy of weather\nforecasts) that can tell how the climate will look like when they grow up as big as\ntheir parents.\nFinally, tell us a bit about your journey to this point: what piqued your\npassion for climate data science, and how did this passion carry you from\nKorea to UC Irvine and through the ClimSim project?\nIt has been a long and winding journey to come this far. I studied mechanical\nengineering and biology in college, oceanography for my Master‚Äôs, and atmospheric\nscience for my Ph.D. I have also worked for a renewable energy development\ncompany specializing in CDM projects under the (now-expired) Kyoto Protocol for a\nfew years after college. They all seem unrelated, but the common theme here is\nthat I was following my curiosity. I never knew that I would be doing what I‚Äôm\ndoing today, but now I feel lucky that I ended up in climate data science, where I\ncan quench my scientific curiosities and, at the same time, contribute towards\nsolving the climate crisis we face.\n\n(On second thought, I guess my future as a computer modeler must have been\npresaged when as a little kid, I played games on an 8-bit computer that used\ncassette tapes as storage media. üòä)\n* Some questions and responses have been edited for clarity and conciseness.\nClick here for more LEAP news and information about upcoming events.\nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/04/IN-BRIEF_-2023-Lab-to-School-Summer-Institute-1.pdf",
    "transcript": "2023 LAB-TO-SCHOOL SUMMER INSTITUTE:\nIntegration Climate Education in NYC Public Schools\nThe 2023 Summer Institute Application is open!\n{ Deadline for applications is May 1, 2023. }\nThe Center for Sustainable Futures at\nTeachers College, Columbia University,\nand\nNYC\nPublic\nSchools\nOffice\nof\nSustainability are partnering to present the 2023 Summer Institute: Integrating\nClimate Education in NYC Public Schools. As the need and demand grows for the\nintegration of climate change education in schools, the Summer Institute will help\ncatalyze the preparation of K-5 teachers* to develop and lead lessons aligned with\nstandards- and place-based learning.\nA vital part of LEAP‚Äôs education programming, the 2023 Summer Institute constitutes\nfour full days of professional learning, bringing together faculty, doctoral students,\nexpert teachers, and leaders from the NYC Department of Education to create a\nlearning community that will explore, make sense of, and provide tangible support for\nincorporating climate change across different subject areas.\nThe 2023 Summer Institute is free.\nParticipants will receive a $1,000 stipend, and may earn up to 28 CTLE credits.\n* Subsequent summers will focus on preparing middle and high school teachers.\nQuestions? Please contact the Summer Institute team.\nTo request disability-related accommodations, please complete the Accommodations Request\nForm or contact the Office of Access and Services for Individuals with Disabilities (OASID)\n[phone: (212) 678-3689; video-phone: (646) 755-3144] as soon as possible.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/03/IN-BRIEF_-Summer-2023-Momentum-Fellowship-Application-Announcement.pdf",
    "transcript": "SUMMER 2023 LEAP MOMENTUM FELLOWSHIP\nCall for Applications\nThe Momentum Fellowship Application is open!\nLEAP‚Äôs Summer Momentum Fellows are doctoral students in data science (CS,\nStatistics, OR, and related fields) who are interested in having a summer research\nimmersion in climate data science, applying their data science/machine learning\nskills\nin\nclimate\nmodeling,\nand developing research interests in climate data\nscience. Each Fellow will receive a summer stipend, travel support, and access to\nLEAP resources.\n‚óè\nJune 5, 2023 - August 11, 2023 (avg. 25hrs / week)\n‚óè\n$8,000 summer stipend to support research and professional growth\n‚óè\nUp to $1,500 travel support for travel related to the project\n‚óè\nAdditional housing support will not be available\n{ Review of applications begins April 1, 2023 until all positions are filled. }\nSummer 2023 Momentum\nResearch Projects:\n‚ñ†\nAutomated Calibration Systems for CESM\nLand Component\n(Project Lead: Katie Dagon)\n‚ñ†\nEstimating Ocean Currents From\nMeasurements of Sea Surface Height (SSH)\n(Project Lead: Dhruv Balwada)\n‚ñ†\nEquivariant Graph Neural Networks for\nEfficient Emulation of Aerosol Optical\nProperties\n(Project Co-Leads: Kara Lamb, Carl Vondrick)\nQuestions?\nPlease email Jihae Moon, Asst. Director of Education Programs, at jm4509@columbia.edu.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/03/WEF-_-LEAP-Hoffmann-Fellowship-Announcement.pdf",
    "transcript": "LEAP Announces Andr√© Hoffmann\nFellowship with the World Economic\nForum (WEF)\nMarch 6, 2023\nThe Center for Learning the Earth with Artificial Intelligence & Physics\n(LEAP) at Columbia University is excited to announce a partnership between the\nWorld Economic Forum (WEF) and Columbia Business School to hire a\nHoffmann Fellow in Data & AI Systems for Global Climate Modeling.\nThis two-year Fellowship, primarily based at Columbia University with project\nevents occurring in Geneva, Switzerland and San Francisco, California, is intended\nto help identify the best path forward to leverage climate projection information and\nartificial\nintelligence\nfor\nclimate\naction\nat\nscale.\nThe\nFellow\nwill\nconduct\nstate-of-the-art research related to climate change and climate adaptation, and\nspearhead a project aimed at addressing barriers to scale hindering the effective\nuse of climate model data for climate action. Such a project may include working\ntoward inclusive and accessible climate datasets, increasing inclusive access to\nclimate projection data, particularly for developing nations, and strengthening\ndomain expertise and management capabilities to assist in making actionable\npolicy-making decisions.\nThe Hoffmann Fellow will report to and work with the Head of Artificial Intelligence\n&\nMachine\nLearning\nat\nthe\nWEF\nand\nVanessa\nBurbano, the Sidney Taurel\nAssociate\nProfessor\nof\nManagement\nat\nColumbia\nBusiness School and LEAP‚Äôs\nDirector of Corporate Engagement.\n\nThe Hoffmann Fellowship Programme, which offers early-career academics the\nopportunity to work at the intersection of society, science and technology through a\njoint\nappointment\nbetween\nthe World Economic Forum and leading academic\ninstitutions, is funded by Andr√© and Rosalie Hoffmann. Andr√© Hoffmann is a\nmember of the World Economic Forum‚Äôs Board of Trustees and co-chairs the\nHoffmann family foundation alongside his wife, Rosalie Hoffmann.\nFind out more and apply for the Hoffmann Fellowship in Data & AI Systems for\nGlobal Climate Modeling HERE.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2023/02/REU-2023_-Application-Announcement-FINAL.pdf",
    "transcript": "2023 SUMMER RESEARCH EXPERIENCE\nfor UNDERGRADUATES (REU)\nCall for Applications\nZoom Info Session\nApplications open February 24, 2023\n(March 3, 2023 at 1PM EDT)                  (Deadline: March 10, 2023 at 11:59PM EDT)\nThe Center for Learning the Earth with AI and Physics (LEAP) offers a summer\nResearch Experience for Undergraduates (REU) program on synergistic innovations\nin data science and climate science. This program is in partnership with the\nSummer\nat\nSEAS\nprogram,\nthe\nDSI\nScholars\nprogram\nand\nthe\nSignificant\nOpportunities in Atmospheric Research and Science (SOARS) program at UCAR. The\nREU\nprogram\nhosts\nundergraduate\nresearchers\nand\noffers\na\nwide\narray\nof\nenrichment learning and networking opportunities. The Center is committed to\nbuilding a diverse research community at the intersection of geosciences and data\nsciences\nwith the objective to build a LEAP community on par with the US\npopulation in terms of gender and race diversity.\nWe invite applications from students who will be a rising sophomore, junior, or\nsenior in Fall 2023 at a LEAP partner institution (Columbia/Barnard, NYU, University\nof California at Irvine, University of Minnesota), and current SOARS Prot√©g√©s. We\ninvite students from groups underrepresented in the STEM discipline in terms of\nrace,\nethnicity,\ngender,\nsocioeconomic\nbackground,\nfamily\nhistory\nof\npost-baccalaureate opportunity, gender identity and gender expression, disability,\nor military service to apply.\nImportant Dates:\n‚óè\nMarch 10: REU Application deadline\n‚óè\nMarch 31: REU Application decision notification\n‚óè\nJune 5 ‚Äì June 23: Momentum Bootcamp (Online)\n‚óè\nJune 25: Move-in to Columbia University campus\n‚óè\nJune 26 ‚Äì July 28: In-person Research Experience\n‚óè\nJuly 29: Move out\nQuestions?\nPlease email leap@columbia.edu or visit the LEAP Summer REU Program webpage.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/2023-TTT-Bootcamp-News.pdf",
    "transcript": "LEAP‚Äôs Train-the-Trainer Bootcamp on\nClimate Data Science Held at Columbia\nUniversity‚Äôs Teachers College\nJanuary 9, 2023\nBy Catherine Cha\nOn January 5 - 6, 2023, researchers from Learning the Earth with AI & Physics\n(LEAP), an NSF-funded Science and Technology Center (STC), presented the first\nannual Train-the-Trainer Bootcamp on Climate Data Science. The Bootcamp was\noverseen\nby\nLEAP‚Äôs education team and Tian Zheng, Professor and Chair of\nStatistics and LEAP‚Äôs Director of Education, and was hosted at the Smith Learning\nCenter\nat\nColumbia\nUniversity‚Äôs\nTeachers\nCollege.\nThis\ntwo-day\nimmersive,\nhands-on educational workshop fostered convergence of climate science and data\nscience,\npromoted\nvertically\nintegrated\nresearch\nand\neducation,\nand\nhelped\ncultivate\na\nbroad,\nmulti-disciplinary\nLEAP\nresearch\nand\nlearning\ncommunity.\nInstruction was provided by Ryan Abernathey, Associate Professor of Earth &\nEnvironmental\nStudies at Columbia University and LEAP‚Äôs Director of Data &\nComputation; Pierre Gentine, Professor of Earth and Environmental Engineering at\nColumbia University and LEAP‚Äôs Director; and Julius Busecke, Assistant Research\nScientist at Columbia University‚Äôs Climate Data Science Lab and LEAP‚Äôs Manager of\nData & Computing.\nAbernathey\ndescribed\nLEAP‚Äôs\ngoal to create, define, and train\npeople\nin\na\nnew\ndiscipline,\nClimate\nData\nScience:\n‚ÄúThis\nworkshop is really focused on\nthe data itself, the tools we use\nto work with that data, and the\nmethods\nwe‚Äôre\nusing\nwithin\nLEAP.‚Äù\nHe,\nGentine,\nand\nBusecke led attendees through\nbasic and next-level lessons on\n\nClimate Data Science in the cloud using Python tools. They prioritized collaboration,\nand aimed to prepare attendees for Machine Learning-enabled climate research, as\nwell as more intensive, research-oriented workshops in the future. ‚ÄúMy main\nmotivation is always making more people into climate scientists - at least, amateur\nclimate scientists,‚Äù stated Busecke. ‚ÄúThe essence of open-source, open-community\nis to [share knowledge] and get to data quickly so that you can actually really do\nthe science.‚Äù\nAccordingly,\nthe\nBootcamp\naudience\nwas\nintroduced\nto\nXarray,\na\nPython\npackage\ndesigned\nto\noffer\na\nmore\nintuitive, efficient, and accurate\nexperience when working with\nlabeled\nmulti-dimensional\narrays. ‚ÄúXarray is particularly\nwell-suited\nto\nworking\nwith\nclimate\ndata,‚Äù\ndescribed\nAbernathey.\n‚Äú[It]\nsolves\nthe\nproblem\nof\n‚ÄòHow\ndo\nwe\norganize these large, complex datasets within our computer code, and how do we\nefficiently do calculations with it?‚Äô Many scientists find Xarray really intuitive and\neasy to use. The result is that people can do pretty advanced things pretty easily,\nand they can move more quickly from idea to actual results.‚Äù\nGroup-oriented lab and team-hacking sessions throughout the two days of the\nBootcamp helped participants acclimate to using Xarray to explore open-access\nclimate datasets, calculate common climate statistics and diagnostics of variability\nand change, and perform open science in the cloud. ‚ÄúWe‚Äôve pretty much shown\nthat, in a day, you can get people the data, and they can actually develop a\nhypothesis and go to the pure science,‚Äù noted Busecke.\n\nBootcamp participants agreed. Yoni Nachmany, a former R&D engineer at The New\nYork Times, had not yet encountered Xarray in his work but came to understand\nwhy labeled variables are useful for working with multi-variable datasets and the\nbenefits of storing meta-data in Xarray datasets: ‚ÄúI enjoyed being able to actually\nengage with climate phenomenon like El Ni√±o, and produce output visualizations\nthat reflect something that would show up on an official site.‚Äù Arnab Kar, a Machine\nLearning specialist in the Finance & Operations department at Saks Fifth Ave., was\nimpressed by the variety of disciplines collaborating within LEAP: ‚ÄúI was blown away\nby the ease of the tools introduced.‚Äù\nCandace\nAgonafir is a postdoc research\nscientist\nworking\nwith\nTian\nZheng,\nProfessor\nand\nChair\nof\nStatistics\nat\nColumbia\nUniversity\nand\nLEAP‚Äôs\nChief\nConvergence\nOfficer\n/\nDirector\nof\nEducation. She also interacted with Xarray\nfor the first time at the Bootcamp and\nfound\nit\nuseful.\n‚ÄúI\nappreciate\nthe\nopportunity\nto\nstay\ncalibrated\nand\nstay\nfresh; there‚Äôs always something innovative\n‚Äì\nways\nto\nimprove\na\nmodel\nor\nmethodologies\nto\nadvance\ntoward\nan\nobjective,‚Äù she observed.\nHer thoughts resonate LEAP‚Äôs philosophy,\nenthusiastically\nexpressed\nby\nBusecke:\n‚ÄúWe need everyone from different disciplines, different backgrounds ‚Äì that‚Äôs our\nfuture. The planet basically depends on getting the most out of this amazing\namount of data. These model datasets are beautiful, and more people should look\nat them. They need to be explored more, and I hope what we‚Äôre doing here is\nenabling that.‚Äù\nClick\nhere\nfor more information\nabout LEAP‚Äôs education programs\nand upcoming events.\nCatherine Cha is the Senior Manager\nof Communications and Knowledge\nTransfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2022/10/10-4-22-News_PGentine-Award.docx.pdf",
    "transcript": "October 4, 2022\nPierre Gentine is 2022 AGU\nJames B. Macelwane Medal Recipient\nAGU press contact:\nHope Garland, news@agu.org (UTC-4 hours)\nLEAP communications contact:\nCatherine Cha, cc102@columbia.edu (UTC-4 hours)\nNew York ‚Äî Pierre Gentine, LEAP‚Äôs Director was announced as the American\nGeophysical Union‚Äôs (AGU) 2022 James B. Macelwane Medal recipient for significant\ncontributions\nto Earth and space science. AGU, a nonprofit organization that\nsupports 130,000 enthusiasts to experts worldwide in Earth and space sciences,\nannually recognizes a select number of individuals as part of its Honors and\nRecognition program.\nThe Macelwane Medal was named in honor of former AGU president James B.\nMacelwane (1953-1956) who was renowned for his contributions to geophysics.\nMacelwane was also deeply interested in teaching and encouraging scientists,\nfounding the Department of Geophysics at St. Louis University and serving as Dean\nof the Graduate School, along with various other roles, all while always committing\nto teach at least one course.\nGentine is recognized by the global Earth and space sciences community for his\ntremendous personal sacrifices and selfless dedication to advancing Earth and space\nsciences. In addition, he is honored for the depth and breadth of his research,\nimpact, and creativity, as well as his commitment to service, outreach, mentoring,\nand diversity.\nGentine is the Maurice Ewing and J. Lamar Worzel Professor of Geophysics in the\nDepartments of Earth and Environmental Engineering and Earth Environmental\nSciences at Columbia University. He also serves as the director of LEAP (Learning\nthe Earth with Artificial Intelligence and Physics), an NSF Science and Technology\n\nCenter (STC) launched in 2021. ‚ÄúI am absolutely delighted, humbled, and grateful\nto be the recipient of the AGU Macelwane Medal,‚Äù says Gentine. He continues to\nreflect, ‚ÄúLooking back, it is clear that the most important thing to me as a PI has\nbeen to build up a group and work with people most aligned with my values. When\nthe group has some shared values, we advance together, we are synergistic, and\nalso have fun working together. In our group, several of the core values are\ninnovation, diversity, open-mindedness and respect to others‚Äô point of view and\nculture, but also having a critical eye on scientific results. Those are aligned with\nmy key values.‚Äù\nGentine joins other scientists, leaders, educators, journalists, and communicators\nfrom around the world who have made outstanding achievements and contributions\nby pushing forward the frontiers of science. Each recipient embodies the AGU‚Äôs\ncommunity‚Äôs shared vision of a thriving, sustainable, and equitable future powered\nby discovery, innovation, and action. These recipients have worked with integrity,\nrespect, and collaboration while creating deep engagement in education, diversity,\nand outreach.\nAGU will formally recognize this year‚Äôs recipients during #AGU22 Fall Meeting,\n12-16 December 2022 in Chicago, IL and online everywhere. This celebration is a\nchance for AGU‚Äôs community to recognize the outstanding work of our colleagues\nand be inspired by their accomplishments and stories.¬†\n###\nAGU (www.agu.org) supports 130,000 enthusiasts to experts worldwide in Earth\nand\nspace\nsciences.\nThrough\nbroad\nand\ninclusive\npartnerships,\nwe\nadvance\ndiscovery and solution science that accelerate knowledge and create solutions that\nare\nethical,\nunbiased\nand\nrespectful of communities and their values. AGU‚Äôs\nprograms include serving as a scholarly publisher, convening virtual and in-person\nevents and providing career support. We live our values in everything we do, such\nas our net zero energy renovated building in Washington, D.C. and our Ethics and\nEquity Center, which fosters a diverse and inclusive geoscience community to\nensure responsible conduct.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2022/03/In-Brief_-LEAP-Partners-with-NCAR-for-TAI4ES-Summer-School.pdf",
    "transcript": "LEAP\nPartners\nwith\nNational\nCenter\nfor\nAtmospheric Research (NACR) for Trustworthy\nArtificial Intelligence for Environmental Science\n(TAI4ES) Summer School\nMarch 17, 2022\nThe Center for Learning the Earth with Artificial Intelligence and Physics\n(LEAP) announced today that it will partner with the National Center for\nAtmospheric Research (NACR), one of LEAP‚Äôs critical partner institutions, for\nthis summer‚Äôs Trustworthy Artificial Intelligence for Environmental Science\n(TAI4ES) Summer School.\nThe TAI4ES Summer School will include lectures from leading researchers in\nthe field, with a focus on how to develop trustworthy AI for the earth and\nenvironmental sciences. The experience will feature a machine learning\ntrust-a-thon, where students will gain hands-on experience with evaluating\nthe\ntrustworthiness\nof\nAI\nto\nsolve\nreal-world\nenvironmental\nscience\nchallenges. The five-day program will begin June 27 and conclude on July 1,\n2022.\nFor more information, please contact leap@columbia.edu or visit the TAI4ES\nSummer School webpage.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2022/03/In-Brief_-LEAP-Launches-Summer-REU-Program.pdf",
    "transcript": "LEAP\nLaunches\nInaugural\nSummer\nResearch\nExperience for Undergraduates (REU) Program\nFebruary 17, 2022\nThe Center for Learning the Earth with Artificial Intelligence and Physics\n(LEAP) announced a call for applications today for the inaugural Research\nExperiences for Undergraduates (REU) Program this summer at Columbia\nUniversity.\nThe LEAP Summer REU Program will host undergraduate researchers and\nprovide experiences on synergistic innovations in data science and climate\nscience. The program also consists of enrichment learning and networking\nopportunities, including seminars and workshops, and is in partnership with\nthe Summer at SEAS program, the DSI Scholars program and the Significant\nOpportunities in Atmospheric Research and Science (SOARS) program at\nUCAR.\nAn\ninformation\nsession\nwill\nbe\nheld\non\nFebruary\n24\nand\nthe\napplication deadline is March 3. The eight-week program will begin June 6\nand conclude on July 29, 2022.\nFor more information, please contact leap@columbia.edu or visit the LEAP\nSummer REU Program webpage.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2025/01/2025-Momentum-Bootcamp-CTLE-News-v2.pdf",
    "transcript": " \n \n \nLEAP‚Äôs Winter 2025 Momentum Bootcamp‚Äã\nReceives Approval to Issue Continuing Teacher‚Äã\nand Leader Education (CTLE) Credits  \n \n \n \n \n \n \n \nJanuary 19, 2025‚Äã\nby Catherine Cha \n \n \nIn September 2024, the Momentum Bootcamp on Climate Data Science offered by \nLearning the Earth with AI & Physics (LEAP), an NSF-funded Science and Technology \nCenter (STC), received approval by the New York State Education Department (NYSED) \nto issue Continuing Teacher and Leader Education (CTLE) credits to teachers and \neducation leaders who seek to satisfy certificate renewal requirements. The NYSED \ndetermined that LEAP‚Äôs Momentum Bootcamp satisfied a wide range of program criteria, \nincluding offering content that is research-based and provides educators with \nopportunities to analyze, apply, and engage in research, and promotes technological \nliteracy and facilitates the effective use of all appropriate technology. \n \nOn January 13-14, 2025, three middle and high school teachers participated in the \nWinter 2025 Momentum Bootcamp on Climate Data Science offered by Learning \nthe Earth with AI & Physics (LEAP), an NSF-funded Science and Technology Center \n(STC). This immersive, hands-on educational workshop, held at the Smith Learning \n\n \nCenter at Columbia University‚Äôs Teachers \nCollege was designed to foster convergence \nof climate science and data science, and \npromote vertically-integrated research and \neducation, and was overseen by LEAP‚Äôs \neducation team and Tian Zheng, LEAP‚Äôs \nDeputy Director / Director of Education, with \ninstruction provided by Qingyuan Yang and \nCandace \nAgonafir, \nAssociate \nResearch \nScientists at Columbia University. Julius \nBusecke, Assistant Research Scientist at \nColumbia Climate School / Lamont-Doherty \nEarth Observatory‚Äôs Climate Data Science \nLab and LEAP‚Äôs Manager of Data and \nComputing, \nprovided \nvital LEAP Pangeo \ntechnical \nsupport. \nBootcamp participants \nwere also served by a volunteer team of \nMachine Learning Coaches - Aya Lahlou, \nDavid Lee, and Xingyao Li - who provided \ntechnical and mentoring support on data \nmanipulation \nand \nneural \nnetworks, \nand \nhelped attendees expand their modeling \nskills and gain insights into projected climate \noutcomes. \n \nJulia Demin (Primoris Academy, Westwood, NJ), Alejandro Mundo (Kingsbridge \nInternational High School, Bronx, NY), and Debbie Ogedengbe (M.S. 129, Bronx, NY) \njoined over 30 participants, including undergraduate and graduate students, scientists, \nand colleagues from LEAP partners such as the American Museum of Natural History \n(AMNH), to gain skills in climate data science and coding tools. These educators were \nsupported by fellow participant and 2024 Momentum Bootcamp alumnus, John Park, \n(Principal Academic and Research Technologies, Teachers College Digital Futures \nInstitute).  \n \nAs an alumnae of LEAP‚Äôs 2024 Summer Institute, \nOgedengbe was already equipped to revamp and \nupdate her weather and climate curricular unit, and \npropose the launch of a Climate Action club where \nher \nmiddle \nschool \nstudents \ncould \nfocus \non \nEarth-friendly service projects.  \nShe applied to \nparticipate in the 2025 Winter Momentum Bootcamp \nin order to gain a deeper understanding of how \nscientists use machine learning (ML) and artificial \n\n \nintelligence (AI) to study the Earth‚Äôs climate. ‚ÄúI want to be able to teach my students \nabout these approaches to acquiring and utilizing scientific knowledge and data, [as well \nas provide] opportunities for critical thinking and problem-solving,‚Äù Ogedengbe \nexplained. ‚ÄúThey can study how scientists create models using climate data to make \npredictions about future scenarios, and then possibly use those models to make more \npredictions. The possibilities are endless!‚Äù \n \nMundo, another LEAP 2024 Summer Institute alumnus, \nviews climate data science as a powerful tool that will allow \nhis Earth Science students to engage deeply with \nreal-world data through visualizations. ‚ÄúOpportunities like \n[this Bootcamp] empower educators to stay at the \nforefront of innovation and translate cutting-edge practices \ninto accessible, hands-on learning experiences,‚Äù Mundo \nreflected. \n‚ÄúThis \nalso \ninspires \nmy \nstudents \nto \nsee \nthemselves \nas \ncapable \nof \ncontributing \nto \ncritical \nconversations around climate science, making abstract \nconcepts more tangible and preparing them with skills that \nare relevant and transferable beyond the classroom.‚Äù \n \nDemin was particularly interested to explore how AI tools can help innovate and adapt \nher computer science program. ‚ÄúThis Bootcamp provides an opportunity to align my \nteaching with emerging trends, ensuring my students are prepared for the challenges of \ntomorrow [by connecting] scientific research with real-world problems such as analyzing \nlocal environmental impacts or exploring solutions for global challenges,‚Äù she shared. ‚ÄúBy \nintroducing climate data science in the classroom, we can empower students to develop \ncritical thinking, sustainability, coding, and data analysis skills while fostering their sense \nof responsibility toward environmental stewardship.‚Äù \n \nLearning to code with Python proved to be a steep \nlearning curve for these participants, but steadfast \nsupport and coaching from Park and Yang kept them \non track throughout the two-day Bootcamp. The \nteachers expressed eagerness to continue improving \nthis newly-acquired skill, with Mundo also feeling \nenthusiastic to learn more about convolutional neural \nnetworks and their applications in forecasting climate \nchanges.  \n \nDenim, Mundo, and Ogedengbe also seek more \ncollaborations \nbetween \neducators \nand \nclimate \nscientists \nin \norder \nto generate innovations in \nclimate-related lessons. ‚ÄúTranslating the scientists‚Äô research into resources that can be \n\n \nused in education is crucial for bringing complex concepts into the classroom in an \naccessible way,‚Äù encouraged Mundo. Ogedengbe affirmed that ‚Äúas an educator, I would \nlike climate and data scientists to know that the work they do is invaluable and truly \nappreciated.‚Äù \n \nIn order to fulfill CTLE credit requirements, Demin, Mundo, and Ogedengbe must now \ncomplete additional post-Bootcamp tasks, including creating (1) an instructional syllabus \nthat demonstrates alignment with ML and climate-based topics, (2) an ML and \nclimate-based lesson plan with specific focus on the integration of ML concepts with \nclimate science, and (3) a fully runnable coding example that demonstrates the ML \nmethod taught during Bootcamp and uses real-world or simulated climate data to \nshowcase practical application. LEAP looks forward to remaining connected with this first \nBootcamp cohort of CTLE credit recipients, encouraging them in their professional and \neducational journeys, and building an interdisciplinary community that drives the \nmomentum toward utilizing technology for sustainable and resilient climate solutions. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nClick here for more information about LEAP‚Äôs education programs and upcoming events. \n \nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP. \n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/12/LEAP-KT-White-Paper.pdf",
    "transcript": " \nLEAP: Understanding Climate Modeling Across \nSocietal Sectors \n‚Ä®\nBy Catherine Cha, Geneva List, Molly Lopez, Tian Zheng‚Ä®\nDecember 2024\n____________________________________________________________________________\nIntroduction \n‚Ä®\nClimate change poses multifaceted challenges that require an interdisciplinary approach, \nintegrating scientific modeling with diverse applications to address complex societal impacts. \nClimate modeling is an indispensable tool across multiple sectors, supporting decision-making \nfrom government policy and healthcare to cultural preservation and public engagement. At \nLEAP (Learning the Earth with Artificial Intelligence and Physics), a science and technology \ncenter funded by the National Science Foundation (NSF), we want to ensure that our team is \nconducting impactful research and producing actionable data and tools to increase the reliability, \nutility, and reach of climate models and projections. \nThis paper highlights insights garnered from a panel of LEAP‚Äôs public-sector partners and \ncollaborators who shared how climate data and models are utilized in city government, public \nhealth, humanitarian advocacy, and cultural institutions. The report also identifies existing gaps \nand challenges, and suggests opportunities for advancing climate data's critical impact on social \nresilience, public trust, effective science communication, and equitable adaptation to climate \nrisks.\nUse Cases and Applications of Climate Modeling \n‚Ä®\n1. Climate Modeling in City Government: The Case of New York \nCity \n‚Ä®\nThe New York City Mayor‚Äôs Office of Climate and Environmental Justice (MOCEJ) integrates \nclimate data into long-term planning and policy and plays a coordinating role across New York \nCity agencies that address climate issues. Hayley Elszasz, Climate Science Advisor at MOCEJ, \nspotlights the city‚Äôs efforts to map and understand social vulnerability to flooding and heat risks. \nMOCEJ currently relies on downscaled climate projections and information about climate, risk, \nand vulnerabilities provided by the New York City Panel on Climate Change (NPCC). Tools like \nthe Environmental Justice NYC Mapping Tool (EJNYC), released in spring of 2024, facilitate \nfine-grain neighborhood-level assessments, informing resource allocation, resilience-building, \nand legislation.\n\n- from ‚ÄúNYC Climate Data‚Äù‚Ä®\nHayley Elszasz (October 18, 2024)\nWhen considering the use of a climate data tool, Elszasz emphasizes that public accessibility is \nthe first metric to be applied. Additional key considerations include the degree of specificity of \nthe data to New York City, integration of local infrastructure (e.g., sewers and storm drain \nsystems), compatibility with existing computing resources utilized by City agencies, and \napplicability to policymaking and legislation.\nMOCEJ identifies the following primary challenges with respect to each of the above \nconsiderations, which hamper efforts to provide the average New York City resident with the \nclimate adaptation information and tools they need:\n‚Ä¢\nData Update Frequency: Current tools often lag behind the pace of climate change, \nimpacting their accuracy.\n‚Ä¢\nAccess and Legislation: Public access to precise, local risk information is vital for \nequitable policy but remains limited.\n‚Ä¢\nPolicy Integration: Climate data must be compatible with city infrastructure and \nadaptable for regulatory use, especially as climate risks intensify.\nElszasz invites LEAP - and the climate and data science communities at large - to work on \ndemocratizing access to climate data, and be intentional about equipping the public with \naccurate, local information about climate risk, and more accessible data tools for better \ndecision-making. She suggests the following opportunities:\n‚Ä¢\nDeveloping adaptive tools that can keep pace with climate change, factor in the \ncompound impacts of climate hazards, and visualize how climate change will impact \ndifferent neighborhoods throughout New York City\n\n‚Ä¢\nDesigning the most efficient and effective climate adaptation measures so that \nlimited resources may be used wisely to build climate resilience and protect the  most \nvulnerable citizens\n‚Ä¢\nEnhancing public access to these resources to democratize data, allowing \ncommunities to advocate for protective legislation more effectively\n‚Äú\nThere are communities in New York City already experiencing \nclimate change, already experiencing frequent tidal flooding or \nstormwater flooding, who know the risks intimately. We have to be \nthoughtful when we‚Äôre communicating risk - what options or solutions \nare we providing along with that information? That‚Äôs why we need to \nhave an all-of-government, all-of-academia approach to think through \nthe solutions we are offering to people.‚Äù\n     \n                     - Hayley Elszasz‚Ä®\nClimate Science Advisor‚Ä®\nNYC MOCEJ\n2. Climate Data in Public Health Research \n‚Ä®\nMarianthi-Anna Kioumourtzoglou, Associate Professor of Environmental Health Sciences at the \nColumbia Mailman School of Public Health, focuses on climate and health research, with a \nparticular focus in moving from analysis to action. She explains that climate models are \nessential for linking environmental conditions to health outcomes, offering high-resolution data \nover long timeframes. \nIn a study that analyzed data between 1995 and 2014, Kioumourtzoglou and her team observed \nlinks between heat and instances of myocardial infarctions, with cumulative risk of increasing \ntemperatures. In another study, they identified all tropical cyclones that made landfall in the \nUnited States between 1988 and 2018 (wind speeds of 34 knots or above), and observed that \nseveral categories of adverse health effects (e.g., cardiovascular events, infectious disease \noccurrence, neuropsychiatric conditions, respiratory disease) and mortality risk increased and \nremained elevated for months after the cyclonic events. \n- from ‚ÄúClimate & \nHealth Research: \nFrom Analysis to \nAction‚Äù‚Ä®\nMarianthi-Anna \nKioumourtzoglou \n(October 18, 2024)\n\nWhile such studies are helpful to note and understand the links between climate change and \nadverse health outcomes, Kioumourtzoglou describes two main challenges in informing future \npolicies and interventions:\n‚Ä¢\nPolicy Translation: There is often a delay in converting research findings into policy \ndeterminations due to gaps in data precision, the need for rigorous validation, and lack \nof public understanding or acceptance of the need for climate resilience planning and \naction.\n‚Ä¢\nSpatio-Temporal Resolution: High-resolution models are essential, especially in urban \nsettings where exposure can vary dramatically across neighborhoods and interventions \nmust be tailored accordingly.\nKioumourtzoglou advises that climate and data scientists may contribute significantly by \ndesigning interventional scenarios based on location, studies to test the effectiveness of these \nscenarios, and outcome projections to anticipate intended and unintended consequences, all \nwith the goal of protecting those in society who are most vulnerable to the impacts of climate \nchange. Particular opportunities may include: \n‚Ä¢\nAugmenting traditional epidemiological studies with focus on location-specific, \nmodifiable health interventions, in order to more effectively protect local communities. \n‚Ä¢\nInterdisciplinary collaborations with urban planners and local governments to inform \nthe design of adaptive, health-centric policies.\n‚Ä¢\nEstablishing regular practice of clear communication and translation of climate and \ndata science and research.\n‚Äú\nBringing people to the table is essential for creating local \nchange, testing proposed changes, and identifying the most \neffective solutions for each location. Often science is in one place, \neducation is in another, the public is in another, and community-based \norganizations are in yet another place. We [climate scientists, public \nhealth experts, urban planners, government officials, everyone] need to \nwork together from the beginning to integrate authentic experiences of \nthe science outcomes through direct communication with community-\nbased organizations and educational organizations.‚Äù\n- Marianthi-Anna Kiourmourtzoglou‚Ä®\nAssociate Professor of Environmental Health Sciences‚Ä®\nColumbia University Mailman School of Public Health\n3. Humanitarian Advocacy and Legal Frameworks for Climate-\nDisplaced Persons \n‚Ä®\nThe International Center for Advocates Against Discrimination (ICAAD) works to combat \nstructural discrimination by intersecting legal innovation and human-centered design, and \nleveraging multidisciplinary teams and local organizations and communities to improve \nresilience, safety, and equity across systems. \n\nICAAD co-founder and legal innovator Hansdeep Singh describes that in a decade‚Äôs work in the \nPacific Islands region to address the systemic barriers that impede survivors of gender-based \nviolence from navigating the justice sector, ICAAD found that a significant relationship exists \nbetween intimate partner violence and climate shocks (e.g., storms, landslides, floods). While \nthese results may, in the immediate term, be explained by food and economic insecurity or \namplified inequities in traditional household roles following major climate events, it has become \nmore evident to Singh and his team that Pacific Islanders face significant existential threats from \nenvironmental disasters that are increasing in either intensity or frequency because of climate \nchange.\n - from ‚ÄúCan Climate Modeling Protect One‚Äôs Right to Life With Dignity?‚Äù‚Ä®\nHansdeep Singh (October 18, 2024)\nBecause climate-induced displacement threatens the cultural fabric and physical safety of \ncommunities in this region, ICAAD advocates for a legal framework to recognize the ‚Äúright to life \nwith dignity‚Äù for those impacted by environmental degradation. The organization‚Äôs efforts to \ncreate a legal evidentiary standard for courts to use in evaluating whether one‚Äôs right to life with \ndignity is at risk of being violated implicates climate modeling with respect to accuracy and \ntimescales. Theoretically, Singh explains, climate modeling evidence presented in a courtroom \ncould show the probability of a violation of an asylum seeker‚Äôs right to life with dignity, and that \nthis violation would occur within the applicant‚Äôs lifetime. \nKey barriers that stand in the way of confirming this evidentiary standard include:\n‚Ä¢\nLocalized Data Needs: Existing climate models, in addition to lacking downscaled data, \noften fail to account for culturally significant variables, such as impacts on traditional \nagriculture and human well-being in particular communities.\n‚Ä¢\nLegal Evidence Standards: Creating legally robust evidence from climate projections \nremains complex, especially with compounded climate effects.\n\nAccordingly, Singh urges the climate and data science community to consider opportunities to:\n‚Ä¢\nIntegrate traditional ecological knowledge into downscaled climate models for \nremote regions that could support legal standards for climate-displaced persons.\n‚Ä¢\nCollaborate with local experts and communities to gain insight into traditional \npractices that are at risk, integrating cultural preservation into advocacy.\n‚Äú\nWhen we‚Äôre looking at the cultural practices that are going to be \nimpacted by climate change, we look at the agricultural models \nthat often predict issues around food insecurity. One of the major \nfeatures of design justice is that we should design on the margins. If \nwe design on the margins, we‚Äôre likely to include not only those \nwho have been forgotten, but everyone else as well.‚Äù\n- Hansdeep Singh‚Ä®\nCo-Founder + Legal Innovator ‚Ä®\nICAAD\n‚Ä®\n4. Museums and Public Engagement through Data Visualization \n‚Ä®\nMuseums like the American Museum of Natural History (AMNH) in New York City play an \nincreasingly vital role in climate change education. AMNH utilizes data visualization and \ninteractive exhibits to engage the public in understanding climate change science, impact, and \nmitigation options. The museum‚Äôs climate display includes real-time data dashboards and \ninteractive tools to show temperature trends, climate systems, and regional climate impacts. \nVivian Trakinski, the Director of Science Visualization at AMNH, is continuously considering \nways to infuse the Museum‚Äôs climate-related exhibits with expanded and current science and \ndata. In this vein, she and AMNH colleagues have been studying opportunities for the Museum \nto engage with audiences on climate change, particularly in light of challenges posed by the \nneed for:\n‚Ä¢\nHopeful Engagement: Museums must balance presenting the harsh realities of climate \nchange projection and adaptation science with hopeful messaging that empowers the \npublic to take action.\n‚Ä¢\nLocalized Data Accessibility: Accurate, local climate projection data is challenging to \nobtain for exhibit development, limiting the specificity and impact of engagement.\nThe Museum has discovered that exhibits that foster emotional and social connections to \nclimate facilitate more impactful education about climate change and the need for adaptation. \nFurther, it is important to demonstrate how communities can help to ameliorate and adapt to \nprojected variations in climate. Climate projections illustrating negative impacts (e.g., a \ncontinuously warming Earth will lead to losing swaths of the rainforest) must be balanced with \ngood news (e.g., there are technological advancements that can help ameliorate negative \nimpacts) and examples of how collective action can help build a better climate future. The \nMuseum strives to present climate change education that inspires imagination and creative \nthinking.\n\n- from ‚ÄúConnecting Climate Projections With Action‚Äù‚Ä®\nVivian Trakinski (October 18, 2024)\nTrakinski encourages climate and data scientists to take advantage of opportunities to:\n‚Ä¢\nProvide museums with downsampled, accessible datasets for students and the \npublic, to promote hands-on learning experiences.\n‚Ä¢\nCollaborate with community organizations to create localized, positive scenarios can \nfoster an engaged and hopeful response to climate action.\n‚Äú\nIt really comes down to why are we educating people about climate \nchange? The more relevant you can make the data and the \nprojections to local communities‚Äìwhether it's flooding that's happening \nmore often or the impacts of heat‚ÄìI think the greater chance we have of \nthem engaging in the science. Especially if we're offering them a ray of \nhope and suggested actions to take.‚Äù\n- Vivian Trakinski‚Ä®\nDirector of Science Visualization‚Ä®\nAmerican Museum of Natural History\nConclusion \n‚Ä®\nLEAP found that one of the most helpful aspects of this convened panel discussion was \ndiscovering several points of commonality regarding both the challenges in climate data \nutilization that persist across sectors, and opportunities to leverage climate data and models for \nenhanced adaptation and resilience.\n\nCross-Sector Challenges and Gaps in Climate Data Utilization \n1.\nData Localization and Resolution: High-resolution, local data is crucial but often \nunavailable or difficult to integrate, limiting climate models‚Äô precision.\n2.\nInterdisciplinary Communication: Effective climate adaptation strategies require \ncollaboration across sectors. However, disciplinary silos can prevent shared \nunderstanding and hinder comprehensive policy development.\n3.\nData Democratization: Making climate data accessible to the public fosters trust and \nempowers communities, yet most data remains complex or inaccessible without \nspecialized knowledge.\n4.\nFunding for Adaptation Research: Urban planning, public health, and advocacy all \nrequire funding to develop adaptive responses to climate risks, yet securing resources \nremains a challenge.\nRecommendations for Enhanced Climate Data Impact \n1.\nFostering Interdisciplinary Collaboration: Bringing together climate scientists, urban \nplanners, public health experts, legal advocates, and educators to create targeted, \nactionable insights for local adaptation.\n2.\nDeveloping Downscaled, Culturally Relevant Models: Designing models that \nincorporate social and cultural variables can enhance the relevance of data for \nadvocacy, public engagement, and targeted interventions.\n3.\nBuilding Data Pipelines for Local Engagement: Collaborations with local communities \nand educational institutions can create accessible data tools and resources that \nempower communities to act on climate information.\n4.\nIntegrating Positive Framing and Actionable Insights in Public Messaging: Framing \nclimate action in terms of achievable goals and highlighting community-level impacts can \nstrengthen public trust and inspire proactive engagement.\nClimate modeling is essential for shaping informed responses to climate risks across public \nsectors. By advancing localized data access, fostering cross-sector collaboration, and \nprioritizing accessibility, climate models can serve as powerful tools to drive climate adaptation \nand resilience on both local and global scales. This paper‚Äôs case studies underscore the \npotential for interdisciplinary efforts to yield data-driven, community-focused solutions that \nrespond effectively to the challenges of our changing climate.\n_________________________________________________\nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP.\nGeneva List is a Senior Staff Associate at LEAP.\nMolly Lopez is the Managing Director of LEAP.\nTian Zheng is a Professor + Chair of the Department of Statistics at Columbia University,‚Ä®\nand the Deputy Director, Chief Convergence Officer, and Education Director at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/IN-BRIEF_-Winter-2025-Train-the-Trainer-Bootcamp-Announcement-4.pdf",
    "transcript": "WINTER 2025 : MOMENTUM BOOTCAMP\nON CLIMATE DATA SCIENCE\nCall for Applications\nDate:\nJanuary 13‚Äì14, 2025\nSchedule: 9am‚Äì5pm (EDT), both days\nVenue:\nSmith Learning Theater at Teachers College, Columbia University\nThe\nNational\nScience\nFoundation-funded\nLearning\nthe\nEarth\nwith\nArtificial\nintelligence and Physics (LEAP) Science and Technology Center (STC),\na large\nmulti-institutional\ncenter\nworking\nto\nimprove\nclimate\nprojections\nusing\nnovel\nartificial intelligence for better climate adaptation, invites registration for the LEAP\nMomentum Bootcamp.\nThis immersive, hands-on event is designed by LEAP researchers and is part of the\ncenter's educational programming. It is intended to foster convergence of climate\nscience and data science; vertically integrate research and education; and forge a\nLEAP research and learning community including K-12 teachers, PhDs, postdocs,\nfaculty, and other stakeholders.\nThe two-day Bootcamp aims to teach Climate Data Science in the cloud using\npython tools with an emphasis on reproducibility and collaboration. Participants will\nlearn through a variety of activities including lectures, labs, and team hacking,\nwhich will prepare participants for machine learning-enabled climate research and\nmore advanced and intensive research-oriented workshops.\nApplications are welcome from:\n‚óè\nFaculty members and research scientists from LEAP institutions (Columbia,\nNYU, University of Minnesota, University of California at Irvine, NASA/GISS,\nNCAR)\n‚óè\nPostdocs and PhD students\n‚óè\nResearch scientists\n‚óè\nNYC Public School teachers, Summer Institute\n‚óè\nLEAP Partners\n‚óè\nMembers of the general public\n\nPre-requisites:\n‚óè\nBasic knowledge of scientific computing with Python: numpy, matplotlib,\npandas\n‚óè\nAccess to LEAP-Pangeo Hub (participants will be granted access upon\nacceptance)\nBootcamp participants will:\n‚óè\nDiscover,\naccess,\nand\nexplore\nopen-access\nclimate\ndatasets,\nincluding\nsatellite\nobservations\nand\nclimate\nsimulations, using the Xarray python\npackage.\n‚óè\nCalculate common climate statistics and diagnostics of variability and change\nusing Xarray.\n‚óè\nPerform interactive visualization of climate data using the Holoviews package.\n‚óè\nPerform machine learning on spatio-temporal climate data\n‚óè\nCompare machine learning models‚Äô performance and prediction skill\n‚óè\nPerform open science in the cloud using the LEAP-Pangeo Jupyter Hub\n‚óè\nBe required to bring their own laptop / charger.\nAPPLICATION DEADLINE:\nDecember 16, 2024 at 11:59 PM (EDT).\n‚óè\nApplicants will be contacted by December 20, 2024 to complete registration and\nsecure their spot.\n‚óè\nThe registration fee ($250) will cover participant meals and operational costs.\n‚óè\nYou may be eligible for partial or full financial aid (e.g., LEAP researcher/students,\nNYC DoE teacher, non-profit partner). Please respond to the last question on the\napplication form for consideration.\nPlease contact LEAP with any questions.\nSeats are limited, so apply today!\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/LEAP-Subseasonal-Forecasting-Hackathon-v2-January-2025-Open-Call-for-Registration.pdf",
    "transcript": "LEARNING THE EARTH WITH\nARTIFICIAL INTELLIGENCE & PHYSICS\nA Hackathon + Bidirectional Learning Opportunity:\n‚ÄúHarnessing Machine Learning to Improve\nSubseasonal-to-Seasonal Climate Predictions‚Äù\nOPEN CALL FOR APPLICATIONS\nOVERVIEW:\nThe National Science Foundation-funded Learning the Earth with Artificial Intelligence and Physics\n(LEAP) Science and Technology Center (STC), a large multi-institutional center effort working to\nimprove climate projections using novel artificial intelligence (AI) for better climate adaptation, along\nwith Columbia University‚Äôs Data Science Institute, Amazon Web Services (AWS) and NVIDIA, invites\napplications to participate in, ‚ÄúHarnessing Machine Learning to Improve Subseasonal-to-Seasonal\nClimate Predictions‚Äù on January 15-16, 2025.\nWe welcome people from all disciplines and levels of data science familiarity to attend, as a central\ngoal of this event is to broaden access to the climate data science field. This includes community\nmembers who are interested in learning more about data and climate adaptation and data scientists\nwho work closely with climate data and AI. Hackathon activities will accommodate various technical\nand social areas of expertise. We particularly encourage people from the Global South or other areas\nof the world disproportionately impacted by the climate crisis to apply; travel support may be\navailable for a limited number of participants traveling to the event (additional details are in the\napplication form).\nThis hackathon is unique in that we would value input from a broad range of participants, including\nthose who a) study climate science and/or data science, b) can help us understand diverse needs or\napplications for using subseasonal forecasting, and c) anyone interested in learning more about how\ndata can support climate adaptation. If you‚Äôre curious, keep reading and join us!\nBACKGROUND:\nAccurate climate predictions are critical to all aspects of climate adaptation efforts across all spheres\nof society. Climate predictions at subseasonal-to-seasonal scales - which range from a few weeks to a\nLEAP.COLUMBIA.EDU\n\nLEARNING THE EARTH WITH\nARTIFICIAL INTELLIGENCE & PHYSICS\nfew months and is known as ‚Äúsubseasonal-to-seasonal (S2S) forecasting‚Äù - play a crucial role in\nvarious sectors, including agriculture, energy, water resource management, public health, and disaster\npreparedness. While not as immediate as short-range weather forecasts or as long-term as climate\nprojections, subseasonal-to-seasonal forecasts are a critical bridge between these two-time scales,\noffering insights into medium-range weather patterns and trends.\nSubseasonal-to-seasonal forecasting is critical for mitigating risks, enhancing decision-making, and\noptimizing outcomes in various sectors. For example, in agriculture, knowing the likelihood of a\ndrought or heavy rainfall two weeks in advance can allow farmers to adapt irrigation and planting\nplans to mitigate the impact of adverse weather conditions on their crops. In the energy sector,\nanticipating extreme temperature or wind patterns can enable decision-makers to optimize resource\nallocations.\nSubseasonal-to-seasonal forecasts also play a vital role in disaster preparedness and response.\nPredicting the likelihood of extreme weather events, such as hurricanes, heatwaves, or heavy\nprecipitation weeks in advance enables authorities to issue early warnings, evacuate vulnerable areas,\nand implement measures to reduce the impact of disasters on communities and infrastructure. This\nproactive approach can save lives, minimize property damage, and improve overall resilience to\nnatural hazards.\nAmong climate data researchers, subseasonal-to-seasonal forecasting contributes to improving\nclimate modeling. By analyzing atmospheric, oceanic, land, and glacial conditions over intermediate\ntime scales, scientists can gain insights into the mechanisms driving climate variability and change.\nThis, in turn, informs the development of more accurate long-term climate models and predictions,\nwhich enables policymakers and other stakeholders to develop effective climate change mitigation\nand adaptation strategies that support community resilience.\nIn recent years, advancements in technology, such as improved numerical weather prediction models\nand enhanced computing power, have enhanced the skill and reliability of subseasonal forecasts.\nMachine learning algorithms and data assimilation techniques have also been employed to better\nincorporate observational data and improve forecast accuracy.\nYet, accurate climate predictions remain a major challenge for the climate community because of\ninherent uncertainties in atmospheric, oceanic, land, and glacial processes; model biases; and the\nLEAP.COLUMBIA.EDU\n\nLEARNING THE EARTH WITH\nARTIFICIAL INTELLIGENCE & PHYSICS\ncomplex interactions between different components of the Earth system. Addressing these challenges\nrequires ongoing research and collaboration among scientists and stakeholders.\nGiven\nthese\nchallenges,\nthis\nhackathon\nwill\nfocus\non\nimproving\nthe\nprediction\nof\nsubseasonal-to-seasonal climate time scales (from a few days to a few weeks) using machine learning.\nThe hackathon will provide historical data that will be used to train the machine learning algorithms\nand for validation of the strategy on unseen data (years left out from the training).\nThe hackathon will also include a ‚Äúgreen‚Äù computing component where the simplest models with high\nskill will be rewarded, as it aligns with our mission to reduce greenhouse gas emissions.\nFor those with climate and data science research backgrounds, the following resources provide\nadditional background:\n‚óè\nhttp://www.s2sprediction.net/\n‚óè\nhttps://journals.ametsoc.org/view/journals/bams/103/12/BAMS-D-22-0046.1.xml\nFor those coming from other disciplines and areas of expertise, the following links provide additional\ndefinitions and background:\n‚óè\nTo learn more about climate modeling at LEAP, check out this 2-minute video:\nhttps://www.youtube.com/watch?v=JjLcZUjnl-w\nHACKATHON GOAL:\nThis hackathon challenges teams of data science students, professionals working in the climate sector,\nand interested community members to build innovative demonstrations and machine-learning\nsolutions for sub-seasonal climate modeling and prediction. Participants will use a newly released\nbenchmarking dataset, ChaosBench (please read here and here for a description), to 1) illustrate the\nskill and limitations of current predictive tools, 2) explore the value of such predictions for\ndownstream applications, or 3) improve current models by integrating machine learning, physics, and\nother domain knowledge. Teams can supplement this dataset with additional data sources to enrich\ntheir project. Projects will be evaluated on real-world relevance, innovative integration of machine\nlearning and domain science, clear presentation, and the effective use of external data.\nMENTORSHIP AND COLLABORATION:\nWe‚Äôre inviting mentors from both private and public sectors who will act as potential consumers of\nsub-seasonal forecasts. Mentors will guide students in shaping their project ideas, aligning them with\nLEAP.COLUMBIA.EDU\n\nLEARNING THE EARTH WITH\nARTIFICIAL INTELLIGENCE & PHYSICS\nreal-world applications. Researchers from our sponsoring organizations will participate as coaches,\noffering technical expertise and advice throughout the hackathon.\nELIGIBILITY\nThis hackathon is open to participants at all skill levels and from a range of disciplines and data\nneeds in support of LEAP‚Äôs commitment to broadening participation in climate data science and\nmaking LEAP data and code broadly accessible. We aim to support travel to the Hackathon for a\nlimited number of participants from the Global South and areas most gravely impacted by the climate\ncrisis.\nDATE + LOCATION\nThis hackathon will be held in-person at Columbia University in New York City on January 15-16,\n2025. For participants who would like to learn more about coding prior to participating in the\nHackathon, please consider joining LEAP‚Äôs optional pre-Hackathon event, the Momentum Bootcamp.\nJUDGING AND AWARDS:\nStakeholders from the private and public sectors will attend the final presentations and rank the\ndemos based on specific evaluation criteria. Top teams will earn recognition and prizes, along with\nopportunities for networking with climate and data science professionals.\nREGISTRATION\nTo apply for participation in this hackathon, please complete this form by December 16, 2024\nIf you have questions, please email LEAP@columbia.edu, with ‚ÄúHackathon‚Äù in the subject line.\nLEAP.COLUMBIA.EDU\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2024/11/Nov24-ClimateML-Methods-Workshop-News.pdf",
    "transcript": "LEAP‚Äôs Junior Scientists Advisory Group (JSAG)\nPresents ClimateML Methods Workshop\nNovember 15, 2024\nBy Catherine Cha\nOn November 8, 2024, LEAP‚Äôs Junior Scientists Advisory Group (JSAG) presented our\nfirst\never\n‚ÄúClimateML\nMethods\nWorkshop.‚Äù\nThis\nday-long\nevent\ngathered\nover 20\nearly-career scientists in the New York City area to share progress at the intersection of\nclimate science and machine learning.\nThea Heimdal and Joseph Ko, two of the Workshop organizers, were eager to create a\nspace for researchers, including colleagues external to LEAP, who are using machine\nlearning (ML) and artificial intelligence (AI) for climate-related research. ‚ÄúWe wanted to\nbetter understand what kind of methods people use, and see if we can learn from each\nother,‚Äù explains Heimdal. Ko adds, ‚ÄúWe also hoped to create a friendly environment for\njunior researchers to meet each other and spark collaborations.‚Äù\n\nThe Workshop showcased nine talks:\n‚óè\n‚ÄúCausality and Equation Discovery for Hybrid Modeling‚Äù - Kai-Hendrik Cohrs (U.\nValencia)\n‚óè\n‚ÄúDifferentiable Land Model Reveals Global Environmental Controls on Ecological\nParameters‚Äù - Jianing Fang (Columbia)\n‚óè\n‚ÄúReduced Order Modeling for Linearized Representations of Microphysical Process\nRates‚Äù - Kara Lamb (Columbia)\n‚óè\n‚ÄúEmulating NASA ModelE Shared Socioeconomic Pathways (SSPs) Simulations with\na Convolutional Neural Network‚Äù - Paul Lerner (Columbia)\n‚óè\n‚ÄúA Simple Emulator That Enables Interpretation of Parameter-Output\nRelationships, Applied to Two Climate Model PPEs‚Äù - Qingyuan Yang (Columbia)\n‚óè\n‚ÄúClimate-Invariant Machine Learning: Insights from Philosophy of Induction‚Äù - Dan\nLi (Baruch)\n‚óè\n‚ÄúAsking For the Third Time, Are Transformers Effective For Time Series\nForecasting?‚Äù - Olya Skulovich (NASA GSFC)\n‚óè\n‚ÄúTorchEOF: Differentiable Decomposition Techniques for Multivariate\nSpatiotemporal Data‚Äù - Juan Nathaniel (Columbia)\n‚óè\n‚ÄúSeasonal Predictions of Chl Variability in the High Latitude Southern Ocean Using\nDeep Learning‚Äù - Gian Giacomo Navarra (Princeton)\nThis informal Workshop, punctuated by lunch together, and followed by a happy hour\nsocial event, made it easy for Workshop participants to engage with the presentations,\nask questions, and get to know each other. Ko hopes that the personal connections made\nduring the Workshop will pave the way for collaborative projects and deeper peer-to-peer\nnetworking. The Workshop organizers also look forward to building upon feedback from\n\nthis\nevent,\nand\noffering\nfuture\nWorkshops\nwith\nmore\nfocused\nthemes\nand\na\nbroader\nrepresentation\nof\nspeakers\nand\ninstitutions.\nClick here for more information about\nLEAP‚Äôs upcoming events.\nCatherine Cha is the Senior Manager of Communications and Knowledge Transfer at LEAP.\n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/wp-content/uploads/2025/02/LEAP-Ethics-Handbook.pdf",
    "transcript": " \n \nLEAP Ethics Handbook \nPurpose  \nTo establish expectations among members of the LEAP community regarding how to \nfoster a productive research environment.  \n \nGeneral \nMembers of the LEAP community commit to high ethical standards. In addition to \nabiding by all municipal, state, and federal laws, LEAP members are also \nresponsible to comply with their own institutional codes of conduct and ethical \npolicies as set forth below: \n \nColumbia University Code of Conduct‚Äã\nNew York University Code of Conduct‚Äã\nUniversity of California Irvine, Code of Conduct‚Äã\nNational Center for Atmospheric Research / UCAR Code of Conduct \nCUNY New York City College of Technology Ethics \nUniversity of California San Diego Policies and Principles \n \n \nExpectations \nScientists and students are expected to conduct their research professionally, and \nprioritize safety and ethics in their work. Working with others involves respectfully \nmanaging interpersonal relationships with all group members. LEAP expects its \nmembers to engage in prompt and clear communication, whether personal and/or \nacross all available platforms utilized by the Center (e.g., Slack, email, Github). \nOften there are opportunities for collaborations within a research group and also \noutside the lab with other groups or researchers. Collaboration is a key component \nof research and can increase your knowledge of the field as well as expand the tools \navailable to you. LEAP members are expected to share their expertise, and be \nflexible to learn new things from colleagues at all professional stages.  \n\nFor early-career scientists, experience is not just about research. There are other \n‚Äúmetrics‚Äù of success, such as teaching, data generation, scripts, or communication. \nIf there are things you would like to prioritize, discuss and identify them with your \nPI or another mentor and see how they could be integrated within your research. \nSeek help if you are stuck or have difficulties, either with your PI, another mentor, \nor with others in the group. If you‚Äôre not sure who to contact or would like to \ndiscuss a sensitive matter please reach out to Chief Broadening Participation Officer, \nCourtney Cogburn.  \n \nCulture + Environment \nLEAP‚Äôs expectation is that you demonstrate a positive attitude towards your work \nand that the group culture remains positive. LEAP members will create and maintain \na safe and supportive professional environment for creativity and learning, by being \na welcoming, courteous, and respectful community that is thoughtful about cultural \nand economic barriers to inclusion, and by communicating in ways that uphold each \nother‚Äôs dignity. \nWelcoming people from all backgrounds is important and benefits the entire Center, \nas demonstrated by substantial science. Our Center expects utmost respect towards \nall, especially as our members hail from diverse histories and work in different \nareas of expertise. Mutual respect requires an environment where people of all \nbackgrounds and personalities feel comfortable and welcome in scientific \ndiscussions. Mutual respect includes being conscious of how we talk to each other \nand leaving space in conversations for everyone‚Äôs voices. Interrupting, talking over \nothers, and aggressively dominating a conversation are unacceptable behaviors. \nLEAP will not accept any forms of bullying, sexual misconduct, discrimination, \nharassment or prejudice. For more information at Columbia University, visit the \nOffice of Institutional Equity website. \n \nWork-Life Balance \nAt LEAP, we expect you to take holidays to breathe, rest, and step back, as \nguaranteed by your institution‚Äôs policies. You are more creative and productive \nwhen you are fresh. While Center members are expected to be professional and \nfocused in their work, we do not expect unreasonable, unsustainable hours, and do \nnot desire any of our members to feel exhausted or burned out. \nFlexibility where possible is an important asset of our work. Communicate and \ncollaborate with your colleagues to establish an effective, productive hybrid work \nschedule and cadence. \n\nMental Health \nOur Center emphasizes an environment that is respectful and safe, for your mental \nand physical health always takes priority over studies and work. In addition to \nencouraging a healthy work-life balance and holistic well-being among its \ncommunity, LEAP supports you in seeking resources to support your mental health. \nAt Columbia University, staff can access the Employee Assistance Program for a \nrange of supportive services. For students and staff, a range of services are offered \nthrough Columbia‚Äôs Counseling and Psychological Services. \n \nConflict Resolution \nIn the event that a conflict arises, we encourage respectful discussion in a forum in \nwhich all involved feel comfortable. If you have concerns related to ethics or \nbehavior at LEAP, or would like someone to facilitate a discussion, please contact \nany of the following: \n‚óè‚Äã Courtney Cogburn, Chief Broadening Participation Officer & Knowledge \nTransfer Director, LEAP; Associate Professor of Social Work at Columbia \nUniversity (cc3803@columbia.edu) \n‚óè‚Äã Pierre Gentine, Director, LEAP; Maurice Ewing and J. Lamar Worzel \nProfessor \nof \nGeophysics, \nDepartments \nof \nEarth \nand \nEnvironmental \nEngineering and Earth and Environmental Sciences at Columbia University \n(pg2328@columbia.edu)  \n‚óè‚Äã Molly Lopez, Managing Director, LEAP, (ml3122@columbia.edu) \n‚óè‚Äã Columbia University Ombuds Office: (212) 854-1234 \n  \n \nAuthorship \nOur Center collaborates with many other groups. We will give credit to people \ndeserving co-authorship and expect the same in return. \nLEAP suggests that research project PIs explicitly discuss with team members \nauthorship criteria as early as possible and before the paper writing stage using \nclear definitions of contribution roles such as the AGU‚Äôs CRediT (Contributor Roles \nTaxonomy). These roles can include: Conceptualization, Data Curation, Formal \nAnalysis, Funding Acquisition, Investigation, Methodology, Project Administration, \nResources, Software, Supervision, Validation, Visualization, Writing ‚Äì Original Draft, \nReview & Editing. \n\nAs much as possible, take notes during meetings about roles and send follow up \nemail(s) to keep a record of the discussion and delineation of roles in your \ncollaborations. \nFor large collaborations, LEAP suggests the best practice of making specific requests \nand proposing timelines (even if hypothetical) to clarify involvement and potential \ncollaborations/co-authorship. \nResearch PIs should take steps to ensure that early-career scientists are fairly \ncredited in papers and collaborations, especially when there are senior scientists/PIs \ninvolved. Anyone who has contributed more should be given more visibility in the \npaper‚Äôs author order and due credit in the contribution statement (if there is one), \nfollowing the convention of the publication venue and discipline. If anyone has \njoined  a collaboration but has not contributed, we should not feel obliged to include \nthem in the author list. Even when a contribution to a project leads to unanticipated \nresults (e.g., the research results achieved are less impactful than the research \nteam originally envisioned), contributions should still be acknowledged and credited \nby an authorship.  \n \nAdministrative  \nLEAP‚Äôs main offices are located at the Columbia Engineering Innovation Hub (2276 \n12th Ave, New York, NY 10027) on the Manhattanville Campus.  \nIf you have questions related to administrative concerns, please contact: \n‚óè‚Äã Molly Lopez (ml3122@columbia.edu): general administrative questions \n‚óè‚Äã Catherine Cha (cc102@columbia.edu): questions related to LEAP‚Äôs website \nor social media channels \n‚óè‚Äã James \nLu \n(jl6525@columbia.edu): \nquestions \nrelated \nto \nfinances, \nreimbursements or purchasing \n‚óè‚Äã LEAP‚Äôs main contact (LEAP@columbia.edu): all other questions, including \nroom reservation requests \n \nMore information about life within the LEAP community may be found in our \nCenter‚Äôs Code of Conduct. \n\n"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/opportunities/",
    "transcript": "Opportunities - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Job Opportunities LEAP Center Opportunities **NEW** Climate Data Science Education + Outreach Coordinator Deadline : open until filled, with ideal start in February 2025. Student Opportunities Please check back for future opportunities! Postdoctoral Opportunities **NEW** Postdoctoral Researcher, Quantitative Ecosystem Dynamics Lab (UC Berkeley) Deadline : December 22, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Extreme Ocean Events) (UC Irvine) Deadline : December 31, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Marine Heat Waves) (UC Irvine) Deadline : December 31, 2024 **NEW** Postdoctoral Research Associate, Atmospheric Sciences + Machine Learning (NYU Courant Institute) Deadline : January 15, 2025, with anticipated start date of March 2025. **NEW** Postdoctoral Research Scientist, Physical Oceanography (LDEO/Columbia) Deadline : ASAP, with anticipated start date of January 1, 2025 Faculty + Research Opportunities **NEW** Software Engineer, Global Ocean And Land Alkalinization DoE Earth Shot Project (Yale School of the Environment) **NEW** Geospatial Data Engineer (Planette AI) **NEW** Assistant Professor (limited term), Statistics (Columbia University) Deadline : review of applications begins December 1, 2024, and will continue until position is filled. **NEW** Lecturer in Discipline, Statistics (Columbia University) Deadline : review of applications begins February 1, 2025, and will continue until position is filled. Anticipated start date is Fall 2024. Faculty Positions (multiple) : Marine, Atmospheric, Environmental or Earth Sciences with Data Science Expertise (Univ. of Miami) Deadline : Review of applications is ongoing, and each appointment is expected to start as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/opportunities/#content",
    "transcript": "Opportunities - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Job Opportunities LEAP Center Opportunities **NEW** Climate Data Science Education + Outreach Coordinator Deadline : open until filled, with ideal start in February 2025. Student Opportunities Please check back for future opportunities! Postdoctoral Opportunities **NEW** Postdoctoral Researcher, Quantitative Ecosystem Dynamics Lab (UC Berkeley) Deadline : December 22, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Extreme Ocean Events) (UC Irvine) Deadline : December 31, 2024 Postdoctoral Scholar, Ocean + Climate Dynamics (Marine Heat Waves) (UC Irvine) Deadline : December 31, 2024 **NEW** Postdoctoral Research Associate, Atmospheric Sciences + Machine Learning (NYU Courant Institute) Deadline : January 15, 2025, with anticipated start date of March 2025. **NEW** Postdoctoral Research Scientist, Physical Oceanography (LDEO/Columbia) Deadline : ASAP, with anticipated start date of January 1, 2025 Faculty + Research Opportunities **NEW** Software Engineer, Global Ocean And Land Alkalinization DoE Earth Shot Project (Yale School of the Environment) **NEW** Geospatial Data Engineer (Planette AI) **NEW** Assistant Professor (limited term), Statistics (Columbia University) Deadline : review of applications begins December 1, 2024, and will continue until position is filled. **NEW** Lecturer in Discipline, Statistics (Columbia University) Deadline : review of applications begins February 1, 2025, and will continue until position is filled. Anticipated start date is Fall 2024. Faculty Positions (multiple) : Marine, Atmospheric, Environmental or Earth Sciences with Data Science Expertise (Univ. of Miami) Deadline : Review of applications is ongoing, and each appointment is expected to start as soon as possible. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/jsag/",
    "transcript": "Junior Scientists Advisory Group - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Junior Scientists Advisory Group In 2023, LEAP established the Junior Scientists Advisory Group (JSAG) to foster transparency and communication throughout the LEAP community, as well as promote a deeper sense of community within our Center.¬† Current JSAG members are listed below. JSAG activities and responsibilities include: Liaison representation at a portion of the monthly LEAP Executive Committee meeting Providing input regarding Center events, such as Journal Club meetings Spearheading community-building events and social activities If you are interested in joining LEAP‚Äôs Junior Scientist Advisory Group, please contact Molly Lopez, Managing Director, at ml3122@columbia.edu . Jatan Buch Postdoc, Earth + Environmental Engineering (Columbia University) Savannah Ferretti Graduate Research Assistant, Earth System Science (UC Irvine) Linnia Hawkins Postdoc, Earth + Environmental Engineering (Columbia University) Thea Heimdal Associate Research Scientist, Geochemistry (Columbia University) Arthur Hu Postdoc, Center for Climate Systems Research (Columbia University) Joseph Ko Postdoc, Ocean + Climate Physics (Columbia University) Shuolin (Shawn) Li Postdoc, Data Science Institute (Columbia University) Kaitlyn Loftus Postdoc, Climate School (Columbia University) Racheet Matai Postdoc, Marine and Polar Geophysics (Columbia University) Yongquan (Francis) Qu Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Christina Torres Graduate Research Assistant, Science Education (Teachers College, Columbia University) Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/about/jsag/#content",
    "transcript": "Junior Scientists Advisory Group - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Junior Scientists Advisory Group In 2023, LEAP established the Junior Scientists Advisory Group (JSAG) to foster transparency and communication throughout the LEAP community, as well as promote a deeper sense of community within our Center.¬† Current JSAG members are listed below. JSAG activities and responsibilities include: Liaison representation at a portion of the monthly LEAP Executive Committee meeting Providing input regarding Center events, such as Journal Club meetings Spearheading community-building events and social activities If you are interested in joining LEAP‚Äôs Junior Scientist Advisory Group, please contact Molly Lopez, Managing Director, at ml3122@columbia.edu . Jatan Buch Postdoc, Earth + Environmental Engineering (Columbia University) Savannah Ferretti Graduate Research Assistant, Earth System Science (UC Irvine) Linnia Hawkins Postdoc, Earth + Environmental Engineering (Columbia University) Thea Heimdal Associate Research Scientist, Geochemistry (Columbia University) Arthur Hu Postdoc, Center for Climate Systems Research (Columbia University) Joseph Ko Postdoc, Ocean + Climate Physics (Columbia University) Shuolin (Shawn) Li Postdoc, Data Science Institute (Columbia University) Kaitlyn Loftus Postdoc, Climate School (Columbia University) Racheet Matai Postdoc, Marine and Polar Geophysics (Columbia University) Yongquan (Francis) Qu Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Christina Torres Graduate Research Assistant, Science Education (Teachers College, Columbia University) Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/ec-and-staff/",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/ec-and-staff/#content",
    "transcript": "Page not found ‚Äì LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP The page can‚Äôt be found. It looks like nothing was found at this location. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/ec-and-staff/sr-personnel/",
    "transcript": "Senior Personnel - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Senior Personnel Viviana Acquaviva Professor of Physics, CUNY NYC College of Technology and CUNY Graduate Center Candace Agonafir Associate Research Scientist, Data Science + Civil Engineering, Columbia University Dhruv Balwada Lamont Assistant Research Professor, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Katie Dagon Project Scientist II - Climate Change Research Section, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Gregory Elsaesser Research Scientist, Columbia University / NASA Goddard Institute for Space Studies (NASA GISS) David John Gagne Machine Learning Scientist II & Head of the Machine Integration and Learning for Earth Systems Group, National Center for Atmospheric Research (NSF NCAR) Marco Giometto Assistant Professor of Civil Engineering & Engineering Mechanics, Columbia University Linnia Hawkins Associate Research Scientist, Earth + Environmental Engineering, Columbia University Jonathan Kingslake Associate Professor, Department of Earth & Environmental Sciences (DEES), Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Samory Kpotufe Associate Professor, Department of Statistics, Columbia University Vipin Kumar Regents Professor and William Norris Endowed Chair, Department of Computer Science & Engineering, University of Minnesota; Director of the CSE Data Science Initiative Kara Lamb Associate Research Scientist, Earth + Environmental Engineering, Columbia University Charles Lang Senior Executive Director of the Digital Futures Institute, Teachers College of Columbia University Stephan Mandt Associate Professor of Computer Science and Statistics, University of California, Irvine Brian Medeiros Project Scientist, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Robert Pincus Lamont Research Professor, Ocean & Climate Physics, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School; Adjunct Professor, Department of Applied Mathematics & Applied Physics, School of Engineering & Applied Science, Columbia University Oren Pizmony-Levy Associate Professor of International & Comparative Education, Teachers College of Columbia University; Director of the Center for Sustainable Futures David Porter Associate Research Scientist, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Addisu Semie Associate Research Scientist, Columbia University; National Center for Atmospheric Research (NSF NCAR) Sara Shamekh Assistant Professor, Courant Institute of Mathematical Sciences, New York University Marcus van Lier-Walqui Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Ensheng Weng Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/ec-and-staff/sr-personnel/#content",
    "transcript": "Senior Personnel - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Senior Personnel Viviana Acquaviva Professor of Physics, CUNY NYC College of Technology and CUNY Graduate Center Candace Agonafir Associate Research Scientist, Data Science + Civil Engineering, Columbia University Dhruv Balwada Lamont Assistant Research Professor, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Katie Dagon Project Scientist II - Climate Change Research Section, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Gregory Elsaesser Research Scientist, Columbia University / NASA Goddard Institute for Space Studies (NASA GISS) David John Gagne Machine Learning Scientist II & Head of the Machine Integration and Learning for Earth Systems Group, National Center for Atmospheric Research (NSF NCAR) Marco Giometto Assistant Professor of Civil Engineering & Engineering Mechanics, Columbia University Linnia Hawkins Associate Research Scientist, Earth + Environmental Engineering, Columbia University Jonathan Kingslake Associate Professor, Department of Earth & Environmental Sciences (DEES), Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Samory Kpotufe Associate Professor, Department of Statistics, Columbia University Vipin Kumar Regents Professor and William Norris Endowed Chair, Department of Computer Science & Engineering, University of Minnesota; Director of the CSE Data Science Initiative Kara Lamb Associate Research Scientist, Earth + Environmental Engineering, Columbia University Charles Lang Senior Executive Director of the Digital Futures Institute, Teachers College of Columbia University Stephan Mandt Associate Professor of Computer Science and Statistics, University of California, Irvine Brian Medeiros Project Scientist, National Center for Atmospheric Research (NSF NCAR) Climate and Global Dynamics Laboratory Robert Pincus Lamont Research Professor, Ocean & Climate Physics, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School; Adjunct Professor, Department of Applied Mathematics & Applied Physics, School of Engineering & Applied Science, Columbia University Oren Pizmony-Levy Associate Professor of International & Comparative Education, Teachers College of Columbia University; Director of the Center for Sustainable Futures David Porter Associate Research Scientist, Lamont-Doherty Earth Observatory (LDEO) / Columbia Climate School Addisu Semie Associate Research Scientist, Columbia University; National Center for Atmospheric Research (NSF NCAR) Sara Shamekh Assistant Professor, Courant Institute of Mathematical Sciences, New York University Marcus van Lier-Walqui Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Ensheng Weng Associate Research Scientist, Center for Climate Systems Research (CCSR), Columbia Climate School Qingyuan Yang Associate Research Scientist, Earth + Environmental Engineering, Columbia University Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/ec-and-staff/studentsandpostdocs/",
    "transcript": "Students + Postdocs - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Students + Postdocs Gabriele Accarino Postdoc (Columbia University) Guillaume Bertoli Postdoctoral Fellow (Columbia University) Matthieu Blanke Postdoc, Courant Institute of Mathematical Sciences (NYU) Da Fan Postdoc, Meteorology + Atmospheric Science (NSF NCAR) Savannah Ferretti ** Graduate Research Assistant, Earth System Science (UC Irvine) Dion Ho Graduate Research Assistant, Applied Physics + Applied Mathematics (Columbia University) Francesco Immorlano Postdoc, Computer Science (UC Irvine) Jaeyoung Jung Postdoc, Civil Engineering + Engineering Mechanics (Columbia University) Joseph Ko ** Postdoc, Climate School (Columbia University) Michelle Krakora Bridge-to-PhD Scholar (Columbia University) Aya Lahlou Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Shuolin (Shawn) Li ** Postdoc, Data Science Institute (Columbia University) Jerry Lin Graduate Research Assistant, Earth System Science (UC Irvine) Kaitlyn Loftus ** Postdoc, Climate School (Columbia University) Racheet Matai ** Postdoc, Climate School (Columbia University) Yongquan Francis Qu ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Arvind Renganathan Graduate Research Assistant, Computer Science (Univ. of Minnesota) Julia Simpson ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Abby Shaum Graduate Staff Associate, Climate School (Columbia University) Christina Torres ** Graduate Research Assistant, Science Education (Teachers College, Columbia University) Jiarong Wu Postdoc, Courant Institute of Mathematical Sciences (NYU) ** These LEAPers are also members of the LEAP Junior Scientists Advisory Group (JSAG). Read more about JSAG here . Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://leap.columbia.edu/ec-and-staff/studentsandpostdocs/#content",
    "transcript": "Students + Postdocs - LEAP Skip to content About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Menu About About Our Center Meet Our Team Executive Committee + Staff Senior Personnel Students + Postdocs Organization Chart Junior Scientists Advisory Group External Advisory Committee Partners + Supporters Strategic Partnership Opportunities LEAP Policies News LEAP News Archive Research LEAP Research Research Projects || Years 3 ‚Äì 4 Research Projects || Years 1 ‚Äì 2 Publications, Papers, Conferences Seminars and Panels Policies LEAP Pangeo LEAP Pangeo Benefits Request Access Citing LEAP Pangeo Education LEAP Education Education Reach Curriculum Momentum Bootcamps Winter 2025 LEAP Momentum Bootcamp Summer REU Program 2025 LEAP Summer REU Program Info 2024 LEAP Summer REU Program 2023 LEAP Summer REU Program 2022 LEAP Summer REU Program Summer Momentum Fellowship 2025 LEAP Summer Momentum Fellowship Info 2024 LEAP Summer Momentum Fellowship 2023 LEAP Summer Momentum Fellowship 2022 LEAP Summer Momentum Fellowship Summer Institute 2025 NYC Summer Institute 2024 NYC Summer Institute 2023 NYC Summer Institute Design Studio Broadening Participation Knowledge Transfer Events LEAP Events Calendar Wallerstein LEAP Panel Series on AI + Extreme Weather Preparedness 2025 Spring Lectures 2024 Fall Lectures LEAP @ AGU24 Year 4 Annual Meeting YEAR 4 ANNUAL MEETING YEAR 4 ANNUAL MEETING: DAY 1 YEAR 4 ANNUAL MEETING: DAY 2 YEAR 4 ANNUAL MEETING: POSTERS Past Events Internal Internal Resources Year 3 NSF Site Visit Subscribe Support LEAP Students + Postdocs Gabriele Accarino Postdoc (Columbia University) Guillaume Bertoli Postdoctoral Fellow (Columbia University) Matthieu Blanke Postdoc, Courant Institute of Mathematical Sciences (NYU) Da Fan Postdoc, Meteorology + Atmospheric Science (NSF NCAR) Savannah Ferretti ** Graduate Research Assistant, Earth System Science (UC Irvine) Dion Ho Graduate Research Assistant, Applied Physics + Applied Mathematics (Columbia University) Francesco Immorlano Postdoc, Computer Science (UC Irvine) Jaeyoung Jung Postdoc, Civil Engineering + Engineering Mechanics (Columbia University) Joseph Ko ** Postdoc, Climate School (Columbia University) Michelle Krakora Bridge-to-PhD Scholar (Columbia University) Aya Lahlou Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Shuolin (Shawn) Li ** Postdoc, Data Science Institute (Columbia University) Jerry Lin Graduate Research Assistant, Earth System Science (UC Irvine) Kaitlyn Loftus ** Postdoc, Climate School (Columbia University) Racheet Matai ** Postdoc, Climate School (Columbia University) Yongquan Francis Qu ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Arvind Renganathan Graduate Research Assistant, Computer Science (Univ. of Minnesota) Julia Simpson ** Graduate Research Assistant, Earth + Environmental Engineering (Columbia University) Abby Shaum Graduate Staff Associate, Climate School (Columbia University) Christina Torres ** Graduate Research Assistant, Science Education (Teachers College, Columbia University) Jiarong Wu Postdoc, Courant Institute of Mathematical Sciences (NYU) ** These LEAPers are also members of the LEAP Junior Scientists Advisory Group (JSAG). Read more about JSAG here . Join LEAP Join LEAP and help develop the next generation of climate modeling and climate projection for tailored adaptation. Click to browse opportunities to be part of the LEAP Center. Contact Us E-mail: leap@columbia.edu Columbia Engineering Innovation Hub 2276 12th Ave, 2nd Floor New York, NY 10027 Copyright ¬© 2025 Linkedin Youtube Instagram"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu",
    "transcript": "LEAP Data Catalog Data Catalog Search carbon machine learning ocean zarr LDEO-pCO2 HPD Back in Time The ocean reduces human impact on the climate by absorbing and sequestering CO2 . From 1950s to the 1980s, observations of pCO2 and related ocean carbon variables were sparse and uncertain. Thus, glob... meteorology open-storage-network osn pangeo-forge precipitation zarr Australian Gridded Climate Data (AGCD) The Australian Gridded Climate Data dataset is a 5KM gridded daily product provided by the Australian Bureau of Meteorology. Version 1 contains precipitation, vapour pressure and minimum and maximum... atmosphere reanalysis zarr ARCO ERA5 Analysis-Ready, Cloud Optimized ERA5 data ingested by Google Research noaa oisst open-storage-network osn pangeo-forge sst zarr AWS NOAA Optimum Interpolated SST Analysis-ready datasets derived from AWS NOAA OISST NetCDF open-storage-network osn pangeo-forge zarr cesm-atm-025deg Daily atmospheric data from coupled NCAR CESM run hybrid_v5_rel04_BC5_ne120_t12_pop62 my-custom-tag zarr NOAA-GFDL CM2-O Coupled Climate Model Simulations cmip6 gcs zarr CMIP6 CMIP6 datasets converted to zarr stores from ESGF files climate europe open-storage-network osn regional zarr E-OBS dataset E-OBS is a high resolution gridded climate dataset covering Europe and Northern Africa. This dataset is version 23.1e with a 0.1 degree spatial resolution. Variables available are: TG: daily mean tem... open-storage-network osn pangeo-forge volcanology zarr eVolv2k VSSI database eVolv2k includes estimates of the magnitudes and approximate source latitudes of major volcanic stratospheric sulfur injection (VSSI) events from 500BCE to 1900CE. open-storage-network osn pangeo-forge zarr Global Precipitation Climatology Project Global Precipitation Climatology Project (GPCP) Daily Version 1.3 gridded, merged ty satellite/gauge precipitation Climate data Record (CDR) from 1996 to present. climate ice oceanography open-storage-network osn pangeo-forge sea temperature zarr Hadley Centre Sea Ice and Sea Surface Temperature (HadISST) Analysis-ready Zarr datasets derived from HadISST NetCDF carbon gcp zarr Large ensemble pCO2 testbed Large ensemble pCO2 testbed (manually ingested) open-storage-network osn pangeo-forge zarr LMRv2.1 Monte Carlo Runs for gridded fields Gridded variables from ensemble runs executed as part of the Last Millennium Reanalysis (LMR) project (v2.1) which utilizes an ensemble methodology to assimilate paleoclimate data for the production o... my-custom-tag zarr MetaFlux MetaFlux: Meta-learning global carbon fluxes from sparse spatiotemporal observations noaa open-storage-network osn pangeo-forge polar zarr NOAA Coastwatch Geo-Polar SST NOAA Geo-Polar Blended Global Sea Surface Temperature Analysis (Level 4) carbon ocean osn satellite zarr Sea-surface altimetry data from The Copernicus Marine Environment SSALTO/DUACS Delayed-Time Level-4 sea surface height and derived variables measured by multi-satellite altimetry observations over Global Ocean. This dataset was moved from the legacy pangeo catalog. open-storage-network osn pangeo-forge zarr WOA 2018 1 degree monthly 1 Degree monthly climatology from WOA my-custom-tag zarr ClimSim \"Analysis-ready optimized ClimSim data\" llc4320 m2lines ocean zarr High-resolution ocean simulation (LLC4320) 12-hourly averaged 3D regions The LLC4320 is an ocean only simulation forced by 6 hourly atmospheric fields, which was run at NASA JPL by Dimitris Menemenlis. These are 12 hourly averaged fields for from 12 different regions in... precipitation zarr CHIRPS-2.0 global daily Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) neural network ocean surface boundary zarr GOTM NN Training Data Data for NN training from Sane et al. 2023. (Parameterizing Vertical Mixing Coefficients in the Ocean Surface Boundary Layer using Neural Networks). An additional text data file is available at: https... global machine learning remote sensing SMAP soil moisture CASM CASM: A long-term Consistent Artificial-intelligence based Soil Moisture dataset based on machine learning and remote sensing m2lines model ocean zarr eNATL eNATL60-TSW-60m & 600m are extractions of a very high resolution oceanic simulation of the North Atlantic performed at MEOM, IGE (FRANCE). ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/",
    "transcript": "LEAP Data Catalog Data Catalog Search carbon machine learning ocean zarr LDEO-pCO2 HPD Back in Time The ocean reduces human impact on the climate by absorbing and sequestering CO2 . From 1950s to the 1980s, observations of pCO2 and related ocean carbon variables were sparse and uncertain. Thus, glob... meteorology open-storage-network osn pangeo-forge precipitation zarr Australian Gridded Climate Data (AGCD) The Australian Gridded Climate Data dataset is a 5KM gridded daily product provided by the Australian Bureau of Meteorology. Version 1 contains precipitation, vapour pressure and minimum and maximum... atmosphere reanalysis zarr ARCO ERA5 Analysis-Ready, Cloud Optimized ERA5 data ingested by Google Research noaa oisst open-storage-network osn pangeo-forge sst zarr AWS NOAA Optimum Interpolated SST Analysis-ready datasets derived from AWS NOAA OISST NetCDF open-storage-network osn pangeo-forge zarr cesm-atm-025deg Daily atmospheric data from coupled NCAR CESM run hybrid_v5_rel04_BC5_ne120_t12_pop62 my-custom-tag zarr NOAA-GFDL CM2-O Coupled Climate Model Simulations cmip6 gcs zarr CMIP6 CMIP6 datasets converted to zarr stores from ESGF files climate europe open-storage-network osn regional zarr E-OBS dataset E-OBS is a high resolution gridded climate dataset covering Europe and Northern Africa. This dataset is version 23.1e with a 0.1 degree spatial resolution. Variables available are: TG: daily mean tem... open-storage-network osn pangeo-forge volcanology zarr eVolv2k VSSI database eVolv2k includes estimates of the magnitudes and approximate source latitudes of major volcanic stratospheric sulfur injection (VSSI) events from 500BCE to 1900CE. open-storage-network osn pangeo-forge zarr Global Precipitation Climatology Project Global Precipitation Climatology Project (GPCP) Daily Version 1.3 gridded, merged ty satellite/gauge precipitation Climate data Record (CDR) from 1996 to present. climate ice oceanography open-storage-network osn pangeo-forge sea temperature zarr Hadley Centre Sea Ice and Sea Surface Temperature (HadISST) Analysis-ready Zarr datasets derived from HadISST NetCDF carbon gcp zarr Large ensemble pCO2 testbed Large ensemble pCO2 testbed (manually ingested) open-storage-network osn pangeo-forge zarr LMRv2.1 Monte Carlo Runs for gridded fields Gridded variables from ensemble runs executed as part of the Last Millennium Reanalysis (LMR) project (v2.1) which utilizes an ensemble methodology to assimilate paleoclimate data for the production o... my-custom-tag zarr MetaFlux MetaFlux: Meta-learning global carbon fluxes from sparse spatiotemporal observations noaa open-storage-network osn pangeo-forge polar zarr NOAA Coastwatch Geo-Polar SST NOAA Geo-Polar Blended Global Sea Surface Temperature Analysis (Level 4) carbon ocean osn satellite zarr Sea-surface altimetry data from The Copernicus Marine Environment SSALTO/DUACS Delayed-Time Level-4 sea surface height and derived variables measured by multi-satellite altimetry observations over Global Ocean. This dataset was moved from the legacy pangeo catalog. open-storage-network osn pangeo-forge zarr WOA 2018 1 degree monthly 1 Degree monthly climatology from WOA my-custom-tag zarr ClimSim \"Analysis-ready optimized ClimSim data\" llc4320 m2lines ocean zarr High-resolution ocean simulation (LLC4320) 12-hourly averaged 3D regions The LLC4320 is an ocean only simulation forced by 6 hourly atmospheric fields, which was run at NASA JPL by Dimitris Menemenlis. These are 12 hourly averaged fields for from 12 different regions in... precipitation zarr CHIRPS-2.0 global daily Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) neural network ocean surface boundary zarr GOTM NN Training Data Data for NN training from Sane et al. 2023. (Parameterizing Vertical Mixing Coefficients in the Ocean Surface Boundary Layer using Neural Networks). An additional text data file is available at: https... global machine learning remote sensing SMAP soil moisture CASM CASM: A long-term Consistent Artificial-intelligence based Soil Moisture dataset based on machine learning and remote sensing m2lines model ocean zarr eNATL eNATL60-TSW-60m & 600m are extractions of a very high resolution oceanic simulation of the North Atlantic performed at MEOM, IGE (FRANCE). ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/ldeopco2-hpd-back-in-time",
    "transcript": "Feedstock: ldeopco2-hpd-back-in-time Back LDEO-pCO2 HPD Back in Time carbon machine learning ocean zarr The ocean reduces human impact on the climate by absorbing and sequestering CO2 . From 1950s to the 1980s, observations of pCO2 and related ocean carbon variables were sparse and uncertain. Thus, global ocean biogeochemical models (GOBMs) have been the basis for quantifying the ocean carbon sink. The LDEO-Hybrid Physics Data product (LDEO-HPD) interpolates sparse surface ocean pCO2 data to global coverage by using GOBMs as priors, and applying machine learning to estimate full-coverage corrections. Lamont-Doherty Earth Observatory Hybrid Physics Data pCO2 Product The ocean carbon sink data story Access Data flux Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 's3://carbonplan-data-viewer/demo/leap-data-stories/GCB-2023_dataprod_LDEO-HPD_1959-2022-updated-flipped-lon.zarr/' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Annual fgco2 Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 's3://carbonplan-data-viewer/demo/leap-data-stories/annual-sfco2.zarr/' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC BY Repository carbonplan/ocean-carbon-sink-data-feedstock Providers LDEO LEAP ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/australian-gridded-climate-data-agcd",
    "transcript": "Feedstock: australian-gridded-climate-data-agcd Back Australian Gridded Climate Data (AGCD) meteorology open-storage-network osn pangeo-forge precipitation zarr The Australian Gridded Climate Data dataset is a 5KM gridded daily product provided by the Australian Bureau of Meteorology. Version 1 contains precipitation, vapour pressure and minimum and maximum temperature. Australian Bureau of Meteorology (2020), Australian Gridded Climate Data (AGCD); v1.0.0 Snapshot (1900-01-01 to 2020-05-31). Downloaded from NCI on 09-21-2021. Access Data AGCD Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/pangeo-forge/AGDC-feedstock/AGCD.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC-BY-NC-4.0 Repository pangeo-forge/AGDC-feedstock Providers Australian Bureau of Meteorology ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/arco-era5",
    "transcript": "Feedstock: arco-era5 Back ARCO ERA5 atmosphere reanalysis zarr Analysis-Ready, Cloud Optimized ERA5 data ingested by Google Research Access Data 0.25¬∞ ERA5 Surface Level Pressure Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... 0.25¬∞ ERA5 Model Level Pressure Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'gs://gcp-public-data-arco-era5/ar/model-level-1h-0p25deg.zarr-v1' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License Apache Version 2.0 Repository leap-stc/arco-era5_feedstock Providers Google Research ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/aws-noaa-optimum-interpolated-sst",
    "transcript": "Feedstock: aws-noaa-optimum-interpolated-sst Back AWS NOAA Optimum Interpolated SST noaa oisst open-storage-network osn pangeo-forge sst zarr Analysis-ready datasets derived from AWS NOAA OISST NetCDF Access Data noaa-oisst Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/noaa_oisst/v2.1-avhrr.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License Open Data Repository pangeo-forge/aws-noaa-oisst-feedstock Providers AWS NOAA Oceanic CDR ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/cesmatm025deg",
    "transcript": "Feedstock: cesmatm025deg Back cesm-atm-025deg open-storage-network osn pangeo-forge zarr Daily atmospheric data from coupled NCAR CESM run hybrid_v5_rel04_BC5_ne120_t12_pop62 Access Data cesm-atm-025deg Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/cesm-atm-025deg-feedstock/cesm-atm-025deg.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC-BY-4.0 Repository pangeo-forge/cesm-atm-025deg-feedstock Providers National Center for Atmospheric Research ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/noaagfdl-cm2o-coupled-climate-model-simulations",
    "transcript": "Feedstock: noaagfdl-cm2o-coupled-climate-model-simulations Back NOAA-GFDL CM2-O Coupled Climate Model Simulations my-custom-tag zarr Access Data CM2.6 Control Run Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'gs://cmip6/GFDL_CM2_6/control/surface' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... CM2.6 Control Run Atmosphere Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'gs://cmip6/GFDL_CM2_6/control/atmos_daily.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License Unknown Repository leap-stc/cm2o_feedstock Providers NOAA-GFDL ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/cmip6",
    "transcript": "Feedstock: cmip6 Back CMIP6 cmip6 gcs zarr CMIP6 datasets converted to zarr stores from ESGF files DOI Access Data Due to the large number of data stores associated with the feedstock, we have opted not to list them individually here. Instead, you can access information about the data stores by visiting the feedstock's GitHub repository linked below. Maintainers License CC-BY-4.0 Repository leap-stc/cmip6-leap-feedstock Providers ESGF ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/eobs-dataset",
    "transcript": "Feedstock: eobs-dataset Back E-OBS dataset climate europe open-storage-network osn regional zarr E-OBS is a high resolution gridded climate dataset covering Europe and Northern Africa. This dataset is version 23.1e with a 0.1 degree spatial resolution. Variables available are: TG: daily mean temperature, TN: daily minimum temperature, TX: daily maximum temperature, RR: daily precipitation sum, PP: daily mean sea-level pressure, HU: relative humidity, FG: daily mean wind speed and QQ: global radiation. QQ and FG are have slightly different grids and are not concatenated with other variables. Access Data eobs-wind-speed Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/pangeo-forge/EOBS-feedstock/eobs-wind-speed.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... eobs-tg-tn-tx-rr-hu-pp Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/pangeo-forge/EOBS-feedstock/eobs-tg-tn-tx-rr-hu-pp.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... eobs-surface-downwelling Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/pangeo-forge/EOBS-feedstock/eobs-surface-downwelling.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License proprietary Repository pangeo-forge/EOBS-feedstock Providers Copernicus Climate Change Service ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/evolv2k-vssi-database",
    "transcript": "Feedstock: evolv2k-vssi-database Back eVolv2k VSSI database open-storage-network osn pangeo-forge volcanology zarr eVolv2k includes estimates of the magnitudes and approximate source latitudes of major volcanic stratospheric sulfur injection (VSSI) events from 500BCE to 1900CE. Access Data eVolv2k Data Viewer Dataset incompatible with Data Viewer This dataset contains non-geospatial data not supported by the data viewer. Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/pangeo-forge/eVolv2k-feedstock/eVolv2k.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC-BY-4.0 Repository pangeo-forge/eVolv2k-feedstock Providers Matthew Toohey WDCC ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/global-precipitation-climatology-project",
    "transcript": "Feedstock: global-precipitation-climatology-project Back Global Precipitation Climatology Project open-storage-network osn pangeo-forge zarr Global Precipitation Climatology Project (GPCP) Daily Version 1.3 gridded, merged ty satellite/gauge precipitation Climate data Record (CDR) from 1996 to present. Access Data gpcp Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/gpcp-feedstock/gpcp.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License No constraints on data access or use. Repository pangeo-forge/gpcp-feedstock Providers NOAA NCEI University of Maryland ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/hadley-centre-sea-ice-and-sea-surface-temperature-hadisst",
    "transcript": "Feedstock: hadley-centre-sea-ice-and-sea-surface-temperature-hadisst Back Hadley Centre Sea Ice and Sea Surface Temperature (HadISST) climate ice oceanography open-storage-network osn pangeo-forge sea temperature zarr Analysis-ready Zarr datasets derived from HadISST NetCDF Access Data hadisst Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/HadISST-feedstock/hadisst.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License proprietary Repository pangeo-forge/HadISST-feedstock Providers Met Office Hadley Centre ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/large-ensemble-pco2-testbed",
    "transcript": "Feedstock: large-ensemble-pco2-testbed Back Large ensemble pCO2 testbed carbon gcp zarr Large ensemble pCO2 testbed (manually ingested) Access Data CESM.zarr Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent-ro/data-library-manual/CESM.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... CanESM2.zarr Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent-ro/data-library-manual/CanESM2.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... GFDL.zarr Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent-ro/data-library-manual/GFDL.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... Maintainers License CC-BY Repository leap-stc/large-ensemble-pCO2-testbed-feedstock Providers LEAP ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/lmrv21-monte-carlo-runs-for-gridded-fields",
    "transcript": "Feedstock: lmrv21-monte-carlo-runs-for-gridded-fields Back LMRv2.1 Monte Carlo Runs for gridded fields open-storage-network osn pangeo-forge zarr Gridded variables from ensemble runs executed as part of the Last Millennium Reanalysis (LMR) project (v2.1) which utilizes an ensemble methodology to assimilate paleoclimate data for the production of annually resolved climate field reconstructions of the Common Era. Access Data LMRv2p1_MCruns_ensemble_gridded Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/pangeo-forge/LMRv2p1_MCruns_ensemble_gridded-feedstock/LMRv2p1_MCruns_ensemble_gridded.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC-BY-4.0 Repository pangeo-forge/LMRv2p1_MCruns_ensemble_gridded-feedstock Providers NCEI/NOAAA ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/metaflux",
    "transcript": "Feedstock: metaflux Back MetaFlux my-custom-tag zarr MetaFlux: Meta-learning global carbon fluxes from sparse spatiotemporal observations Access Data METAFLUX_GPP_RECO_daily Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent-ro/data-library/feedstocks/metaflux_feedstock/metaflux_daily.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... METAFLUX_GPP_RECO_monthly Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent-ro/data-library/feedstocks/metaflux_feedstock/meatflux_monthly.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... Maintainers License CC-BY-4.0 Repository leap-stc/metaflux_feedstock Providers Zenodo Nathaniel et al. ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/noaa-coastwatch-geopolar-sst",
    "transcript": "Feedstock: noaa-coastwatch-geopolar-sst Back NOAA Coastwatch Geo-Polar SST noaa open-storage-network osn pangeo-forge polar zarr NOAA Geo-Polar Blended Global Sea Surface Temperature Analysis (Level 4) Access Data noaa-coastwatch-geopolar-sst Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/noaa-coastwatch-geopolar-sst-feedstock/noaa-coastwatch-geopolar-sst.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC-BY-4.0 Repository pangeo-forge/noaa-coastwatch-geopolar-sst-feedstock Providers NOAA Coastwatch/OceanWatch ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/seasurface-altimetry-data-from-the-copernicus-marine-environment",
    "transcript": "Feedstock: seasurface-altimetry-data-from-the-copernicus-marine-environment Back Sea-surface altimetry data from The Copernicus Marine Environment carbon ocean osn satellite zarr SSALTO/DUACS Delayed-Time Level-4 sea surface height and derived variables measured by multi-satellite altimetry observations over Global Ocean. This dataset was moved from the legacy pangeo catalog. Data Source (not original source) Original Documentation (not available anymore) Access Data AVISO Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-cmems-duacs' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License proprietary Repository leap-stc/sea-surface-altimetry-data-from-copernicus-feedstock Providers Copernicus Marine Service LEAP ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/woa-2018-1-degree-monthly",
    "transcript": "Feedstock: woa-2018-1-degree-monthly Back WOA 2018 1 degree monthly open-storage-network osn pangeo-forge zarr 1 Degree monthly climatology from WOA Access Data woa18-1deg-monthly Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/WOA_1degree_monthly-feedstock/woa18-1deg-monthly.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC-BY-4.0 Repository pangeo-forge/WOA_1degree_monthly-feedstock Providers NOAA NCEI ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/climsim",
    "transcript": "Feedstock: climsim Back ClimSim my-custom-tag zarr \"Analysis-ready optimized ClimSim data\" Access Data ClimSim Lowres mli Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent-ro/data-library/feedstocks/climsim_feedstock/climsim_lowres_mli.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... Maintainers License Apache-2.0 Repository leap-stc/climsim_feedstock Providers Hugging Face Sungduk Yu et al ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/highresolution-ocean-simulation-llc4320-12hourly-averaged-3d-regions",
    "transcript": "Feedstock: highresolution-ocean-simulation-llc4320-12hourly-averaged-3d-regions Back High-resolution ocean simulation (LLC4320) 12-hourly averaged 3D regions llc4320 m2lines ocean zarr The LLC4320 is an ocean only simulation forced by 6 hourly atmospheric fields, which was run at NASA JPL by Dimitris Menemenlis. These are 12 hourly averaged fields for from 12 different regions in the ocean, each roughly 10degrees by 10degrees in size, for the top 700m of the water column. There five 3D variables: Temperature, Salinity, and 3 components of velocity, and four 2D surface fields: Boundary layer depth, surface heat flux, and 2 components of wind stress. Note that this is a small part of the original dataset which has been processed and sub-sampled to make the size more manageable. Access Data The Agulhas region subset Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://nyu1.osn.mghpcc.org/leap-pangeo-manual/LLC4320_12h_3D-regions/agulhas.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License CC-BY-NC-4.0 Repository leap-stc/LLC4320_12h_3D-regions_feedstock Providers NASA, Abigail Bodner, Dhruv Balwada ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/chirps20-global-daily",
    "transcript": "Feedstock: chirps20-global-daily Back CHIRPS-2.0 global daily precipitation zarr Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) Access Data CHIRPS-2.0 Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://nyu1.osn.mghpcc.org/leap-pangeo-pipeline/chirps_feedstock/chirps-global-daily.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License Just a Test Repository leap-stc/chirps_feedstock Providers Julius ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/gotm-nn-training-data",
    "transcript": "Feedstock: gotm-nn-training-data Back GOTM NN Training Data neural network ocean surface boundary zarr Data for NN training from Sane et al. 2023. (Parameterizing Vertical Mixing Coefficients in the Ocean Surface Boundary Layer using Neural Networks). An additional text data file is available at: https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/data_testing_4_paper.txt Access Data GOTM: Raw training data Data Viewer Dataset incompatible with Data Viewer This dataset contains non-geospatial data not supported by the data viewer. Load Dataset Copy import xarray as xr store = 'https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/raw_training_data.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... GOTM: SF training data Data Viewer Dataset incompatible with Data Viewer This dataset contains non-geospatial data not supported by the data viewer. Load Dataset Copy import xarray as xr store = 'https://nyu1.osn.mghpcc.org/leap-pangeo-manual/GOTM/sf_training_data.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License Creative Commons Attribution 4.0 International Repository leap-stc/GOTM Providers Zenodo ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/casm",
    "transcript": "Feedstock: casm Back CASM global machine learning remote sensing SMAP soil moisture CASM: A long-term Consistent Artificial-intelligence based Soil Moisture dataset based on machine learning and remote sensing Access Data CASM: Consistent Artificial-intelligence Soil Moisture Data Viewer Open in Data Viewer Load Dataset Copy import xarray as xr store = 'https://nyu1.osn.mghpcc.org/leap-pangeo-pipeline/CASM/casm.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Loading dataset representation... Maintainers License unknown Repository leap-stc/CASM Providers Zenodo ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  },
  {
    "class": "WebPage",
    "title": "",
    "videoId": "",
    "url": "https://catalog.leap.columbia.edu/feedstock/enatl",
    "transcript": "Feedstock: enatl Back eNATL m2lines model ocean zarr eNATL60-TSW-60m & 600m are extractions of a very high resolution oceanic simulation of the North Atlantic performed at MEOM, IGE (FRANCE). Access Data eNATL 60M Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent/data-library/feedstocks/eNATL_feedstock/eNATL60-BLBT02.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... eNATL 600M Data Viewer Dataset incompatible with Data Viewer Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Load Dataset Copy import xarray as xr store = 'gs://leap-persistent/data-library/feedstocks/eNATL_feedstock/eNATL600m-BLBT02.zarr' ds = xr . open_dataset ( store , engine = 'zarr' , chunks = { } ) Metadata Private dataset. Access requires credentials or a Columbia-LEAP JupyterHub server. Loading dataset representation... Maintainers License unknown Repository leap-stc/eNATL_feedstock Providers Zenodo ¬© 2025 , LEAP-STC. MIT Licensed. 9852d78"
  }
]